{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2a7d8a55eac74f7081085f34f9705ce5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90b75c58a7034750b29643d270af19ca",
              "IPY_MODEL_5fe867f1d0d143318c11c568d0c200d2",
              "IPY_MODEL_4da1aa8a3c1b47c9995eb2a0299dbdcb"
            ],
            "layout": "IPY_MODEL_b4e538fc8f0f4a2d930c5724621ad3a4"
          }
        },
        "90b75c58a7034750b29643d270af19ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55ed9e15bd7849569cb2736d6bbcf273",
            "placeholder": "​",
            "style": "IPY_MODEL_a05d206dee494905a2333d221e81c841",
            "value": "modules.json: 100%"
          }
        },
        "5fe867f1d0d143318c11c568d0c200d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f771915c65e542a99cfde2f6c96bb2d9",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d02742026b004653b1bb030e569f9055",
            "value": 349
          }
        },
        "4da1aa8a3c1b47c9995eb2a0299dbdcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdc6af1472ba409087d1921a79a11a91",
            "placeholder": "​",
            "style": "IPY_MODEL_253f1221c94f46a3b035fe4d4f717c43",
            "value": " 349/349 [00:00&lt;00:00, 10.2kB/s]"
          }
        },
        "b4e538fc8f0f4a2d930c5724621ad3a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55ed9e15bd7849569cb2736d6bbcf273": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a05d206dee494905a2333d221e81c841": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f771915c65e542a99cfde2f6c96bb2d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d02742026b004653b1bb030e569f9055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cdc6af1472ba409087d1921a79a11a91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "253f1221c94f46a3b035fe4d4f717c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13c9df9229d941e5974bb4b9626691d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1945e5da2493420caa0096bceda10759",
              "IPY_MODEL_998d04307944456fa6eca1b5e640a640",
              "IPY_MODEL_bfc4d212d1bc471fb94b325e93cdd8cd"
            ],
            "layout": "IPY_MODEL_21718bef4c534db4a3c5f77e61abafc6"
          }
        },
        "1945e5da2493420caa0096bceda10759": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7783989021b6410489de834fea7d5950",
            "placeholder": "​",
            "style": "IPY_MODEL_ef7c09785acd4b9c849ac874173dc893",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "998d04307944456fa6eca1b5e640a640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26e858497e05493f8927dad08a9c58da",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_556ce9eb3fd64094a898c76c356773c9",
            "value": 124
          }
        },
        "bfc4d212d1bc471fb94b325e93cdd8cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed37f06ccc164fe1899b760235775386",
            "placeholder": "​",
            "style": "IPY_MODEL_dc709f0bac3548e0bc07190723003e0a",
            "value": " 124/124 [00:00&lt;00:00, 3.67kB/s]"
          }
        },
        "21718bef4c534db4a3c5f77e61abafc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7783989021b6410489de834fea7d5950": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef7c09785acd4b9c849ac874173dc893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26e858497e05493f8927dad08a9c58da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "556ce9eb3fd64094a898c76c356773c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed37f06ccc164fe1899b760235775386": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc709f0bac3548e0bc07190723003e0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41232f708f9e493ab62549089b9ecfea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94e2325b03be4256b02cafabc9e8a0b4",
              "IPY_MODEL_e6c2e49e62ca46579e2b659217f0d4d8",
              "IPY_MODEL_1a02d5627dd64b9ca5dc38bd1445b13d"
            ],
            "layout": "IPY_MODEL_c8811ee1bb9247b5b6bc1c2b531b2c84"
          }
        },
        "94e2325b03be4256b02cafabc9e8a0b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fc8c387accc4ea88bc2e3bd90982b38",
            "placeholder": "​",
            "style": "IPY_MODEL_a0826237751a4723b4cb323c8855f45d",
            "value": "README.md: 100%"
          }
        },
        "e6c2e49e62ca46579e2b659217f0d4d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b198a1b0b91342479c321ccff3c07e21",
            "max": 94783,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0811b393d8d74decb5c8b4e6c31592fe",
            "value": 94783
          }
        },
        "1a02d5627dd64b9ca5dc38bd1445b13d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fa650ce8d2c477685a5d95b5630e498",
            "placeholder": "​",
            "style": "IPY_MODEL_e4beb8a806be417c8d169f2561bfb9e0",
            "value": " 94.8k/94.8k [00:00&lt;00:00, 1.56MB/s]"
          }
        },
        "c8811ee1bb9247b5b6bc1c2b531b2c84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fc8c387accc4ea88bc2e3bd90982b38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0826237751a4723b4cb323c8855f45d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b198a1b0b91342479c321ccff3c07e21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0811b393d8d74decb5c8b4e6c31592fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1fa650ce8d2c477685a5d95b5630e498": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4beb8a806be417c8d169f2561bfb9e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77138b9b3f5e47bb86ba8433b8b5d88a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc01f71d7d09445387d51a06918b0938",
              "IPY_MODEL_5c21e3c19b92447db289884978bb71f3",
              "IPY_MODEL_4a363e02f4684a0d92836bd646757454"
            ],
            "layout": "IPY_MODEL_497d562e62b541de90e297c44bf8727c"
          }
        },
        "bc01f71d7d09445387d51a06918b0938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f846bdcabc884a6f876cb65ae005d369",
            "placeholder": "​",
            "style": "IPY_MODEL_4ba1846d9d9b425ba398a73f5fa0db77",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "5c21e3c19b92447db289884978bb71f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b366625b2cc48c38fb17ecf4b70fbcf",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38c4a8833be2485dba955fa5bc1a7db8",
            "value": 52
          }
        },
        "4a363e02f4684a0d92836bd646757454": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40b99a4100c64c5694fe9f43f9eaad32",
            "placeholder": "​",
            "style": "IPY_MODEL_068dcf3ee1e34e848110b64aacf14fa9",
            "value": " 52.0/52.0 [00:00&lt;00:00, 1.95kB/s]"
          }
        },
        "497d562e62b541de90e297c44bf8727c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f846bdcabc884a6f876cb65ae005d369": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ba1846d9d9b425ba398a73f5fa0db77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b366625b2cc48c38fb17ecf4b70fbcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38c4a8833be2485dba955fa5bc1a7db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40b99a4100c64c5694fe9f43f9eaad32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "068dcf3ee1e34e848110b64aacf14fa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dadfd96c2f694cf7a6c76de18ddb4f73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e641a047601d4e4084dad08c8dbce369",
              "IPY_MODEL_f8a27179e79848bfaffb13b8dfd518eb",
              "IPY_MODEL_52c5a9dad0f74bf5b6cecbc16b752b97"
            ],
            "layout": "IPY_MODEL_103496ff0a854b08a4babb241f7f791d"
          }
        },
        "e641a047601d4e4084dad08c8dbce369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f3d5cd1d1994d849dc826f113622252",
            "placeholder": "​",
            "style": "IPY_MODEL_124c33c947ab4262a82e9176deb1b4f5",
            "value": "config.json: 100%"
          }
        },
        "f8a27179e79848bfaffb13b8dfd518eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1a0cf2af7a94464b6098d065e510dd1",
            "max": 743,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed2b7884e5c1495db4f0aad3719dce47",
            "value": 743
          }
        },
        "52c5a9dad0f74bf5b6cecbc16b752b97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d56de21a67bc4a3bbcb00c7e11e84b0f",
            "placeholder": "​",
            "style": "IPY_MODEL_a58bae7c70b142549c135642e662fb05",
            "value": " 743/743 [00:00&lt;00:00, 20.7kB/s]"
          }
        },
        "103496ff0a854b08a4babb241f7f791d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f3d5cd1d1994d849dc826f113622252": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "124c33c947ab4262a82e9176deb1b4f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1a0cf2af7a94464b6098d065e510dd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed2b7884e5c1495db4f0aad3719dce47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d56de21a67bc4a3bbcb00c7e11e84b0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a58bae7c70b142549c135642e662fb05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79e683d2166945a3bf8ca0bb2e192396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22f8c9d7759743e2a6e2dc7f20adcdc9",
              "IPY_MODEL_f9040b02d73e4656b0c9858b197cdcf8",
              "IPY_MODEL_9089020aed8a424baf4f44bed36a21cb"
            ],
            "layout": "IPY_MODEL_77d73aa211574624abaaf0970ff42233"
          }
        },
        "22f8c9d7759743e2a6e2dc7f20adcdc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_725f6c79abea48708fe5894da1395ec5",
            "placeholder": "​",
            "style": "IPY_MODEL_b8cfa09bb23840ec9cb6c2724d12793a",
            "value": "model.safetensors: 100%"
          }
        },
        "f9040b02d73e4656b0c9858b197cdcf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49a1cb7df51149498fc1ad5fc28f9051",
            "max": 133466304,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d47700272ed64ae4af67623c0520e4fa",
            "value": 133466304
          }
        },
        "9089020aed8a424baf4f44bed36a21cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42c087bb2a9d4a0c909a6c5c1d6b6175",
            "placeholder": "​",
            "style": "IPY_MODEL_07f66a604b7347f3a97934be7b25f4d4",
            "value": " 133M/133M [00:00&lt;00:00, 168MB/s]"
          }
        },
        "77d73aa211574624abaaf0970ff42233": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "725f6c79abea48708fe5894da1395ec5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8cfa09bb23840ec9cb6c2724d12793a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49a1cb7df51149498fc1ad5fc28f9051": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d47700272ed64ae4af67623c0520e4fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42c087bb2a9d4a0c909a6c5c1d6b6175": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07f66a604b7347f3a97934be7b25f4d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2336df86b00647d88e6432c70882b1e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4da1a6bac1d4347b66ef933eaabc5d7",
              "IPY_MODEL_f2aa535311e943308a1301c2d7ad1308",
              "IPY_MODEL_4d38dc64e46e4853ab2ce135a07444cd"
            ],
            "layout": "IPY_MODEL_c615b438e5d4432492fdb4e2f9ff067c"
          }
        },
        "f4da1a6bac1d4347b66ef933eaabc5d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97da8739054246e1ae252fbd0305660c",
            "placeholder": "​",
            "style": "IPY_MODEL_8ae8d0b0feb547efab1fff75cd0bbc8c",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "f2aa535311e943308a1301c2d7ad1308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77416f5caa194a5c87603bdf34fd3055",
            "max": 366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab2e26c45b58452cad300c478a0ae4ab",
            "value": 366
          }
        },
        "4d38dc64e46e4853ab2ce135a07444cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55713837e67644499435fbaeecb1f877",
            "placeholder": "​",
            "style": "IPY_MODEL_84f6c9520776453a97ee36b694dad165",
            "value": " 366/366 [00:00&lt;00:00, 20.2kB/s]"
          }
        },
        "c615b438e5d4432492fdb4e2f9ff067c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97da8739054246e1ae252fbd0305660c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ae8d0b0feb547efab1fff75cd0bbc8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77416f5caa194a5c87603bdf34fd3055": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab2e26c45b58452cad300c478a0ae4ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55713837e67644499435fbaeecb1f877": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84f6c9520776453a97ee36b694dad165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2f17eab22214f628ecc5b951698bb97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b9b5f5053594e06a99d65359d396636",
              "IPY_MODEL_b302b5d2e7004dddbfaa4f12138c2be4",
              "IPY_MODEL_4df6a21febd748ff8055e5f69c7fb305"
            ],
            "layout": "IPY_MODEL_4201a207e32e4917ad3bc5d6937b4711"
          }
        },
        "0b9b5f5053594e06a99d65359d396636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1c866bd810744d9a918ecc2770fc01d",
            "placeholder": "​",
            "style": "IPY_MODEL_4b756ade93734b0aa8a0c84fcb81e1a9",
            "value": "vocab.txt: 100%"
          }
        },
        "b302b5d2e7004dddbfaa4f12138c2be4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2250370b18614ef1bf1b387ef49334d6",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8321d9dfd8745bab585a4f244d020b2",
            "value": 231508
          }
        },
        "4df6a21febd748ff8055e5f69c7fb305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b91620940ed482882d668b9509ab1ab",
            "placeholder": "​",
            "style": "IPY_MODEL_e451595505f7415d84645dfcb9de7ab0",
            "value": " 232k/232k [00:00&lt;00:00, 1.91MB/s]"
          }
        },
        "4201a207e32e4917ad3bc5d6937b4711": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1c866bd810744d9a918ecc2770fc01d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b756ade93734b0aa8a0c84fcb81e1a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2250370b18614ef1bf1b387ef49334d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8321d9dfd8745bab585a4f244d020b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b91620940ed482882d668b9509ab1ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e451595505f7415d84645dfcb9de7ab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e43c1c7d7e6a4a57a7382f0557780854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a02450fc54184b92ae93e7bc089fcfe7",
              "IPY_MODEL_5ca4ca5b36d3465d9db02312a8a4f868",
              "IPY_MODEL_176990a933804e5cb69535d016af36a1"
            ],
            "layout": "IPY_MODEL_0ea66ad5dace48eab19caa0282d75421"
          }
        },
        "a02450fc54184b92ae93e7bc089fcfe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e3baab3c6934b0aa6cc406aaa7ba3fc",
            "placeholder": "​",
            "style": "IPY_MODEL_18814018052b4aa0a0e2d9c420383420",
            "value": "tokenizer.json: 100%"
          }
        },
        "5ca4ca5b36d3465d9db02312a8a4f868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87e0ff78b52644b794fd14fdb45d0b7b",
            "max": 711396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cfadd1d701942caae5507eff1760d20",
            "value": 711396
          }
        },
        "176990a933804e5cb69535d016af36a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ec7b378d1814759b97cc1b1d716c986",
            "placeholder": "​",
            "style": "IPY_MODEL_952b1e1787764ca588d5fde9b243ef5a",
            "value": " 711k/711k [00:00&lt;00:00, 10.9MB/s]"
          }
        },
        "0ea66ad5dace48eab19caa0282d75421": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e3baab3c6934b0aa6cc406aaa7ba3fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18814018052b4aa0a0e2d9c420383420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87e0ff78b52644b794fd14fdb45d0b7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cfadd1d701942caae5507eff1760d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ec7b378d1814759b97cc1b1d716c986": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "952b1e1787764ca588d5fde9b243ef5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1127cc8a58234881b97a61e9e78e7a37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1a4f1def60a4f9abb7dab588e7e7c58",
              "IPY_MODEL_2320889cda2d4f8691e7754b1b8b02da",
              "IPY_MODEL_1f128e18c2ee43c0a0c1bfe94326f129"
            ],
            "layout": "IPY_MODEL_cf94016c78b1451b98dd46e30e74566a"
          }
        },
        "e1a4f1def60a4f9abb7dab588e7e7c58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfa63e998e8242e296cea0f487ff558d",
            "placeholder": "​",
            "style": "IPY_MODEL_e06da15fc94a48d0998a47ed6aacf2dc",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "2320889cda2d4f8691e7754b1b8b02da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40404e8581744d87b0ecafef024c6eb3",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4378384bcb304e9faef47469cd381158",
            "value": 125
          }
        },
        "1f128e18c2ee43c0a0c1bfe94326f129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fe57d2d9cc343428ee5a9c41a22af40",
            "placeholder": "​",
            "style": "IPY_MODEL_3d582e6039ef430b9cf45e80440e1f52",
            "value": " 125/125 [00:00&lt;00:00, 8.18kB/s]"
          }
        },
        "cf94016c78b1451b98dd46e30e74566a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfa63e998e8242e296cea0f487ff558d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e06da15fc94a48d0998a47ed6aacf2dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40404e8581744d87b0ecafef024c6eb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4378384bcb304e9faef47469cd381158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8fe57d2d9cc343428ee5a9c41a22af40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d582e6039ef430b9cf45e80440e1f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5dce8892b05a4013880fe0c2d472ab29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ce2fece8a3e486ebc516cbfb2db3a47",
              "IPY_MODEL_db65385a84f7405987f87966363d1afc",
              "IPY_MODEL_bbfaaf83c135445ab66f29c968297cac"
            ],
            "layout": "IPY_MODEL_43c75f0ce6fb4702a5bbc75480db098a"
          }
        },
        "3ce2fece8a3e486ebc516cbfb2db3a47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f409487512b44edcb945319116cf118c",
            "placeholder": "​",
            "style": "IPY_MODEL_6c7fbe83569a409fb1d105fb9216658c",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "db65385a84f7405987f87966363d1afc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43e22b9696ab4cea948d598601d8ed57",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1de627970a1545848f30c89fb56763da",
            "value": 190
          }
        },
        "bbfaaf83c135445ab66f29c968297cac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_213b5ba84f5a4be3969a00de487647ca",
            "placeholder": "​",
            "style": "IPY_MODEL_1b3d39cf72a54367944301f770d3ed58",
            "value": " 190/190 [00:00&lt;00:00, 7.42kB/s]"
          }
        },
        "43c75f0ce6fb4702a5bbc75480db098a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f409487512b44edcb945319116cf118c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c7fbe83569a409fb1d105fb9216658c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43e22b9696ab4cea948d598601d8ed57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1de627970a1545848f30c89fb56763da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "213b5ba84f5a4be3969a00de487647ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b3d39cf72a54367944301f770d3ed58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing required Libraries"
      ],
      "metadata": {
        "id": "RJm-lnfUGojB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ienaB-RIGGpc",
        "outputId": "ab27f145-2369-4d4d-8434-d9443011cd00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting beyondllm\n",
            "  Downloading beyondllm-0.2.3-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting youtube_transcript_api\n",
            "  Downloading youtube_transcript_api-0.6.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting llama-index-readers-youtube-transcript\n",
            "  Downloading llama_index_readers_youtube_transcript-0.3.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting llama_index.embeddings.huggingface\n",
            "  Downloading llama_index_embeddings_huggingface-0.5.0-py3-none-any.whl.metadata (767 bytes)\n",
            "Collecting llama-index==0.10.27 (from beyondllm)\n",
            "  Downloading llama_index-0.10.27-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-embeddings-gemini==0.1.6 (from beyondllm)\n",
            "  Downloading llama_index_embeddings_gemini-0.1.6-py3-none-any.whl.metadata (660 bytes)\n",
            "Collecting nltk==3.8.1 (from beyondllm)\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from beyondllm) (1.26.4)\n",
            "Collecting openai==1.20.0 (from beyondllm)\n",
            "  Downloading openai-1.20.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pandas==2.0.3 (from beyondllm)\n",
            "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting pydantic<2,>=1.10.5 (from beyondllm)\n",
            "  Downloading pydantic-1.10.20-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/153.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf==4.2.0 (from beyondllm)\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting pysbd==0.3.4 (from beyondllm)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting pyyaml==6.0.1 (from beyondllm)\n",
            "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting regex==2024.4.16 (from beyondllm)\n",
            "  Downloading regex-2024.4.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sqlalchemy==2.0.29 (from beyondllm)\n",
            "  Downloading SQLAlchemy-2.0.29-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting tiktoken==0.6.0 (from beyondllm)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index==0.10.27->beyondllm)\n",
            "  Downloading llama_index_agent_openai-0.2.9-py3-none-any.whl.metadata (729 bytes)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index==0.10.27->beyondllm)\n",
            "  Downloading llama_index_cli-0.1.13-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.27 (from llama-index==0.10.27->beyondllm)\n",
            "  Downloading llama_index_core-0.10.68.post1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index==0.10.27->beyondllm)\n",
            "  Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl.metadata (655 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index==0.10.27->beyondllm)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index==0.10.27->beyondllm)\n",
            "  Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index==0.10.27->beyondllm)\n",
            "  Downloading llama_index_llms_openai-0.1.31-py3-none-any.whl.metadata (650 bytes)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index==0.10.27->beyondllm)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index==0.10.27->beyondllm)\n",
            "  Downloading llama_index_program_openai-0.1.7-py3-none-any.whl.metadata (760 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index==0.10.27->beyondllm)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index==0.10.27->beyondllm)\n",
            "  Downloading llama_index_readers_file-0.1.33-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index==0.10.27->beyondllm)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting google-generativeai<0.5.0,>=0.4.1 (from llama-index-embeddings-gemini==0.1.6->beyondllm)\n",
            "  Downloading google_generativeai-0.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->beyondllm) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->beyondllm) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->beyondllm) (4.67.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.20.0->beyondllm) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.20.0->beyondllm) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.20.0->beyondllm) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.20.0->beyondllm) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.20.0->beyondllm) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->beyondllm) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->beyondllm) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->beyondllm) (2024.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy==2.0.29->beyondllm) (3.1.1)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.6.0->beyondllm) (2.32.3)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from youtube_transcript_api) (0.7.1)\n",
            "INFO: pip is looking at multiple versions of llama-index-readers-youtube-transcript to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-index-readers-youtube-transcript\n",
            "  Downloading llama_index_readers_youtube_transcript-0.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading llama_index_readers_youtube_transcript-0.1.4-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (0.27.1)\n",
            "INFO: pip is looking at multiple versions of llama-index-embeddings-huggingface to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama_index.embeddings.huggingface\n",
            "  Downloading llama_index_embeddings_huggingface-0.4.0-py3-none-any.whl.metadata (767 bytes)\n",
            "  Downloading llama_index_embeddings_huggingface-0.3.1-py3-none-any.whl.metadata (718 bytes)\n",
            "  Downloading llama_index_embeddings_huggingface-0.3.0-py3-none-any.whl.metadata (769 bytes)\n",
            "  Downloading llama_index_embeddings_huggingface-0.2.3-py3-none-any.whl.metadata (769 bytes)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from llama_index.embeddings.huggingface) (3.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (24.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (3.11.11)\n",
            "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm) (1.2.15)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm) (3.4.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm) (11.1.0)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.6.0->beyondllm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.6.0->beyondllm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.6.0->beyondllm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.6.0->beyondllm) (2024.12.14)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (4.47.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (1.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (1.18.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.20.0->beyondllm) (1.2.2)\n",
            "Collecting google-ai-generativelanguage==0.4.0 (from google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm)\n",
            "  Downloading google_ai_generativelanguage-0.4.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (2.27.0)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (2.19.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (4.25.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.4.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (1.25.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.20.0->beyondllm) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.20.0->beyondllm) (0.14.0)\n",
            "Collecting llamaindex-py-client<0.2.0,>=0.1.19 (from llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2->llama-index==0.10.27->beyondllm)\n",
            "  Downloading llamaindex_py_client-0.1.19-py3-none-any.whl.metadata (760 bytes)\n",
            "INFO: pip is looking at multiple versions of llama-index-llms-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index==0.10.27->beyondllm)\n",
            "  Downloading llama_index_llms_openai-0.1.30-py3-none-any.whl.metadata (650 bytes)\n",
            "  Downloading llama_index_llms_openai-0.1.29-py3-none-any.whl.metadata (650 bytes)\n",
            "  Downloading llama_index_llms_openai-0.1.28-py3-none-any.whl.metadata (650 bytes)\n",
            "  Downloading llama_index_llms_openai-0.1.27-py3-none-any.whl.metadata (610 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.27->beyondllm) (4.12.3)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.27->beyondllm)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.27->beyondllm)\n",
            "  Downloading llama_parse-0.5.19-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3->beyondllm) (1.17.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (0.5.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.27->llama-index==0.10.27->beyondllm)\n",
            "  Downloading marshmallow-3.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (3.5.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.27->beyondllm) (2.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (1.66.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (4.9)\n",
            "INFO: pip is looking at multiple versions of llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.27->beyondllm)\n",
            "  Downloading llama_parse-0.5.18-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading llama_parse-0.5.17-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading llama_parse-0.5.16-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading llama_parse-0.5.15-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading llama_parse-0.5.14-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading llama_parse-0.5.13-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading llama_parse-0.5.12-py3-none-any.whl.metadata (6.9 kB)\n",
            "INFO: pip is still looking at multiple versions of llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading llama_parse-0.5.11-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading llama_parse-0.5.10-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading llama_parse-0.5.9-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading llama_parse-0.5.8-py3-none-any.whl.metadata (6.4 kB)\n",
            "  Downloading llama_parse-0.5.7-py3-none-any.whl.metadata (6.4 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading llama_parse-0.5.6-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading llama_parse-0.5.5-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading llama_parse-0.5.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading llama_parse-0.5.3-py3-none-any.whl.metadata (4.5 kB)\n",
            "  Downloading llama_parse-0.5.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "  Downloading llama_parse-0.5.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "  Downloading llama_parse-0.5.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "  Downloading llama_parse-0.4.9-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (3.0.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (1.69.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (1.62.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini==0.1.6->beyondllm) (0.6.1)\n",
            "Downloading beyondllm-0.2.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index-0.10.27-py3-none-any.whl (6.9 kB)\n",
            "Downloading llama_index_embeddings_gemini-0.1.6-py3-none-any.whl (2.9 kB)\n",
            "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.20.0-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.4.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.29-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading youtube_transcript_api-0.6.3-py3-none-any.whl (622 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_youtube_transcript-0.1.4-py3-none-any.whl (3.7 kB)\n",
            "Downloading llama_index_embeddings_huggingface-0.2.3-py3-none-any.whl (8.6 kB)\n",
            "Downloading llama_index_core-0.10.68.post1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-1.10.20-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading google_generativeai-0.4.1-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_ai_generativelanguage-0.4.0-py3-none-any.whl (598 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m598.7/598.7 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_agent_openai-0.2.9-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.1.13-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl (6.3 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
            "Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.1.27-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.1.7-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.1.33-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading llama_parse-0.4.9-py3-none-any.whl (9.4 kB)\n",
            "Downloading llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.25.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Installing collected packages: striprtf, dirtyjson, tenacity, sqlalchemy, regex, pyyaml, pysbd, pypdf, pydantic, mypy-extensions, marshmallow, youtube_transcript_api, typing-inspect, tiktoken, pandas, nltk, openai, llamaindex-py-client, dataclasses-json, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-youtube-transcript, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, google-ai-generativelanguage, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama_index.embeddings.huggingface, llama-index-cli, llama-index-agent-openai, google-generativeai, llama-index-program-openai, llama-index-embeddings-gemini, llama-index-question-gen-openai, llama-index, beyondllm\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.36\n",
            "    Uninstalling SQLAlchemy-2.0.36:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.36\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.10.4\n",
            "    Uninstalling pydantic-2.10.4:\n",
            "      Successfully uninstalled pydantic-2.10.4\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.59.4\n",
            "    Uninstalling openai-1.59.4:\n",
            "      Successfully uninstalled openai-1.59.4\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.10\n",
            "    Uninstalling google-ai-generativelanguage-0.6.10:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.10\n",
            "  Attempting uninstall: google-generativeai\n",
            "    Found existing installation: google-generativeai 0.8.3\n",
            "    Uninstalling google-generativeai-0.8.3:\n",
            "      Successfully uninstalled google-generativeai-0.8.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 1.4.20 requires pydantic>=2.7.0, but you have pydantic 1.10.20 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.3 which is incompatible.\n",
            "google-genai 0.3.0 requires pydantic<3.0.0dev,>=2.0.0, but you have pydantic 1.10.20 which is incompatible.\n",
            "langchain 0.3.14 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.20 which is incompatible.\n",
            "langchain-core 0.3.29 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.10.20 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "wandb 0.19.1 requires pydantic<3,>=2.6, but you have pydantic 1.10.20 which is incompatible.\n",
            "xarray 2025.1.0 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed beyondllm-0.2.3 dataclasses-json-0.6.7 dirtyjson-1.0.8 google-ai-generativelanguage-0.4.0 google-generativeai-0.4.1 llama-index-0.10.27 llama-index-agent-openai-0.2.9 llama-index-cli-0.1.13 llama-index-core-0.10.68.post1 llama-index-embeddings-gemini-0.1.6 llama-index-embeddings-openai-0.1.11 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48.post4 llama-index-llms-openai-0.1.27 llama-index-multi-modal-llms-openai-0.1.9 llama-index-program-openai-0.1.7 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.33 llama-index-readers-llama-parse-0.1.6 llama-index-readers-youtube-transcript-0.1.4 llama-parse-0.4.9 llama_index.embeddings.huggingface-0.2.3 llamaindex-py-client-0.1.19 marshmallow-3.25.0 mypy-extensions-1.0.0 nltk-3.8.1 openai-1.20.0 pandas-2.0.3 pydantic-1.10.20 pypdf-4.2.0 pysbd-0.3.4 pyyaml-6.0.1 regex-2024.4.16 sqlalchemy-2.0.29 striprtf-0.0.26 tenacity-8.5.0 tiktoken-0.6.0 typing-inspect-0.9.0 youtube_transcript_api-0.6.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "cab179462b274145a31b1a3fb9fedd69"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install beyondllm youtube_transcript_api llama-index-readers-youtube-transcript llama_index.embeddings.huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting environment variables"
      ],
      "metadata": {
        "id": "Bf9negQQIOS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "hf_token = getpass('Enter Hugging Face Token: ')\n",
        "google_api_key = getpass('Enter Google API Key: ')\n",
        "os.environ['HUGGINGFACE_TOKEN'] = hf_token\n",
        "os.environ['GOOGLE_API_KEY'] = google_api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g7Y8SOjIJyK",
        "outputId": "3211572f-e47b-4085-ff06-ca9a262b4d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Hugging Face Token: ··········\n",
            "Enter Google API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from beyondllm import source, retrieve,embeddings,llms,generator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aavUXAuzIt6R",
        "outputId": "0a4a1e49-a655-45dd-994b-d264afb1f6a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /usr/local/lib/python3.10/dist-\n",
            "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Youtube video"
      ],
      "metadata": {
        "id": "xe2ckk6cJ15P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data =source.fit(\n",
        "    path ='https://www.youtube.com/watch?v=ZM1bdh2mDJQ',\n",
        "    dtype='youtube',\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kQiAOiMJ5PP",
        "outputId": "70c9cec0-e7fb-4302-a378-bc5b4ab37605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://www.youtube.com/watch?v=ZM1bdh2mDJQ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWBM_tozKhvR",
        "outputId": "3485128c-0ad2-4853-ca8f-c6b2c21ce62e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TextNode(id_='7a72c8df-abe5-49e3-90a6-2356aadf61a3', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec')}, text=\"hello guys welcome back to my Channel\\ntoday we are going to explore flesh\\nattention now we are going to fles\\nexplore flesh attention from first\\nprinciple which means that not only we\\nwill code flesh attention we will\\nactually derive it so we pretend that\\nthe paper flesh attention paper never\\nexisted and we look at the attention\\ncompetition and we look at the problem\\nit has and we try to solve it step by\\nstep pretending that flashh attention\\nnever existed this will give us a deep\\nunderstanding of how it works and also\\nwe will combine Theory with practice\\npractice because we will code it now in\\norder to code flash attention we we will\\nneed to write a kernel for our GPU and\\nin our specific case I will be using an\\nNvidia GPU so Auda kernel but instead of\\nwriting C++ code we will use Tron which\\nis a a way of converting python directly\\ninto um Cuda uh kernels that can run\\ndirectly on the GPU and Tron you can\\nthink of it as a compiler that takes in\\nPython and converts it into something\\nthat can run on the GPU um so let's look\\nat the topics for today first of all I\\nwill give an introduction to multi-ad\\nattention because we need to look at\\nwhat is attention and how it's computed\\nand what are the problems in Computing\\nthis attention then we will look at\\nactually the most critical part of the\\nattention computation is this softmax\\nand how it impacts the computation and\\nuh complexity we will look at what is\\nonline softmax then we will explore what\\nis the GPU because we are going to write\\na kernel that will run on the GPU so we\\nneed to understand what is the\\ndifference for example the CPU and the\\nGPU and what is the kernel and how it\\ndiffer from a normal program that you\\nwrite for the CPU we will look at how\\ntensors are layout in memory so row\\nmajor layout column major out Etc\\nstrides um we are going to look at block\\nmatrix multiplication Tron software\\npipeline all the optimization that Tryon\\ndoes to our code finally we will be able\\nto code the flash attention forward pass\\nbut of course we are not satisfied only\\nby coding the forward pass we also want\\nto code the backward pass but in order\\nto code the backward pass we also need\\nto understand how out works and the\\ngradient descent works in the case of\\ncustom operations so we need to\\nunderstand what are derivatives what are\\ngradients what are jauan and then we\\ncalculate the gradient of the common\\noperations that we use in Flash\\nattention and finally we will have\\nenough knowledge to code the backward\\npass for this reason this video is going\\nto be super long but I hope you don't\\nmind because we are going to learn a lot\\nof course you may be wondering all of\\nthis requires a lot of knowledge that\\nyou may not have but that's not a\\nproblem because that's my problem\\nbecause in this video I will make sure\\nthat if you only have high school\\ncalculus so you know what are\\nderivatives you have basics of linear\\nalgebra like you know what is matrix\\nmultiplication or what is the transpose\\nof a matrix and you have a basic\\nknowledge of attention mechanism so like\\nfor example you have watched my previous\\nvideo on the uh attention is all you\\nneed paper and you have a lot of patient\\nthat should be enough to understand all\\nof this video because all the topics\\nthat I will introduce I will always\\nintroduce them in such a way that I um\\npretend that you don't know anything\\nabout that topic so so we try to derive\\neverything from first principle\\neverything from scratch okay now that we\\nhave seen the introduction let's go see\\nthe first part of the video which is the\\nmultihead attention all right let's talk\\nabout multi-head attention now I am\\nusing the slides from my previous video\\nattention is all you need so uh we can\\nlook at very fast at what multi-head\\nattention is and how it works I hope you\\nremember the formula soft Max of the\\nquery multiplied by the transpose of the\\nkey divide by DK all multiplied by V\\nbecause we will be using that a lot\\nthroughout the video now multi-ad\\nattention starts from an input sequence\\nor two input sequence in case we are\\ntalking about cross attention in the\\nsimple case of self attention we have\\none input sequence which is a sequence\\nof in the case of language Model A\\nsequence of tokens where we have sec\\nnumber of tokens and each token is\\nrepresented by an embedding so a vector\\nwith the model Dimensions the first\\nthing that we do is we um convert this\\nuh input sequence into query key and\\nvalues through three linear projections\\none called WQ one called\", mimetype='text/plain', start_char_idx=0, end_char_idx=4469, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='461ef597-1feb-45c9-88da-a7679a6c032b', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='7a72c8df-abe5-49e3-90a6-2356aadf61a3', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='762a7deae31f65367847ba71921fe4e45d4a2a924e369745e873330ccdc4d47b')}, text=\"WK one called\\nWV which in pytorch are represented\\nthrough linear layers and these linear\\nlayers are of D model by D model so they\\ndo not change the shape of the input\\ntensor and\\nthe after we do this job of projecting\\nthem they become three different\\nsequences one called qu one called key\\nand one called value so here I'm calling\\nthem Q Prime K Prime and V Prime then we\\ndivide them into smaller embeddings so\\neach of this token which is made up of\\nthe model Dimensions we divide it into\\nsmaller tokens each one suppose we have\\nfour heads each one will have the model\\ndivided by four dimensions so this one\\nis a sequence of tokens where each token\\nis not the entire token but a part of\\nthe embedding of each token and this one\\nis a another part of the embedding of\\nthe tokens and this one is another part\\nof the embedding of the token Etc and we\\ndo this job for the query key and value\\nsequence then we compute the attention\\nas follows so the soft Marx of the query\\nmultiplied by the transpose of the key\\ndivide by the um the square root of DK\\nwhere DK is the dimension of each head\\nso how many dimensions each head is\\nworking with and then we do the\\nmultiplication with v and this will give\\nus the output of the attention mechanism\\nfor each head and this job is done\\nindependently for each head this should\\nbe clear to you if it's not please watch\\nmy previous video on the attention\\nmechanism because we will be working\\nwith this uh scenario a\\nlot now uh then we take this o this uh\\nthe output of each head and then we\\nconcatenate it back in order to get the\\nrepresentation of each token uh as a\\nfull embedding so before we split this\\nembedding into smaller embeddings this\\none here it's called the q1 Q2 Q3 Q4\\nthen after we compute the attention we\\nget back um the output of each head and\\nwe concut it together to get back the\\nfull embedding Dimension which is this\\nedge here we run it through another\\nlinear projection called wo which will\\nbe the output of the multi head\\nattention now flesh attention is not\\nconcerned with all of these operations\\nactually flesh attention is only\\nconcerned with the operation that\\nrequire uh optimization and the\\noperations that require optimizations\\nare this one so the soft Marx of the\\nquery multiplied by the transpose of the\\nkey divide by the square root of DK\\nmultiplied by V which means that the\\nprojection of the input sequence through\\nWQ WK and WV is not something that\\nflashh attention is concerned about\\nbecause that's a matrix multiplication\\nso when you use a linear layer it's just\\na matrix multiplication of the inut with\\nthe weight Matrix of the linear layer\\nand this kind of operation so the matrix\\nmultiplication is one of the most um\\noptimized operation that we have in the\\nGPU because the manufacturer of the GPU\\nusually also releases um the necessary\\nlibrary for computing the the the metrix\\nmultiplication so actually these are\\nquite fast and they do not require any\\noptimization so flesh attention will\\npretend that the query is has already\\nbeen passed through by WQ and the key\\nhas already passed through WK and the V\\nhas already passed from WB moreover\\nflash attention will not be concerned\\nwith the projection with wo because\\nthat's also a matrix multiplication\\nbecause the wo is always represented in\\nbyor as a linear layer so it's a matrix\\nmultiplication and matrix multiplication\\nas we have seen are very um optimized so\\nthere is nothing to optimize there uh\\nbut what we need to optimize in terms of\\nspeed is this operation here soft marks\\nof the query multiply by transpose of\\nthe keys by the square root of DK\\nmultiplied by V all right guys so now we\\nhave rehearsed what is multi head\\nattention I also want to give you a lot\\nof visualization which is\\nbasically here in the paper of the multi\\nhead attention we can see that we have\\nthe input that is uh v um K and Q so q k\\nand V each of them runs through a linear\\nlayer which is the WQ WK and\\nWV um then we do the scale dot product\\nattention which is done independently\\nfor each head so each head will do query\\nmultipli by the transpose of the key\\ndivide by the square root of DEC where\\neach query and each key is not the full\\nembedding of each token but a part of\\nthe embedding of the token because we\\nsplit them into smaller embeddings and\\neventually we take all the output of\\neach of these head which are computed in\\nparallel so that's why you see this\\nDimension Edge in the depth we\\nconcatenate them and then we run\", mimetype='text/plain', start_char_idx=4470, end_char_idx=8904, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='47153776-e199-452c-952b-b3623f99b9c5', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='461ef597-1feb-45c9-88da-a7679a6c032b', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='2764b92b5f6805246801b4da3e0b5a7063d79b57e69fd57fcfe4b470a6f86897')}, text=\"them\\nthrough\\nwo uh what are we we concerned with we\\nare concerned with optimizing this\\nparticular block here the scaled do\\nproduct attention so let's start our\\njourney one thing that is very important\\nto understand is why do we even need a\\nbetter implementation of the attention\\nmechanism and if you look at the flashh\\nattention paper you will notice The\\nFollowing part this is the paper flashh\\nattention one and in the flashh\\nattention one paper they describe the\\nattention Implement imple implementation\\nas it's done naively when using pytorch\\nso first we do the multiplication of the\\nquery multiplied by the transpose of the\\nkeys then we apply the softmax to the\\noutput of this operation and finally we\\nmultiply the output of the softmax with\\nthe V Matrix to obtain the output of the\\nattention the way this implementation is\\ndone by pytorch without any optimization\\nis as follows so we load the first of\\nall these tensors are residing in the\\nGPU the GPU is made up of two main\\nmemories one is called the hbm which is\\nthe dram which is the the the ram of the\\nGPU which is the 40 GB of the a100 for\\nexample so it's the biggest memory that\\nwe have in the GPU and then uh there are\\nthere is the shared memory so the\\nproblem of the GPU is that accessing\\nthis hbm so the global it's also called\\nthe global memory it's very very slow\\ncompared to the shared memory however\\nthe shared memory it's much much much\\nsmaller compared to the hbm and what\\nthey claim in the flesh attention paper\\nis that the operation of the attention\\nis IO bound meaning that if we keep\\naccessing the um the um Global memory\\nthe overall uh operation of computing\\nthe attention is not because Computing\\nall these operations it's slow but\\nbecause we keep accessing the global\\nmemory which is slow so we call these\\nkind of operations iob bound so the only\\nway to improve this situation is to\\ncompute the attention inside the shared\\nmemory of the GPU which is much smaller\\nwhich is much closer to the cores that\\nactually do the computation so we we\\nwill need to kind of also split the\\nattention computation into smaller\\nblocks that can reside in the shared\\nmemory and we will see later in how this\\nis possible through block matrix\\nmultiplication and this is in the paper\\nhere they call it the tiling and it's a\\nvery um uh how to say use the technique\\nwhen doing um when writing kernels for\\nthe GPU uh which are usually involve\\nsome kind of metrix multiplication so\\nnow we know what problem the flashh\\nattention is trying to solve it's trying\\nto make sure that we do not need to\\naccess the hbm so the high bandwidth\\nmemory when Computing the uh attention\\nbut copying only a part of each Matrix\\ninside the local memory so the sh shared\\nmemory of the GPU that is closer to the\\ncourse and Computing a part of the\\noutput Matrix there then copying that\\npart to the output in that is residing\\nin the hbm and keep doing it for all the\\nblocks in which we can divide these\\nquery key and value matrices and later\\nwe will see how this blocked computation\\nis done but also we will see that the\\nbiggest problem in Computing this\\nblocked computation is the softmax\\nbecause the softmax needs to access\\nall the row of the S Matrix to apply the\\nsoft Max\\nbecause uh the the soft Max needs to\\nhave a normalization factor which is the\\nsum of all the exponentials of all the\\nvalues to which it is applied rowwise\\nand we will see later how we will solve\\nthis problem so let's move\\non all right guys um okay when I say\\nguys I mean guys and girls because I\\ndon't know in my usually I just say guys\\nto you know but please girls don't feel\\nexcluded so we saw that FL fall flashh\\nattention is only concerned in\\noptimizing this soft Max of the\\ntranspose of soft Max of the query\\nmultip by three divide by the square\\nroot of DK multiplied by V and we need\\nto introduce a little bit of notation so\\nthat we don't get lost um in the future\\nslides first of all this is the formulas\\nI took from the flashh attention paper\\nbut for now we Let's Pretend flashh\\nattention never existed so we are trying\\nto solve the the problem step by step\\nnow um we should treat this Q as\\nsomething that has as the sequence that\\nis the output of the input sequence that\\nhas already passed through WQ the K as\\nsomething that has already passed\\nthrough WK and v as something that has\\nalready passed through WV because we\\ndon't want to optimize the matrix\\nmultiplication because it's already fast\\nenough\", mimetype='text/plain', start_char_idx=8905, end_char_idx=13318, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='274a8650-1a3f-4d04-87b2-77198e8918c7', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='47153776-e199-452c-952b-b3623f99b9c5', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='0f134f6d570086961c2dc4aa5247faefe17d538d6be3b4a1d3025bf8872baac7')}, text=\"another thing is let's talk about\\nwhat are the dimensions of these\\nmatrices so we can then understand what\\nis the the dimensions of the output of\\nthese operations we will see treat Q as\\na sequence of tokens with end tokens so\\nn tokens where each token is D has D\\nDimension so lowercase D Dimensions why\\nbecause usually we take the queries and\\nthen we split them into multiple heads\\nso we have we pretend we have already\\ndone this splitting so we pretend we\\nalready took our input sequence we\\nalready run it through WQ and then we\\nhave already splited into multiple heads\\nand each of these head will do the\\nfollowing operation so the the one we\\nalready saw and so the usual formula\\nwhere multiply the transpose of the keys\\nand each of this head will work with\\nthis dimensions for the query for the\\nkey and for the value sequence so now\\nlet's look at the uh the the dimensions\\nof the output so the first operation\\nthat we will do is the query multiply by\\nthe transpose of the keys where the\\ntranspose of the keys is is a matrix\\nthat originally is n by D but become but\\nwith the transpose will be D by n so d\\nby n and the result will be a matrix\\nthat is n byn because in a matrix\\nmultiplication the outer Dimensions\\nbecome the dimension of the output\\nMatrix what do we what is the next\\noperation that we do we take the output\\nof this operation so the query multiply\\nby transpose of the keys and we run it\\nthrough a soft Max operation and we will\\nsee what is the soft Max\\noperation which preserves the shape of\\nthe input so it doesn't change the shape\\nof the input Matrix it just changes the\\nvalues of it and then we take the output\\nof the softmax and we multiply it by V\\nwhich will change the um which will uh\\nchange the of course the shape because\\nthe p Matrix is n by n so this one is n\\nby n and V is um n by D so this one the\\noutput will be n byd the outer\\ndimensions of this matrix\\nmultiplication now let's look at the\\ndetails of each of these operations so\\nwhen we do query multiply by transpose\\nof the keys we will get a matrix that is\\nn byn where each value in this Matrix is\\na dotproduct\\nof a row of q and a column of K in\\nparticular the first element of this\\nMatrix will be the dot product of the\\nfirst query with the first key Vector\\nthe second element will be the\\ndotproduct of the first query with the\\nsecond key vector and the third element\\nwill be the first quy with the third key\\netc etc and the the let's say the the\\nlast row of this Matrix will be the dot\\nproduct of the last query with the first\\nkey then the last query with the second\\nkey the last query with the third key\\netc etc until the last query with the\\nlast key um you may also notice that\\nhere I have written query transpose the\\nkey because when we um what is q1 first\\nof all q1 is the first r of the query\\nMatrix so little bit of um background on\\nmatrix multiplication so we know that\\nwhen we do matrix multiplication each\\noutput element is one row of the first\\nMatrix with one column of the second\\nMatrix but we are doing the product of\\nthe first Matrix with the transpose of\\nthe second so it will be the do product\\nof the one row of the query Matrix with\\none row of the key Matrix because we are\\ndoing the multiplication with key uh K\\ntranspose\\num when you take a vector from a matrix\\nthe usual notation so the in as as in\\nhow to say in um in in mathematics in a\\nlinear algebra we always pretend that a\\nvector is a column Vector so we cannot\\njust write Q multiplied by K because\\nthat would be mean uh that would mean we\\nare doing the dot product of uh we are\\ndoing the kind of the matrix\\nmultiplication of one column Matrix with\\none column Matrix that is not possible\\nbecause the shapes do not match so as a\\nnotation we write that we do the dot\\nproduct of the first Matrix transpose\\nwhich is a column Vector but we\\ntranspose it so it becomes a row Vector\\nwith the second Vector this is just\\nbecause of notation guys so um you just\\nneed to pretend that this is the first\\nquery with the first key then the first\\nquery with the second key the first\\nquery with the third key etc etc etc so\\nwe are doing do of vectors then we apply\\nthis softmax\\noperation the softmax operation what it\\nwill do it will transform each of these\\ndot products which are scalars so the\\noutput of a DOT product is a scalar and\\nit will transform each of these numbers\\nin such a way\", mimetype='text/plain', start_char_idx=13319, end_char_idx=17648, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='fc804597-e91f-474a-a4a2-cb981da83d61', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='274a8650-1a3f-4d04-87b2-77198e8918c7', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='fd10a798df7613e3c676e372f6c03843418f30ed446631e8741268add159e3e0')}, text=\"that they become kind of a\\nprobability distribution rowwise which\\nmeans that each of these numbers is\\nbetween 0er and one and when we sum up\\nthese numbers together they are sum up\\nto one and this condition this property\\nwill be valid for each row so this row\\nalso will sum up to one this row will\\nsum up to one and this row will sum up\\nto one etc etc\\netc let's see what is the softmax\\noperation now uh given a vector so let's\\ncall it X which is made up of n\\nDimensions the softmax is defined as\\nfollows so it is the um the soft Max\\nbasically transforms this transforms\\nthis Vector into another Vector with the\\nsame Dimension where each item of the\\noutput Vector is calculated as follows\\nso the E element of the output Vector is\\nthe exponential of the element input\\nelement divided by the summation of all\\nthe exponentials of all the dimensions\\nof the\\nvector basically this is called the\\nnormalization factor to make it all\\nthese numbers between zero and one we\\nusually normalize that's why it's called\\nthe normalization factor and uh we use\\nthe soft Max because we want each of\\nthese numbers to be positive we don't\\nwant the stuff the output of this\\noperation to be negative so that's why\\nwe use the exponential but there is a\\nproblem the problem is Imagine our input\\nVector is made up of many numbers that\\nare maybe large so for example let's say\\nX1 is equal to 100 X2 is equal to 200 X3\\nis equal to 300 which is can happen um\\nif we do the exponential of this number\\nso the exponential of 100 that is going\\nto be a huge number it's going to very\\nclose to Infinity uh at least compared\\nto what we can store in a computer so\\nthe output of exponential of 100 may not\\nfit into a floating Point 32 or a\\nfloating Point 16 number or even an\\ninteger of 32 bit so we cannot compute\\nit because it will overflow our uh our\\nvariable our integer that is storing\\nthis value this\\noutput so we talk in this case about\\nnumerical instability so every time you\\nhear the term numerical instability in\\ncomputer science it means that the\\nnumber cannot be represented within a\\nfixed representation with the bits we\\nhave available which are usually 32 bit\\nor 16 bit uh we have also 64bit but that\\nwould be too expensive to use um so\\nlet's try to find a solution to make\\nthis stuff here computable and\\nnumerically\\nstable in order to make this soft Max\\noperation numerically stable which means\\nthat we want these numbers to not\\nexplode or to become too small that they\\nare not representable we need to find a\\nsolution and luckily it's quite easy so\\nthe softmax as we have seen before it is\\nthe following formula so each number is\\nexponentiated and then we divide it by\\nthis normalization factor which is just\\nthe sum of the exponential of each input\\ndimension of the input\\nVector um if we multiply the numerator\\nand the denominator of a fraction with a\\nconstant uh with a number number then\\nthe fraction will not change so that's\\nwhat we are going to do we are\\nmultiplying the numerator and this\\ndenominator with this factor C as long\\nas C is not equal to zero of course uh\\nthen we can U take this C and by using\\nthe the distributive property of the\\nproduct with respect to the sum we can\\nbring this C inside of the summation as\\nyou can see here um then we can also\\nwrite every number as the exponential of\\nthe log of itself because the\\nexponential and the log will cancel out\\nand um then we can by using the\\nproperties of the exponentials we know\\nthat the product of two exponential is\\nequal to the sum of the is equal to the\\nexponential of the sum of the arguments\\nof each exponential and we do it on the\\nnumerator and in the denominator then we\\njust call this quantity minus log c\\nequal to K or k is equal to minus K is\\nequal to log C so we can replace this\\nquantity with k\\nwe can do that because this is a\\nconstant that we have chosen and we just\\nare assigning it to another\\nconstant um so basically by doing this\\nderivation we can see that we can sneak\\nin a value inside of this exponential\\nthat if chosen carefully can reduce the\\nargument of this exponential and we will\\nchoose this k equal to the maximum\\nelement inside of the input Vector that\\nwe are applying the soft Max to so that\\neach this argument will be either zero\\nin case x i is equal to the maximum\\nelement that we are processing of the\\nvector or it will be less than zero and\\nwe know that\", mimetype='text/plain', start_char_idx=17649, end_char_idx=21981, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='8089818e-680f-45b7-92ca-bd65451c3956', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='fc804597-e91f-474a-a4a2-cb981da83d61', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c25429f94071071832f1727c3b23e431267e40557793f6b863531d966776bf93')}, text=\"the exponential when it's\\nequal to zero will be equal to the\\noutput of the exponential will be one so\\nthe argument when it's zero it will be\\nequal to one and when it's smaller than\\nzero so it's in the negative range it\\nwill be between Z and one so which is\\neasily representable with floating Point\\n32 for example so this exponential will\\nnot explode anymore so basically to\\napply the the soft Max to a vector in a\\nnumerically safe way we need to find a k\\nconstant which is the maximum value of\\nthis vector and when we apply it we need\\nto substract each element minus this\\nconstant that we have chosen so let's\\nlook at the algorithm to compute the\\nsoft Max so first of all given a vector\\nor given an N byn Matrix because we want\\nto apply the soft Max to this Matrix\\nhere which is n byn we need to go\\nthrough each row of this Matrix and for\\neach row we need to find the maximum\\nvalue among the elements which takes\\ntime complexity linear with respect to\\nthe size of the vector to the size of\\nthe row to which we are applying the\\nsoftmax then we need to compute the\\nnormalization factor which is this stuff\\nhere and we we cannot compute it before\\nthe step number one because we need to\\nhave the maximum element to compute this\\nsummation here and after we have\\ncalculated the the normalization factor\\nwe can then divide each elements\\nexponential by the normalization factor\\nand we cannot do the step number three\\nbefore uh calculating the normalization\\nfactor because uh we need to divide each\\nnumber by the normalization factor so if\\nyou like a pyo code for algorithms this\\nis an algorithm for computing the\\nsoftmax that we have seen right now so\\nfirst we find the maximum of the row of\\nto which we are applying the soft Max\\nthen we comput the normalization factor\\nand then we apply the soft Max to each\\nelement which means that we calculate\\ncomput the exponential of each element\\nminus the maximum value of the vector\\ndivided by the normalization factor now\\nthis pseo code is an algorithm that is\\nquite slow because look at a practical\\nexample imagine we have this Vector here\\nfirst we need to do step one find the\\nmaximum value in this Vector which is\\nnumber five and this takes linear time\\ncomputation then we need to calculate\\nthe normalization constant which is the\\nsum of the exponential of each element\\nminus the maximum value so e ^ of 3 - 5\\nplus e to ^ of 2 - 5 etc etc and this we\\nwill call it l and then each we need to\\ngo again through this Vector again and\\ntake the exponential of each element\\nminus the maximum divided by the uh\\nnormalization Factor so to apply the\\nsoft Max to an n byn Matrix we need to\\ngo through each element of this Matrix\\nthree times and these operations must be\\ndone sequentially so we cannot start\\noperation two until we have done\\noperation one and we cannot start\\noperation three until we have done one\\nand\\ntwo so this is quite slow only to apply\\nan operation that doesn't even change\\nthe shape of the matrix it's just uh\\nnormal normalizing the values so there\\nmust be a better way that that does not\\ninvolve three sequential operations in\\nwhich we need to go through this metrix\\nthree times let's\\nsee all right guys let's rehearse what\\nis the problem that we are trying to\\nsolve the problem statement is the\\nfollowing can we find a better way to\\ncompute the softmax that does not\\ninvolve going through the vector three\\ntimes because let's look at the pseo\\ncode of the algorithm for computing the\\nLo the softmax that we have found so far\\nimagine we have a vector made up of four\\nelements the first thing that we need to\\ndo is to compute the maximum element in\\nthis Vector which means going through\\nthis for Loop here that allow us to\\ncompute the maximum element in this\\nVector which means that we start from\\nthe left side of the vector and\\niteratively go to the right side so we\\nstart from the first element arrive to\\nthe end and we compare the previously\\nfound maximum with the current element\\nto find the global maximum basically\\nthis means that uh I I know that this is\\nvery simple uh I'm probably sure that\\nyou don't need to this example but\\nmaking this example will help us\\nunderstand what we will do next so\\nplease bear with me even if it's super\\nsimple what I'm doing um okay we at the\\nbeginning\\nm0 is equal to minus infinity M1 is\\nbasically the for loop at the iteration\\nnumber one which means that we are M1\\nwill be equal to the maximum of the\\nprevious estimate of the M which\", mimetype='text/plain', start_char_idx=21982, end_char_idx=26403, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='99c5f4da-cccb-4d81-b86f-be574ff51291', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8089818e-680f-45b7-92ca-bd65451c3956', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='18f464598f59ce28c442b89e12948a709a713b95b9a199c80d58e6cc21f2f6f9')}, text=\"is\\nminus infinity with the current element\\nwhich is three so it will become equal\\nto three then M2 will be equal to the\\nmaximum of the previously computed\\nmaximum so M1 so three with the current\\nelement which is two so it will be equal\\nto three M3 will be equal to the maximum\\nof the previously computed maximum so\\nthree with the current oops three with\\nthe current element which is five so it\\nwill be equal to five and M4 will be\\nequal to the maximum of the previously\\ncomputed maximum and the current element\\nso it will be equal to five so this\\nallow us to compute the maximum element\\nso at the fourth iteration we will have\\nthe maximum the global maximum\\nindependently of what is the input\\narray um delete okay after we have\\ncomputed the maximum which we know is\\nfive we can compute the normalization\\nfactor so let's start with the l0 l0 is\\nequal to Z L1 will be equal to the\\nexponential of l0 so actually sorry it\\nwill be l0 plus the exponential of the\\ncurrent element so three minus the\\nmaximum element we have found in the\\nprevious for Loop so five then L2 will\\nbe equal to L1 plus the exponential of\\nthe uh the current element so it's two\\nminus the maximum then L3 will be equal\\nto L2 plus the exponential of um the\\ncurrent element 5 - 5 then L4 will be\\nequal to the uh L3 + exponential of 1 -\\n5 if you expand this L this this will be\\nbasically equal to E power of 3 - 5 + e\\n^ of 2 - 5 + e the^ of 5 - 5 + e the ^\\nof 1 - 1 -\\n5 after we have computed this\\nnormalization Factor we can use it to\\nnormalize the each element in the input\\nVector which means that the X new X1 so\\nX1 Prime let's see will be equal to e to\\nthe power of um uh what's the first\\nelement\\nthree - 5 divided by L that we computed\\nin the previous for Loop so the L at the\\nfourth\\niteration uh the new\\nX2 so X2 Prime will be equal to the E\\nto^ of 2 - 5 / L4 and X3 Prime will be\\nequal to the E to power of 5 - 5 ided L4\\netc etc for all the\\nelements uh I know this is super simple\\nbut it will help us later so in this for\\nLoop we have that we need to go through\\nthe vector three TS because first we\\nneed to compute this for Loop then we\\nneed to compute this for Loop and then\\nwe need to compute another for Loop we\\ncannot do them not in this sequence\\nbecause in order to compute this for\\nLoop we need to have the maximum element\\nbecause we need it here and we cannot\\ncompute this forloop until we have\\ncomputed the previous one because we\\nneed to have the normalization factor\\nhowever we are stubborn and let's try to\\nfuse these two operations into one for\\nLoop which means that we go through the\\narray and simultaneously compute Mi and\\nsimil in the same iteration we also try\\nto compute LJ of course we will not be\\nable to compute LJ because we don't have\\nthe global maximum because we didn't go\\nthrough the all the um array yet however\\nlet's try to use the locally um the\\nwhatever estimate we have of the maximum\\nso far so let's try to use instead of MN\\nlet's try to use m I so the local\\nmaximum that we have computed so far so\\nif we apply the soft Max in this way in\\nthis fused way to this Vector we will\\nhave the following\\niterations um so this is our array or\\nvector and the first step is MI so M1\\nwill be equal to the previous maximum\\nwhich is minus infinity with the current\\nelement so the maximum minus infinity\\nand the current element is equal to\\nthree and L1 will be equal to the\\nprevious l so l0 which is starts from\\nzero plus e to the power of the current\\nelement minus we should be using the\\nglobal maximum but we don't have the\\nglobal maximum so let's use the whatever\\nmaximum we have so far so we can use\\nthree now at the second iteration we are\\nat this element of the vector and we\\ncomput the maximum so far so the maximum\\nso far is the previous maximum and the\\ncurrent element so the maximum of the\\nprevious maximum and the current element\\nwhich is the maximum between three and\\ntwo which is three uh and the the\\nnormalization factor is the previous\\nnormalization Factor plus exponential of\\n2 - 3 which is the current element minus\\nwhatever maximum we have so far now if\\nour array were\", mimetype='text/plain', start_char_idx=26404, end_char_idx=30477, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='8ad5c979-87e0-4305-9115-db0f79269918', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='99c5f4da-cccb-4d81-b86f-be574ff51291', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='196deb3014e93beaea07146a72624aa19e63bba7531eb8f09d14d318b6a2ae75')}, text=\"made only of these two\\nelements so three and two then whatever\\nwe have computed is actually correct\\nbecause the maximum that we have found\\nis a three and it's actually the global\\nmaximum and the um normalization factor\\nthat we have computed is actually\\ncorrect because each of the exponent\\nitial has been computed with the global\\nmaximum because the first element was\\ncomputed using three as the uh with the\\nargument minus three and also the second\\nelement was computed with the argument\\nwith the argument having minus three in\\nthe in the argument which is the global\\nmaximum of the vector however when we\\narrive at the third iteration so let me\\ndelete this Vector so we arrive here at\\nthe third iteration the maximum will\\nchange which will also uh cause our\\nnormalization factor to get to to be\\nwrong because we arrive at the element\\nnumber three uh so the number five here\\nand we computed the maximum so the\\nmaximum is the comparison of the\\nprevious maximum and the current element\\nso the new maximum becomes five and the\\nnormalization factor is the previous\\nnormalization Factor so L2 plus the\\nexponential of the current element minus\\nthe current estimate of the maximum\\nwhich is five however\\nif you look at this L3 this is wrong why\\nbecause L3 is equal to if you expand\\nthis summation it will be equal to e to\\nthe power of 3 - 3 + e to power of 2 - 3\\n+ e to the power of 5 minus 5 this\\nexponential here is using five as the\\nglobal maximum this exponential here is\\nusing three as the global maximum and\\nthis one is using three as the global\\nmaximum so the first two element have\\nbeen computed\\nthinking that the global maximum is\\nthree but actually we later we found a\\nbetter Global maximum which is five so\\nwhich makes this normalization Factor\\nwrong however can we fix at the third\\niteration whatever normalization we have\\ncomputed so far up to the second\\niteration actually we can because if we\\nexpand this so as we have here we have\\nexpanded\\nit what we need here is here to have\\nminus5 because that's actually the\\nGlobal maximum that we have found so far\\nnot the minus three that we had at the\\nprevious iteration so and here we also\\nneed to fix this replace this -3 with\\nminus5 how can we do that well if we\\nmultiply this one here and this one here\\nwith a correction factor that will sneak\\nin a new maximum inside of this\\nexponential then we solve the problem\\nand actually this correction factor is\\nvery easy to calculate because at the\\nthird iteration if we multiply L2 so the\\nprevious prly computed normalization\\nfactor with this Factor here which is\\nthe exponential of the previous estimate\\nof the maximum minus the current\\nestimate of the maximum so five we will\\nsee that by the properties of the\\nexponentials this one here will become e\\nto the^ of 3 - 3 + 3 - 5 so this minus 3\\nwill cancel out with this three and also\\nthe second Factor will have this three\\nwill cancel out with this minus three\\nwill cancel out with this three and they\\nwill become e to ^ of 3 - 5 and 2 to the\\npower of u e to the^ of 2 - 5 which is\\nactually correct because at the third\\niteration we should be actually have we\\nshould be using minus5 as the maximum of\\nthe array so far um so basically what we\\nhave found is a way to fix whatever\\nnormalization Factor we have computed so\\nfar while iterating through the array\\nwhen we found we when we find a better\\nmaximum compared to what we have so far\\nand when we don't need to fix anything\\nthen the formula still stands because\\nwhat we did here as a multiplic as a\\ncorrection factor so this is the\\ncorrection factor this correction factor\\nis nothing more than the previous um\\nprevious maximum so the previous\\nestimate of the maximum minus the\\ncurrent estimates of the maximum at the\\ncurrent iteration so the current\\nMax um so this is basically M of IUS one\\nand this is M of I so the current\\nmaximum at the current iteration and let\\nme delete it otherwise it remains\\nforever in my slides um so basically\\nwhen we arrive to the last element we\\nwill see that the maximum doesn't change\\nbecause we compare the previous maximum\\nwith the current element which is less\\nthan the previous maximum so the maximum\\ndoesn't change and we don't need to fix\\nanything because the the the the\\nprevious L3 so the previously computed\\nuh normalization factor is correct\\nbecause they have all been using the\\nminus5 so when we don't need to fix\\nanything we just multiply by e to the\\npower of the previous maximum minus the\\ncurrent maximum which\", mimetype='text/plain', start_char_idx=30478, end_char_idx=34930, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='c6c61bdc-caf5-4a0f-b145-f1b20db89caa', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8ad5c979-87e0-4305-9115-db0f79269918', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='3593132990bc634d694ae2f924b68fee5c6e68051912d9a7abf2c68e75930562')}, text=\"is e to the power\\nof zero in this case so it's not fixing\\nanything so we have found a way to fix\\nthe previously computed normalization\\nFactor while going through the array\\neven if at the current iteration we\\ndon't have the global maximum yet so\\nthat every time the maximum changes we\\ncan fix and every time it doesn't change\\nwe just multiply with e to the^ of Z\\nwhich is like multiplying with one so\\nthe new algorithm that we have found for\\nthe softmax is the following so we start\\nwith m0 equal to minus infinity we start\\nwith l0 equal to Z we go through the\\narray we compute the locally uh the the\\nlocal maximum so up so the maximum so\\nfar from the zeroth element to the E\\nelement so to the elements at which we\\nare we are doing the\\niteration and the previously computed Li\\ncan be fixed by using this correction\\nfactor which is e to the power of the\\nprevious maximum minus the current\\nmaximum plus the exponential of the\\ncurrent element minus the current\\nestimate of the maximum in this way we\\ngo through the array only once and we\\nobtain two values the global maximum at\\nat the end at the same time the uh the\\nthe ization factor and then we can use\\nit to compute the softmax so we made\\nthree transformed the three passes\\nthrough the array into two passes\\nthrough the array and this is very\\nimportant and we will see how we\\nactually use it to derive flesh\\nattention the example that I have given\\nyou so far is not really a proof that\\nour algorithm will work in every case\\nbecause we made a very simple example by\\nusing a vector made up of four elements\\nbut but does our new algorithm work in\\nevery single case with whatever the\\nnumbers are we need to prove that so we\\nwill prove that by induction so what\\nfirst of all what are we trying to prove\\nwe have fused the first two for Loops\\ninto one for loop as you can see here\\nwhat we expect is that at the end of\\nthis for Loop this MN so the m at the\\nlast iteration will be actually the\\nglobal maximum in the vector and this Ln\\nso the L at the last iteration will be\\nequal to the sum of all the exponential\\nof all the\\nelements minus the maximum element of\\nthe vector so the global maximum of the\\nvector and we need to prove that because\\nwhat I did before was an example and\\nthat was not really a rigorous proof and\\nthe way we will prove it is by induction\\nwhich is a typical way of proving these\\nkind of um\\ntheorems now proof by induction\\nbasically Works in the following way we\\nneed to prove that our algorithm work\\nworks for a base case for example with\\nNal to 1 and then we pretend we assume\\nthat the algorithm work works on n and\\nwe need to prove that it also works for\\nn + one if this holds then we have\\nproven our algorithm for every possible\\nn because it will work for the base case\\nso for example n equal to 1 and then by\\nusing the induction step we say so this\\nif it works for n then it also works for\\nn + one then it means that it will also\\nwork for two but then if it works for\\ntwo then it should also work for three\\nbecause of the induction step that we\\nwill prove and if it works for three\\nthen it will also work for four etc etc\\nup to\\nInfinity so let's prove it for the base\\ncase which is n equal to 1 uh it's very\\nsimple so uh at n equal to 1 this for\\nLoop will only have one iteration so M\\nM1 and L1 M1 will be be the maximum of\\nthe previous M which is minus infinity\\nbecause we initialize m0 equal to minus\\ninfinity um so it will be equal to U the\\nmaximum of the previous M and the\\ncurrent element which is X1 so it will\\nbe equal to X1 whatever X1 is uh X1\\nusually will never be equal it cannot be\\nequal to minus infinity um because it's\\na number in fixed representation so it\\ncannot be minus infinity um so we the\\nthe X the M1 at the end so it will\\nbecause we have only one element n equal\\nto one this is M1 is also the last um M\\nof this of this for Loop it will be\\nequal to the global maximum of the\\nvector made up of only one element and\\nL1 will be equal to the previous L which\\nwe start from zero so l0 multiply by a\\ncorrection factor which will be in this\\ncase e to the power of minus infinity\\nbecause the correction factor is the\\nprevious estimate of the max of the max\\nminus the current estimate of the max\\nbut the previous estimate of\", mimetype='text/plain', start_char_idx=34931, end_char_idx=39138, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='104eb84a-759b-4fc2-ad65-c99593372f5d', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c6c61bdc-caf5-4a0f-b145-f1b20db89caa', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='006d11532f2b12f91fd58fb5c90771690952396b92454eb5e710d0751a3509be')}, text=\"the max is\\nminus infinity minus X1 it is equal to\\nminus infinity so this one will be this\\nwill be canceled out and then plus e to\\nthe^ of X1 minus the current maximum\\nwhich is X1 so\\nM1 and if this one will be equal to the\\nsum of all the elements of the vector\\nwhich is made up of only one element\\nminus uh the maximum element in the\\narray which is X1 so we have proven that\\nit works for n equal to 1 now we assume\\nthat it works for n does it also work\\nfor an array of vect or with a vector of\\nsize n + one so let's see what happens\\nat the n + one iteration at the n+ one\\niteration we will be doing the maximum\\nof the previous estimate of M which is\\nthe m at the end iteration and the\\ncurrent element so xn of + one this by\\nthe properties of the Max uh function it\\nwill be actually equal to the maximum of\\nthe global Vector up to n + one uh\\nbecause um the maximum will choose\\nwhatever is the maximum between the\\nprevious estimate and the current\\nestimate and Ln + one which is the\\nnormalization factor at the n + one\\niteration will be equal to the Ln so the\\nprevious estimate not previous by the\\nprevious normalization Factor at at the\\nend iteration multiplied by the\\ncorrection factor which is the previous\\nmaximum minus the current maximum plus\\nthe exponential of x uh the current\\nelement minus the current estimate of\\nthe\\nmaximum but\\nLn we have we assume that this um\\nproperty so this algorithm works up to n\\nso Ln is for sure equal to the sum of\\nall the exponentials of the previous of\\nthe vector up to n minus the local\\nmaximum of the uh Vector up to the end\\nelement which is\\nMN we multiply by the correction factor\\nif there is something to correct which\\nwill be the previous maximum minus the\\ncurrent maximum plus the exponential of\\nthe current element minus the current\\nexp estimate of the\\nmaximum now\\nby the properties of the exponentials so\\nwe can bring this one inside of the uh\\nsummation and we will see that this MN\\nand this MN will cancel out because it\\nwill be exponential of XJ minus MN + MN\\nminus MN + one so this MN and this MN\\nwill cancel out and we obtain this one\\nplus this Factor here that remains\\nunchanged however you can see that this\\nthis stuff here is exactly the in the\\nargument of this summation for the at\\nthe iteration n + one so it is this one\\nis e to the power of XJ where J is going\\nfrom 1 to n minus MN + 1 + e ^ of x n +\\n1 - MN + 1 so the J only appears here\\nand it's equal maximum to n and this is\\nsimilar to being a j with n + one so we\\ncan increase increase the index of this\\nsummation by one and it will be the same\\num and it will result in the same\\nsummation so we have proven that also at\\nthe n + one iteration we will have that\\nthe L will be equal to the sum of all\\nthe elements of the array the\\nexponential of all the elements of the\\narray up to the n+ one\\nelement uh minus the maximum up to the n\\n+ one element so we have proven that if\\nit works for n then it also works for n\\n+ one um this is enough to prove that it\\nworks for all sides of\\narrays um don't worry if you didn't get\\nuh the proof by induction it is uh if\\nit's the first time you are seeing this\\nkind of proof it may take a little bit\\nto to get it um if you want to learn a\\nbit more about proof by induction I\\nrecommend watching some other proof it's\\nvery simple it's just you need to get\\ninto the right mindset anyway let's move\\nforward\\nall right let's talk about block matrix\\nmultiplication I know that you want to\\njump to the code immediately and we will\\ngo there we just need a little more\\nTheory actually so imagine we are doing\\na matrix multiplication so we have a\\nmatrix a we want to multiply it with a\\nmatrix B and it will produce an output\\nMatrix C imagine the dimensions of the\\nfirst Matrix are M by K the second\\nMatrix is a k by n it will produce an\\noutput Matrix that is m by n now imagine\\nwe want to parallelize the computation\\nof this output Matrix I know that I\\ndidn't talk about gpus yet so we will\\nnot talk about gpus we will talk about\\nparallelization in the case of a\\nmulticore CPU with which you are very\\nprobably familiar with because right now\\nin nowadays when you buy a computer you\\nhave a CPU\", mimetype='text/plain', start_char_idx=39139, end_char_idx=43261, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='7619644e-67fd-4335-b554-54e9f22db1a6', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='104eb84a-759b-4fc2-ad65-c99593372f5d', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='cce74ef9facf874858252115b738b3f6e81aa4356f14f9dda1970c5cfcf6577b')}, text=\"and usually you can buy a\\nsingle core CPU or multie like a two\\ncore four core eight core etc etc each\\nof the these cores are actually kind of\\nsmall CPUs inside your CPU that can\\nexecute operations in parallel how to\\nparallelize the matrix multiplication\\nimagine you have this matrix\\nmultiplication to parallelize each of\\nthe output element in this C Matrix is a\\nDOT product of a row of the a matrix\\nwith a column of the B Matrix for\\nexample this element on the top left is\\nthe dotproduct of the first row of a and\\nthe First Column of B this element on\\nthe top right of C is the dot product of\\nthe first row of a and the last column\\nof B this element on the bottom left is\\nthe dot product of the last row of a and\\nthe First Column of B etc etc for all\\nthe other elements now to parallelize\\nthis computation we need as many cores\\nas is as there are elements in C if we\\nwant to parallelize it uh so if M and N\\nare very small then maybe we have enough\\ncourse but imagine M and N are quite big\\nwe imagine like 100 by 100 we don't have\\n10,000 cores right now in the CPUs so\\nhow can we parallelize a matrix um\\noperation by using less cores than there\\nare elements in The Matrix itself that's\\nwhen we talk about block matrix\\nmultiplication basically block matrix\\nmultiplication means that you can divide\\nthe original Matrix into smaller blocks\\nof of elements and then the operations\\nof matrix multiplication can be computed\\nbetween these blocks for example imagine\\nwe have a matrix that is 8x4 it means\\nthat it has eight\\nrows and four columns which means that\\nit has 32 elements and then we are\\nmultiplying it with another Matrix that\\nis 4X 8 so it has four rows and eight\\ncolumns so it also has a 3 two elements\\nthe output Matrix will should have 64\\nelements we don't have 64 cores so how\\ncan we parallelize it imagine we only\\nhave eight cores now with eight cores we\\ncan divide this original Matrix a into\\nfour blocks where the first block is\\nthis top left block of two by no 4X two\\nelements so um uh how to say um eight\\nelements on the top top left and then\\neight elements on the top right of this\\nMatrix then eight elements on the bottom\\nleft and eight elements in the bottom\\nright of this Matrix these are four\\nblocks then we divide also the B Matrix\\ninto um eight blocks where each block is\\nmade up of four elements so this b11 is\\nthe top left four elements in the\\noriginal Matrix this B4 is the top right\\nfour elements in the original Matrix\\nthis B21 is the\\num bottom left for elements in the\\norigin etc etc etc how do we do this\\nblock matrix multiplication we can watch\\nthese matrices as made only by their\\nblocks so we can view this Matrix here\\nas made up only by its blocks we can\\nview this Matrix here as made up only by\\nits blocks and the output of this\\nmultiplication will be a\\nmatrices that is computed in the same\\nway as the original Matrix\\nbut where the output of each dot product\\nwill not be a single element of the\\noutput Matrix but it will be a block of\\nelements of the output Matrix for\\nexample the top left block here is the\\ndot product of the first row of this\\nMatrix with the First Column of this\\nMatrix and it will be computed as\\nfollows so it will be a11 * b11 plus a12\\n* B21 and this output will not be a\\nsingle scalar but it will be a uh well\\nlet me count it should be uh eight\\nelements so it should be four um made up\\nit's it should be a block of four\\nelements um or eight elements let me let\\nme count actually so because we have\\neight blocks and it should be made up of\\neight elements uh let's we can see that\\nhere um how to find the dimensions of\\nthis output block\\nuh well we can check what is a11 a11 is\\n4X two so it's eight elements in as a\\nsmaller Matrix made made up of eight\\nelements where the elements are\\ndistributed in four rows and two columns\\nwe are multiplying it by b11 which is a\\nsmaller Matrix compared to the original\\nmade up of 2x two elements so four\\nelements so when we multiply a 4x2\\nmultiply by 2x two it will produce a 4x2\\noutput uh block Matrix so\\nblock so if we do this computation here\\nblock by block it will produce a block\\nof output elements of the original\\nMatrix so not not a single scalar but a\\nblock of outputs which makes it\", mimetype='text/plain', start_char_idx=43262, end_char_idx=47439, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='1050fffd-289e-4e95-9cb2-8d375b4beb2a', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='7619644e-67fd-4335-b554-54e9f22db1a6', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='a0940e9a8390814b513d18344369e6a3a509ccf8c149c061543127baa841f574')}, text=\"very\\neasy to parallelize because if we have\\nonly eight cores we can assign each\\noutput block to One Core and each core\\nwill not produce one output element of\\nthe original Matrix but it will produce\\neight elements of the original Matrix as\\na 4x2\\nMatrix um so basically block Matrix\\nallow us to um uh to do the matrix\\nmultiplication either by element by\\nelement so like in the original Matrix\\nso each row with each column or blocks\\nby blocks in the same way like we do\\nnormal matrix multiplication because the\\nthe matrix multiplication that we are\\ndoing between blocks is the same way as\\nwe do matrix multiplication with the\\noriginal Matrix and it will produce not\\na scolar but a block and now let's see\\nwhy this is very important for us\\nso why should we care about block matrix\\nmultiplication because we are trying to\\ncompute the following operation so the\\nquery multiplied by the transpose of the\\nkeys and then we will should apply the\\nsoft Marx of this operation and then we\\nshould multiply the output of the\\nsoftmax with V for now let's ignore the\\nsoft Marx let's pretend that we are not\\ngoing to apply any softmax so we take\\nthe output of the query multiplied by\\nthe transpose of the keys and we just\\nmultiply it by V to obtain the output of\\nthe attention which is wrong of course\\nbut it simplifies our tract of what we\\nare going to do next so for for this\\nmoment let's pretend that we are not\\ngoing to apply any soft Max so we just\\ndo the query multiply by transpose of\\nthe keys and directly we multiply the\\nresult of this operation with v this\\nwill result in a matrix that is n by D\\nso n tokens each made up of an embedding\\nof D Dimensions so lower case D\\ndimensions and we know that query key\\nand values are themselves matrices of n\\nby D Dimensions so the um um n tokens\\nwhich made up of embedding of the\\ndimensions so imagine we have a query\\nMatrix and the key and the value Matrix\\nthat are 8 by 128 so we have eight\\ntokens each token is made up of 128\\nDimensions we can divide as we have seen\\neach when we compute matrix\\nmultiplication we can divide our um\\nMatrix into blocks how we choose the\\nblocks is up to us as long as the oper\\nthe the shapes of the blocks match when\\ndoing the matrix multiplication so for\\nexample in the previous case we divided\\nour Matrix a into blocks such that the\\nthe the shape of the block Matrix so the\\nMatrix that is made up only of the\\nblocks is compatible with the block\\nMatrix B so that this operation is\\npossible so this is the only requirement\\nthat we need to be aware when doing the\\nmatrix multiplication the shapes of the\\nblocked Matrix so the Matrix that is\\nmade only of the blocks should match in\\nthe matrix multiplication for the rest\\nit doesn't matter how we divide it so\\nimagine that we choose to divide this\\nquery Matrix into blocks of rows and we\\ncan do that we don't have to necessarily\\ndivide also the columns we can just\\ndivide the rows so that each Q is not a\\nsingle row but it's a group of two rows\\nso q1 is a group of the first two rows\\nof the Q Matrix of the Q sequence Q2 is\\nthe group of the second two rows of the\\nQ sequence etc etc and we do the same\\nalso for V for K we don't do it because\\nwe are actually going to multiply with K\\ntransposed so we do this subdivision\\ndirectly on K transposed so so we have\\nthe Q which has been divided into groups\\nof rows and then we have a k transposed\\nwhich is a matrix that is 108 by 8\\nbecause it's the transpose of the Keys\\nwhich is 8 by\\n108 and we decide to divide each of the\\ncolumn group of columns of K into a\\nsingle block so the K1 is the first two\\ncolumns of K transposed K2 is the second\\ngroup of two columns in K transposed etc\\netc until K4 which is the last two\\ncolumns in K transposed the first\\noperation that we do is the\\nmultiplication query multiply by the\\ntranspose of the keys which basically\\nmeans that we need to multiply each\\nquery with all the keys then the second\\nquery with the all the keys etc etc now\\neach query is not a single row of the Q\\nsequence it's a group of two rows of the\\nQ sequence and each K is not a single\\ncolumn of K transposed it's a group of\\ntwo columns of K transposed but doesn't\\nmatter because we have seen that the\\nmatrix multiplication if we write the\\nmatrixes as made up of blocks we just\\ncompute it in the same way when we do\", mimetype='text/plain', start_char_idx=47440, end_char_idx=51713, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='9d29d134-6deb-45e5-829b-41a4fa3f4a8b', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1050fffd-289e-4e95-9cb2-8d375b4beb2a', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='e367f097fb316ade694c53ca8a32e8205f14c9053b4d9f5833a6669040495408')}, text=\"uh\\nnormal matrix multiplication ation so we\\nare multiplying this matrix by this\\nMatrix and for what we know this Matrix\\nhere is made up of four rows with some\\nDimensions which is uh 128 dimensions\\nand this one here is made up of uh how\\nmany rows 128 rows and four uh\\ncolumns I didn't uh draw The Columns\\nbecause it's too many to draw here but\\nyou need to pretend it's a lot of\\nDimensions one for each 128 for each\\nvector and here you need to pretend that\\nthis is 128 rows when we do the matrix\\nmultiplication we apply the normal M\\nmatrix multiplication procedure which is\\neach output element so this first of all\\nthe output shape of this Matrix of this\\nmatrix multiplication will be 4x4\\nbecause it's the outer dimensions of the\\ntwo Matrix that they are\\nmultiplying the first element of the\\noutput will be the dot product of this\\nVector here with this Vector here the\\nsecond element so this one here will be\\ndone dot product of this Vector here\\nwith this Vector here however this is\\nnot vector and this is not a vector so\\nit's actually a matrix multiplication in\\nthis case this element here is not a\\nscalar it is a group of elements of the\\noutput Matrix because we are doing block\\nmatrix multiplication and how many\\nelements it will be well we know that\\nthe original q1 is a 2 by\\n128 the K1 is 108x 2 so it will be a\\ngroup of 2x two elements of the output\\nMatrix so we are doing the matrix\\nmultiplication of the q1 with K1 then q1\\nwith K2 then q1 with K3 q1 with K4 etc\\netc for the first row and then the\\nsecond row will be Q2 with all the K and\\nthe Q3 with all the ks and Q4 with all\\nthe ks so as you can see when we do\\nmatrix multiplication we don't even care\\nif what is underlying is a block or a\\nvector or a scalar we just apply the\\nsame procedure for first um row of the\\nBlack Block matrix multiplication with\\nthe First Column of the Matrix M of the\\nsecond Matrix uh and then the first row\\nwith the second column the first row\\nwith the third column etc\\netc let's then multiply because the\\nformula says that we need to multiply\\nquery with the transpose of the keys and\\nthen multiply by V all of these are um\\nblock matrices now as you can see from\\nmy using of colors every time time I\\nrefer to the original Matrix I use the\\nblue color and every time I refer to the\\nblock Matrix I use the pink color so we\\nneed to multiply the output of the query\\nmultiplied by the transpose of the key\\nthen by V because we are skipping for\\nnow the soft Max and later we will see\\nwhy so if we want to do this\\nmultiplication we need to do the\\nfollowing so it will be uh this Matrix\\nis made up of blocks and block matrix\\nmultiplication just ignores this fact\\nand just does the matrix multiplication\\nlike it is a normal matrix\\nmultiplication so we do the first row\\nwith the First Column then the first row\\nwith the second column then the third\\nrow the first row with the third column\\netc etc so the first block of row how is\\ngoing to be calculated this out um this\\noutput in the output Matrix of this\\nmatrix multiplication well it will be\\nthe um the the first row so the dot\\nproduct of the first row the dot product\\nbecause it's not really Dot product it's\\nthe actually the matrix multiplication\\nof the first row but in a dotproduct way\\nlet's say uh with the First Column which\\nis made up of V1 vs2 V3 and V4 so it\\nwill be this element with V1 plus this\\nelement with vs2 plus this element with\\nV3 plus this element with V4 uh and this\\nwill be the first output element the\\nsecond output uh block will be this row\\nwith this\\ncolumn uh which will be this element\\nwith V1 this element plus this element\\nwith V2 plus this element with V3 plus\\nthis element with V4 and this will\\nproduce the second output\\nblock etc etc also for the third and the\\nfourth block output let's look at what\\nis each block made up of so each block\\nis made up of the um the first element\\nso query 1 multip by qy1 because um\\nit's the result of the query multiply by\\nthe keys with the V1 of the second\\nMatrix plus the this element with this\\none plus this element with this one plus\\nthis element with this one so the pelo\\ncode for generating this output of this\\nattention mechanism which is not really\\nattention mechanism because we skip the\\nsoft\", mimetype='text/plain', start_char_idx=51714, end_char_idx=55906, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='0e9c18e7-b2d7-4a97-b5cb-ddc9eda7e9a1', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9d29d134-6deb-45e5-829b-41a4fa3f4a8b', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='33dd4166dcd39b8cc92b9e6f3ea20c69ac9f11a834c56b4a11e96bddab5aa522')}, text=\"Max but I just want you to get into\\nthe habit of thinking in terms of blocks\\nis the following so we take each query\\nlock um we go through each\\nquery and as you can see let's look at\\nactually what this output is made up it\\nis made up of the query one multiplied\\nby key1 and the result multiply by V1\\nthen the query one with K2 then the\\nresult multiply by V2 then the query one\\nwith the K3 and the result multiply by\\nV3 plus the query one with the K4 and\\nresult multiplied by V4 this is\\nbasically what we are doing is the dot\\nproduct of this row with this column\\nmade up of\\nblocks um so the the pelo code for\\ngenerating this first row is the query\\nis the query number one and then we\\niterate through the keys and the values\\nfrom one to four and we sum iteratively\\nso for each block basically to generate\\nthis output Matrix and if you um for\\neach row we will see that it's a\\ndifferent query with all the keys and\\nvalues and then this will be the the the\\nquery number number three with all the\\nkeys and values and this will be the\\nquery four with all the keys and values\\nso to generate this output Matrix we\\nneed to do we iterate through the\\nqueries and this will be one row of this\\noutput Matrix and then we need to do\\nthis iterative sum of the query I that\\nwe are iterating through multiply by the\\nJ K and V and we keep summing them\\niteratively and that would that will\\nproduce the output Matrix or you can see\\nhere I know that what I have done so far\\nis not useless not useful for Flash\\nattention but it's useful for us to get\\ninto the mindset of computing this\\nproduct by blocks because later we will\\nuse it also with the\\nsoftmax all right guys I I know that we\\nhave comput what we have computed so far\\nis not really the soft Max operation is\\nnot sorry the really the attention\\nmechanism because we have skipped the\\nsoft Max so somehow we need to restore\\nit and the the following few I think\\nthink 10 20 minutes we are going to be\\nreally really challenging because I am\\ngoing to do a lot of operations that\\nwill involve a lot of different blocks\\nand a lot of different metrix\\nmultiplication and variants of the\\nsoftmax so it may be difficult to follow\\nhowever don't give up you can watch this\\npart twice three times and every time\\nyou it will have a better understanding\\nI also recommend watch it until we reach\\nthe flesh attention algorithm before we\\nstart restarting from to to go back to\\nto rewatch it because you watch it we\\nreach the flesh attention algorithm and\\nit will give you a better understanding\\nof what have happened so far and then\\nyou can rewatch it to deepen your\\nunderstanding another thing that I\\nrecommend is take pen and paper and\\nwrite exactly the operations that you\\nare seeing and write the shapes of each\\nof these blocks of these elements that\\nare made in that are part in this um\\nMatrix\\nmultiplications so that you better uh\\nunderstand what is happening and you\\nbetter remember what when I refer to a\\nparticular element or a particular block\\nokay after giving this small uh\\nmotivational speech Let's Start so what\\nwe have done so far was query multipli\\nby the transpose of the keys however\\neach query is not a single row of the\\nquery sequence but it's a block of\\nqueries it's a block of rows in our\\nparticular case this q1 is not one row\\nof the query sequence it's two rows of\\nthe query sequence because we have\\nchosen as a block size a group of two\\nrows and this K transposed one is not\\none column of the K transposed Matrix is\\ntwo columns of the K transposed Matrix\\nbecause we have chosen it like this and\\nif you don't remember let's go back to\\nsee it uh here we have chosen K1 is 2\\ntwo columns and q1 is two rows of the\\nquery original Matrix and every time I\\nuse the blue color I am referring to the\\noriginal shape and every time I'm using\\nthe pink or Violet whatever it is I am\\nreferring to the block metrix so it's a\\nblock of elements of the original\\nMatrix okay now uh the first thing that\\nwe have done was query multiplied by the\\ntranspose of the keys and this produces\\na block patrix as output that we will\\ncall S where each element s j so the S11\\nelement of this Matrix will be the query\\none with K transposed one the S12 will\\nbe query one with K transposed 2 s13\\nwill be query one with K\", mimetype='text/plain', start_char_idx=55907, end_char_idx=60130, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='cc1daf91-c6f0-4382-89c4-b3f110bf75c3', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='0e9c18e7-b2d7-4a97-b5cb-ddc9eda7e9a1', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='df94bb73e472166d2cba5bcf96197e9f49c23d99ab038eda84aa116f12a56bb7')}, text=\"transposed\\nthree etc etc for all the rows and for\\nall the columns then we should be\\napplying the soft Max because if you\\nremember the formula is soft Max of the\\nquery multiplied by the transpose of the\\nkeys however I want to restore the\\nsoftmax operation but with a Twist which\\nmeans that we will apply the simplified\\nversion of the softmax and we will call\\nit softmax star which is just the\\nsoftmax without the normalization so let\\nme write it for you what it\\nmeans uh let's do it with the same color\\nthat I chose for the softmax which is\\norange so the soft max if you remember\\ncorrectly if we remember it's the soft\\nMax\\nof to of a vector we apply it element\\nwise so each element is modified\\naccording to the following formula so\\nthe E element of the output Vector to\\nwhich we are applying the softmax is\\nequal to the\\nexponential of the E element of the\\ninput Vector minus the maximum element\\nin the input Vector divided by a\\nnormalization factor that is calculated\\naccording to this\\nsummation that is going from Jal to 1 up\\nto n of the\\nexponential of x i minus x max so\\nbasically uh we are doing the\\nexponential of each element minus this x\\nMax and why are if you remember\\ncorrectly why are we subtracting this x\\nmax to make this exponential numerically\\nstable computable because otherwise it\\nwill explode and because we are applying\\nit to the numerator we also need to\\napply it to the denominator okay the\\nsoftmax start operation is exactly like\\nthe softmax but without the\\nnormalization part which means that it's\\njust the numerator of the soft Max so we\\nwill modify each element of the vector\\nto which we apply the soft Max star\\naccording to this formula let me move it\\nmore aligned like this so we just do\\nelement element wise uh operation that\\nis the exponential of each element minus\\nthe maximum to the of the vector to\\nwhich we are applying soft Max star\\nokay now why did I introduce this soft\\nmaxar operation because we will be\\napplying it to the Matrix that we have\\ncomputed so far which is this s Matrix\\nso we applied soft Max star to each\\nelement of this s Matrix but each\\nelement of this s Matrix is itself a\\nmatrix because it's a block Matrix and\\neach element of this s Matrix so for\\nexample the element S11 is a 2x2 matrix\\nbecause it is coming from the product of\\ntwo matrices which are a group of rows\\nand a group of columns from the Q and\\nthe K uh so for example this S11 is what\\nis um let's draw it actually this S11\\nwill be for example made up of four\\nelements let's call it uh I don't know a\\nof\\nS11 uh let's let's choose better naming\\nlet's call it I don't know\\na uh\\nb c and d just the generic elements when\\nwe apply the soft Max star to this S11\\nit will result so let's apply the soft\\nMax\\nstar soft Max\\nstar it will result in a\\nmatrix that\\nis each element the exponential of each\\nelement minus the maximum for each row\\nnow we don't know which is the maximum\\nso let's choose one suppose that's the\\nmaximum for this row is a and the\\nmaximum for this row is D the first\\nelement of the output of this soft Max\\nstar applied to this block S11 will be\\nthe\\nexponential of a multi minus a because\\nthat's what we chose as the maximum for\\nthis row the second element will be the\\nexponential of B minus a because it's\\nthe maximum for that row then in the\\nbottom row it will be the exponential of\\nuh C minus D because that's the maximum\\nfor the bottom row and this will be the\\nexponential of D minus D and that's the\\nexponential that's how the softmax star\\nwill modify each block in this block\\nMatrix let me delete this stuff\\notherwise it will remain in my slides\\nforever and later I want to share the\\nslides with you guys so you can use my\\nsame slides so delete delete delete okay\\nafter we have applied the soft Max to\\neach of the elements in this s Matrix we\\nwill call it the P Matrix and each\\nelement P11 will again be block of 2x\\ntwo\\nelements um so P11 will be the soft Max\\nso P11 will be the soft Max star applied\\nto S11 where S11 is what is query one k\\ntransposed one and the p12 will be the\\nsoft Max star applied to S12 where S12\\nis what is query one\\nmultiplied by K transpose the two etc\\netc etc for all the elements of\\ns okay now that we have applied\", mimetype='text/plain', start_char_idx=60131, end_char_idx=64299, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='9744aaa2-d04d-42f5-9131-2cba8208beda', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='cc1daf91-c6f0-4382-89c4-b3f110bf75c3', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='ea6e47e6ec4248870f2e5b5cd52e4c146ffe02940adae030afcb9036f65ee564')}, text=\"this\\nsoftmax star operation the next\\noperation that we should be doing\\naccording to the formula of the\\nattention is uh the softmax of the query\\nmultiplied by the transpose of the keys\\nthen the result of the softmax\\nmultiplied by V I know that we didn't\\napply the real soft Max we applied\\nsoftmax star which is softmax without\\nthe normalization later we will see how\\nto compens at this lack of normalization\\nbecause we will do it at the end and\\nit's something that we can\\ndo okay so we take this P Matrix which\\nis the result of the soft Max star\\napplied to this s Matrix and we multiply\\nit by\\nV what how do we do it well it's a block\\nit's a matrix made up of blocks of\\nmatrices um so P11 is actually not a\\nscolar but it's a matrix of 2x two\\nelements and we need to multiplied by V\\nbut we don't multiply with the original\\nsequence V but with the blocked sequence\\nV just like before where each V is not\\none row of V but it's a group of rows of\\nv and how many rows is it is it is two\\nrows of V uh for now please ignore\\ncompletely whatever I have written here\\nbecause we will use it later so we need\\nto do this product of this Matrix here\\nwhich is made up of blocks remember with\\nthis Matrix here which is made up of\\nblocks it is made up of four\\nfour rows where each row is not really a\\nrow it is a block of rows and this one\\nit is made up of 4x4 elements where each\\nelement is not really a scolar but it's\\na\\nmatrix so as you remember in the block\\nmatrix multiplication when the algorithm\\nfor computing the matrix multiplication\\nis the same as the normal matrix\\nmultiplication except that we use blocks\\nso what I am doing is guys uh the\\nfollowing operation so let's write it\\nsomewhere let's say o is equal to P\\nmultip by V okay so um the first output\\num row row because it's not really a row\\nbut it's a block row uh will be computed\\nas follows the first row of this block\\nMatrix with the First with um the First\\nColumn of this V Matrix and um we are\\ntreating it uh like a block Matrix so it\\nwill be P11 multip by V1 plus p12 ultip\\nby vs2 plus p13 * by V3 plus p14\\nmultipli by\\nV4 this will produce the first output\\nrow of O but it's not really a row\\nbecause it's a made up of two rows uh so\\nthis stuff here is not one row it is two\\nrow and we can prove that because what\\nis P11 P11 is let's write it somewhere\\nso P11 is a 2X two Matrix uh yeah 2 by\\ntwo and we are multiplying it with V1\\nwhich is a block of two rows of V so it\\nis a two rows by 128 Dimensions so it is\\nequal to 2x\\n128 so this stuff here is 2 by\\n128 so this block here the output block\\nthat we are Computing is a block of two\\nrows of the output Matrix that we are\\nComputing uh I know this is really uh\\ndifficult to follow because we are\\ninvolving blocks so we need to visualize\\nat the same time metrics as blocks and\\nas the original metrix that's why I\\nhighly recommend you to pause the video\\nthink it's through write down whatever\\nyou need to write down because it's not\\neasy to find follow it just by\\nmemorizing the shapes so you you\\nactually need to write down things\\nanyway we are Compu the first output\\nblock of the output o Matrix now if we\\nif you remember uh the output um the\\noutput this output here should be the\\noutput of the output of the soft Max\\nmultiplied by V now this soft Max has\\nnot been applied to the entire row of\\nthis Matrix here s Matrix here but\\nbasically to compute this soft Max star\\nwhat we did was to compute the soft Max\\nstar at each block independently from\\nthe other blocks which means that the\\nmaximum that we are using to compute\\neach soft Max star is not the global\\nmaximum for the row of this s Matrix but\\nthe local maximum of each block and this\\nis wrong actually because when we\\ncompute the soft Max uh we apply the\\nsoft Max we should be using the global\\nrow I want to give you an example\\nwithout using blocks because otherwise I\\nthink it's not easy to follow so when we\\ndo the normal attention so we have a\\nquery multiplied by the transpose of the\\nkeys this produces a matrix that is n by\\nn so sequence by sequence where each\\nelement\", mimetype='text/plain', start_char_idx=64300, end_char_idx=68330, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='00c07ab5-22d6-4c11-9826-a9a08a8461b1', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9744aaa2-d04d-42f5-9131-2cba8208beda', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='493864e1e8c9191b1aafa0195d126024402ff8a1cb87131401c0f115918be2d1')}, text=\"of this Matrix so let's say\\nthree four five I don't know how many is\\none two three four five six yeah six\\ntwo 3 four and five six should be 1 2 3\\n4 5 6 okay this one here should be the\\ndot product of the first query with the\\nuh first um let me use because query one\\ntranspose the\\nkey1 uh this is because as I said before\\nwhen we do the product of two vectors we\\nalways treat them as column vectors so\\nwhen you want to write the DOT product\\nyou cannot multiply two column vectors\\nyou need to multiply one row Vector with\\none column Vector that's why we\\ntranspose this one if it confuses you\\nyou can also write q1 K1 that's totally\\nfine it's just uh wrong from a notation\\npoint of view anyway the first one will\\nbe the dot product of the query one with\\nK1 the second element will be the dot\\nproduct of the query one with K2 the\\nthird will be the query one with K3 etc\\netc etc um\\nso this is q1 with K1 q1 with\\nK2 K2 and q1 with K3 q1 with\\nK4 um\\nanyway when we do the soft Max we\\nactually calculate the maximum on this\\nentire row however what we are doing is\\nwe are actually doing a block matrix\\nmultiplication and as you remember\\num when we do by blocks we are grouping\\ntogether rows of queries and rows of\\nkeys and in this particular case we are\\ngrouping the two queries together to\\ncreate one uh one group of queries and\\ntwo keys together to create one block of\\nkeys so we need another row of this one\\nso it's the let me ose query one k or\\nquery 2 K1 this should be query 2 2\\nK1 query 2 K2 query 2 K3 query 2 K 4\\nquery 2 K5 and query 2 K\\n6 um when we each of this each of this\\nblock here is\\nComputing this block here is Computing 2\\nby two elements of the original Matrix\\nif we had never applied the blocks so it\\nis\\nComputing these two four elements\\nhere and if we apply the soft Max star\\nto each of these blocks we are not using\\nthe maximum element in this row we are\\nonly using the maximum element in each\\nblock which means that when we will use\\nit in the downstream product with v\\nMatrix we will be summing values that\\nare wrong because each of these values\\nhere will be based on a maximum that is\\nnot the global maximum for this row it\\nis the local maximum of this block here\\nand um and this block here will have the\\nglobal the it will use the local maximum\\nof this block here and this block here\\nwill use the local maximum of this block\\nhere etc etc etc so what I'm trying to\\nsay is that when you sum P11 with V1 P11\\nmay have some Maximum local maximum that\\nis different than from the local maximum\\nof p12 and p13 may have a different\\nmaximum local maximum that of P1 P11 and\\np12 so we need to find a way to fix the\\nmaximum that was used to compute the\\nexponential here with the maximum found\\nhere in case the maximum here is higher\\nthan the one local to P11 so if we have\\nfound for example here a Max maximum\\nthat is higher than the maximum used\\nhere here then we need to fix this one\\nand this one because that maximum in the\\nsoft Max should be the maximum for all\\nthe row not the one belonging to the\\neach\\nblock and this leads to our next step\\nhow to fix\\nthis first of all let me introduce a\\nlittle pseo code for computing this\\noutput Matrix here which is an output\\nblock Matrix and later we will use this\\npelo code to adjust the error that we\\nhave made in some blocks in case the\\nfuture blocks so the p13 has a better\\nmaximum than P11 or p12 so to compute\\nthis output Matrix\\no we go through so for example to\\ncompute the first row we choose well P11\\nis what is is um let's go back P11 is\\nlet me delete also this one it's not\\nneeded anymore P11 is the soft Max star\\nof q1 K1 p12 2 is the um soft maxar of\\nq1 K2 p13 is the soft maxar of q1 K3 p14\\nis the soft Mark star of q1\\nK4 um which means that uh to compute\\nthis block here here we first need to\\ncompute the P11 what is P11 well P11 is\\nthe soft Max star of a block of Q and\\nanother block of k k which in\", mimetype='text/plain', start_char_idx=68331, end_char_idx=72192, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='57ebc17b-3627-4bd4-af41-ebe9f7fd4750', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='00c07ab5-22d6-4c11-9826-a9a08a8461b1', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='3529164f623a5e4ac65c951a62c821a874a00e73bbd20f975a392f4438bdf477')}, text=\"the case\\nof the first first row of the output\\nMatrix means that it is the query one\\nwith the soft maxar of the query one\\nwith key1 the soft maxar of the query 1\\nwith K2 the soft maxar of the query 1\\nwith K3 the soft maxar of the query 1\\nwith K4 which means that we need to go\\nwe need to make a for Loop through all\\nthe keys while keeping the query fixed\\nso to compute the first output row we\\nneed to do the soft maxar to produce P11\\nwe need to do the soft ma out of query 1\\nK1 and um we sum it initially to zeros\\nbecause we don't um we need to\\ninitialize our output somehow and we\\ninitialize it with\\nzeros then we sum the next P1 to which\\nis the query one with the K2 and then we\\nsum the next p13 which is the query one\\nwith the K3 etc etc that's why we have\\nthis inner loop\\nhere all right so however this output\\nthat we Computing is wrong because I\\ntold you we have computed the softmax\\nstar using a statistics the maximum\\nvalue that is belonging to each block\\nand not the one that is the overall row\\nof the original Matrix how to fix that\\nwe have a tool actually we have computed\\nbefore an algorithm called the online\\nsoftmax I don't know if I referred to it\\nbefore as the online softmax but it's\\ncalled the online softmax that allows to\\nfix previous iterations when we are\\nComputing the current iteration\\nBas how well let's review the online\\nsoftmax we start imagine we are working\\nwith one single Vector so we are a\\nvector made up of n elements the what we\\ndo is we do a for Loop where we compute\\niteratively the maximum up to the he\\nelement and we fix the normalization\\nfactor uh computed in previous iteration\\nin case we found a better maximum at the\\ncurrent element if this is not clear\\nguys go go back and watch the online\\nsoft Mar because this is very important\\nbecause this is what we are going to use\\nto fix this P11 p12 blocks in case we\\nfound better maximum in p13 or p14\\nEtc so let's see how to apply this\\nonline soft Marx to this case here so\\nthat we can compute so you may be\\nwondering why are we going through all\\nthese troubles I mean\\nwhy the real reason is when first of all\\nwhy did did we introduce block matrix\\nmultiplication because we want to\\ncompute matrix multiplication in\\nparallel so you can think that each of\\nthis P11 because they are independent\\nfrom each other and because each of them\\nare using the maximum belonging to each\\nblock they can be computed independently\\nfrom each other then however we need to\\nsomehow aggregate their value and to\\naggregate their value we need to fix the\\nvalues that have been calculated\\nindependently because we didn't when\\nComputing values independently we don't\\nhave a global view we have local view so\\nwe compute local blocks P11 p12 p13 etc\\netc and then when we aggregate these\\nvalues we need to fix them so that's why\\nwe are trying to uh come up with this\\nsystem of fixing uh values that have\\nbeen calculated\\nindependently so how to fix this let's\\nlook at the following algorithm first of\\nall um this O Block here as I said\\nbefore it is a block of two rows\\nwhere each row is made up of 128\\ndimensions and we have seen that before\\nby checking the dimensions of P11 and V1\\nthe result of P11 V1 which means that\\nfor each output block we need to take\\ncare of two maximums uh and two\\nnormalization factors so up to now I\\ndidn't use the normalization factor we\\nsaid that we are applying softmax star\\nwhich is the soft Max without the\\nnormalization but eventually we will\\nneed to compute this normalization so we\\nwant to create an an algorithm that\\nfixes the maximum used to compute each\\nof this P11 and also computes\\nsimultaneously the normalization factor\\nand at the end we will apply this\\nnormalization factor and the way we will\\ndo it is as follows we start with\\ninitializing the maximum to minus\\ninfinity one for each row that we are\\nComputing so is our output block is made\\nup of two rows so we need one maximum\\nfor the top row and one maximum for the\\nbottom row and also the normalization\\nfactor which we initialize with zero\\nbecause we didn't sum anything for now\\nand the output we initialize it with all\\nzeros because we didn't sum anything to\\nthis output for now we\\ncompute the we uh to compute the output\\nrow so this output block here so this\\noutput block here we need\", mimetype='text/plain', start_char_idx=72193, end_char_idx=76432, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='223f34d7-946c-4f95-9e92-d588b7e1902c', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='57ebc17b-3627-4bd4-af41-ebe9f7fd4750', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='5691c292d94a9beea7719f809f2adec50e1a472e2a876ce09772cafbfc4b83af')}, text=\"to go through\\nall the keys uh to produce this P11 p12\\np13 p14 while the query is the query\\nnumber one the query block number one so\\nthe first step that we do is we compute\\nthe maximum of the first block\\nP11 which is the row Max so the maximum\\nfor each row of the block um uh uh q1 K1\\nthis is not P11 it's S1 sorry guys this\\nis\\nS11 so we compute the maximum of this\\none and we call it actually S1 as you\\ncan see\\nhere um then we can um calculate P11\\nwhich is the soft Max star which is the\\nexponential of the query multip query 1\\nK1 so S1 minus the maximum in the local\\ngroup\\nS1 and we add it to our\\noutput for now the output is initialized\\nwith zero so for now ignore this part\\nhere I will explain it later so for now\\no1 should be equal only to P11 V1\\nnow at the step number\\ntwo we may find in the local group S12\\nso this one is\\nS12 we may find a better maximum for the\\ntop row and the bottom row and this\\nmaximum is the M2 which may be better\\nthan the previous maximum for each of\\nthese two row but may also not be so we\\nneed to find a way to fix in case it's\\nbetter and to not fix anything in case\\nit's not better\\nand the way we do it is this so we\\ncompute the new maximum of the current\\nlocal row query\\n2 we um calculated the p12 which is the\\nsoft Max star of S2 which is S2 minus M2\\nwhich is the local maximum and then we\\nneed to add it to the output however in\\nthis case we may have found a better\\nmaximum so how to fix the o1 which on\\nonly use the maximum that was local to\\nS1 well we know that we can fix that by\\nusing exponentials because each of these\\nelement of o1 is just an exponential\\nwithout the normalization because we are\\napplying soft Max star so how to fix an\\nexponential with another exponential so\\nbasically we are saying that uh we\\nmultiply o1 which is a matrix so let me\\nshow you what is this Matrix so o1\\nis a matrix made up of two rows so as\\nyou can see here I have the shape of o1\\nit's a 2 by 128\\nMatrix so this is the top row so 011 012\\nblah blah blah until\\no1\\n128 then\\no21 o22 up blah and\\no21\\n128 we need to fix this value how we\\nbasically\\njust using the exponential that we have\\nused in the online softmax that we have\\nseen before so if we multiply this\\nMatrix here by a diagonal matrix that is\\nmade as follows it's a diagonal matrix\\nmade up of two elements because the\\nexponential of M1 minus M2 will be a\\nvector of two elements and the\\nexponential of a element y exponential\\nis another Vector of two elements and\\nthis diag basically means that diagonal\\nMatrix where in the diagonal we have the\\nelements of the vector to which we are\\napplying this diag operation which means\\nthat this value here will be the\\nexponential of the first element of\\nM1 so let me show you uh how to write it\\nexponential\\nof M1 minus M2 minus M2 so the first\\nelement so let's call it one here here\\nis a zero here will be zero and let's\\ndelete this one and we write another one\\nhere\\nexponential M1 minus M2 but the second\\nelement of this Vector so basically the\\nDI this notation here diag means\\nbasically just take the vector and\\ndistribute it over a n byn Matrix where\\nn is the size of the vector to which it\\nis applied and all the other elements of\\nthis Matrix should be zeros this is what\\nthis\\nmeans if we do this operation here we\\nwill see that the output of this\\nmultiplication will fix each element of\\nthe top row using this exponential and\\nthe bottom row with this exponential\\nwhich will basically cancel out this M1\\nthat was computed in the previous\\niteration and introduce the M2 that we\\nhave computed in the current iteration\\nin each of these o elements in this o\\nblock\\nMatrix okay so this output will be this\\nelement will will multiply by this one\\nso it will fix o11 with this Factor here\\nand o1 21 will not be fixed by will be\\nmultiplied by zero so it will not\\ncontribute to this first output element\\nso this element here will only depend on\\no11 fixed by the exponential of M1 minus\\nM2 but the first element of this vector\\nand then\", mimetype='text/plain', start_char_idx=76433, end_char_idx=80379, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='c314db95-e0b9-44a8-bd64-cb9ebf6db2af', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='223f34d7-946c-4f95-9e92-d588b7e1902c', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='d2de6f9ee8094f7de3813d886a4ace4bb574ce278c32ca598d174ef59f18d58a')}, text=\"o12 will also be fixed by um\\no12 will be fixed by this exponential\\nhere but not by this one and all the\\ndimensions of the first row will be\\nfixed by this exponential and all the\\ndimensions of the second uh row here\\nwill be fixed by this uh exponential\\nhere this this scalar here which is the\\nsecond element of the vector X of M1\\nminus\\nM2 or okay it was really challenging\\nthis one so so what we are doing is we\\ncompute p12 and we fix all the elements\\nin P1 by multiplying by this Matrix here\\nby multiplying by this Factor here\\nMatrix Factor here and when we will\\ncompute uh step three we will fix step\\ntwo etc etc etc now let's talk about the\\nnormalization factor because for now we\\nhave been ignoring it um the\\nnormalization factor is something that\\nwe can compute while Computing this\\nmaximums because it is provided in the\\npseo code of the online algorithm that\\nwe have seen before for the soft Max so\\nwhile Computing the maximum we can\\nactually compute the uh normalization\\nFactor by fixing the normalization\\nfactor of the previous iteration and\\nthis is exactly what we are doing here\\nso at the first iteration we comput the\\nnormalization factor using the local\\nmaximum and at the second iteration so\\nyou can for now ignore uh this one\\nbecause we are not fixing l0 with\\nanything because l0 will be zero so we\\nare just basically\\num we are just Computing this summation\\nhere so l0 will be zero so this Factor\\nhere will be\\nzero um and when Computing L2 so the\\nnormalization Step at the second\\niteration we will fix L1 with an\\nexponential which guess what it's\\nexactly the same exponential that fixes\\nthe maximum uh the P11 so it is the\\nprevious estimation of the maximum minus\\nthe current estimation of the maximum\\nplus the new uh normalization factor\\nusing the local maximum and we keep\\ndoing this job at the end we will obtain\\na correct output for this uh metrix for\\nfor this uh block here but without the\\nnormalization how to apply the\\nnormalization well the normalization is\\nsomething that is um we need to divide\\neach element of this o by the\\nnormalization factor but because we are\\nkeeping while iterating through these\\nfor Loops we also calculate the\\nnormalization factor we keep\\naccumulating it until we reach the end\\nof the iteration and then we apply the\\nnormalization factor so we take the last\\noutput and we just divide it by L4 which\\nis the normalization factor calculated\\nat the fourth iteration and that will\\nfix the soft Max all right guys so now\\nthat we have derived the algorithm of\\nhow to compute this output of the\\nattention blockwise why also fixing the\\nsoft Max which is done independently in\\neach single block we know that the\\nnormalization is done at the end I want\\nto also prove it so what we done when we\\nintroduce this algorithm that computes\\nthe soft Max in an online way we proved\\nby induction that this algorithm is\\ncorrect so at the end of this algorithm\\nthis L of the last iteration will\\nactually be the normalization factor\\nthat we can appli to get the soft Max so\\nwe don't apply the normalization while\\nComputing this output in an online way\\niteratively way by multiplying the query\\nwith all the blocks of keys we apply it\\nat the end of this four iteration and at\\nthe end of this four iteration we will\\nhave the last\\noutput um and we also know that the last\\nL will contain the exact normalization\\nfactor that we need to apply to each row\\nbecause this o of four is a block of\\noutput rows which is if you remember\\nfrom the attention mechanism each output\\nthe output of the attention has the same\\nshape as the input query Vector which is\\na sequence of tokens so this O is a\\nsequence of tokens that we need to apply\\nthe normalization to and we know that\\nthe correct factor is L4 so let's prove\\nthis simple formula uh L4 is a vector\\none for that contains as many elements\\nas there are rows in o4 so in this O\\nBlock uh of\\nrows suppose that it contains uh two\\nrows like in the algorithm that I have\\ndescribed so far in which we pretend\\nthat we are grouping two rows of queries\\nwith two columns of keys together so the\\noutput o uh um the block O will contain\\ntwo rows of the output so we will have\\ntwo normalization factor in this L4\\nVector here what we are doing with this\\nformula is we are taking this L4 vector\\nand we are creating a diagonal matrix\\nwith it and then we are Computing the\\ninverse of this diagonal matrix so L4 is\\na vector that contains two normalization\\nFactor so it's l i don't\", mimetype='text/plain', start_char_idx=80380, end_char_idx=84803, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='8a96fdc7-a8da-48aa-ab7c-f5534438c890', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c314db95-e0b9-44a8-bd64-cb9ebf6db2af', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='d89c8bd752cd437a3a4e07228493aeb694f82709df64f34db20365a47833bf76')}, text=\"know let's call\\nit l L4 4 element 1 and L4 element\\n2 this is our\\nL4 uh Vector then we have 04 o4 is a\\nmatrix as you can see from the shape is\\n2 by\\n128 Matrix so o is let's copy it\\nactually oh no let's not copy it o4 is a\\nmatrix that is two\\nrows with 128 elements so so the first\\nrow with 128 dimensions and the second\\nrow with 128 Dimensions the first thing\\nthat we are doing with this L4 is we are\\nconverting it into a diagonal matrix\\nwhich will be a diagonal matrix 2x two\\nbecause it contains two elements so it\\nwill become something like this so it\\nwill be L4 the first element of L4 zero\\nand then zero L4 the second element of\\nthis\\nVector then we are Computing the inverse\\nof this Matrix the inverse of a diagonal\\nmatrix is just the diagonal matrix with\\neach element on the diagonal that\\nbecomes its\\nreciprocal uh this is from linear\\nalgebra it's not I'm making it I'm\\nmaking this up so uh the inverse of this\\nMatrix here is equal\\nto uh the same uh diagonal matrix but\\nwhere each element is one over L4 the\\nfirst element of L4 zero 0 and 1 over uh\\nL4 the second element of L4 and then we\\nare multiplying this stuff here so let\\nme delete some stuff so this stuff here\\nis getting multiplied by\\no which is a matrix that is 2 by\\n128 so we are doing this multiplication\\nnow\\nmultiply now the output of this so this\\nis two let me WR it 2x 2 * 2x 128 will\\nbe a matrix that is 2x\\n128\\nwhere the first um dimension of the\\nfirst row of the output of this\\noperation will be the dot product of\\nthis col this row here with the First\\nColumn so basically we are dividing this\\nelement here by L4 the first element of\\nL four the second uh output element here\\nwill be the dot product of this row with\\nthis second column so we are only\\nmultiply we are dividing the the the the\\nsecond element here of this input Vector\\nhere by L4 the first element of L4\\nbecause the all the elements of the\\nsecond row will be multiplied by zero so\\nthey will not contribute to this output\\nrow while the second output row will be\\nthe dot this element here will be the\\ndot product of this row with the First\\nColumn the first element\\nhere is multiplied by zero so it will\\nnot contribute to this output so it's\\nonly the second ele the first row of the\\nsecond the first element of the second\\nrow of the input Matrix here will be\\ndivided by\\nl42 so basically this will be applied\\nwill divide all the elements in the\\nsecond row and this will divide all the\\nelement in the first row in producing\\nthis one here which is exactly what we\\nneed to do when we want to normalize we\\nneed to apply this normalization factor\\nand this should uh help you better\\nvisualize why this operation is\\nnormalizing the vectors of the output at\\nthe end and still obtaining the same\\nresult now let's proceed further all\\nright guys finally we are ready to see\\nthe flash attention forward pass uh by\\num also comparing it with what we have\\nderived so far so if you look at the\\nflashh attention paper first of all this\\nis the flashh attention to forward pass\\nand later I will explain what are the\\ndifferences between the flashh attention\\none and the flesh attention two um I\\ndidn't want to jump directly to this uh\\nforward pass because I believe that even\\nif the derivation like the derivation\\nwas a little uh difficult to follow I\\nbelieve that it gave you some intuition\\ninto what is happening so even if you\\nunderstand 50% of it that's enough\\nbecause later we will also code it and\\nyou should reach like a 90% of\\nunderstanding so every time we introduce\\nsome new information it should improve\\nyour be the the your understanding so\\nbasically in flesh attention what we are\\nflesh attention to especially we we take\\nour um as input we have our uh query\\nKean values which are sequence of tokens\\neach token is made up of a vector of D\\ndimensions and D lower case D dimensions\\nand we divide this query guess what into\\nblocks in how many blocks well depending\\non this parameter BR which is the size\\nof the query block that we want to\\nchoose so how many rows of query we want\\nto group together into one block and we\\nalso do it with K and V and we divided\\nthat into um blocks of uh um depending\\non this parameter BC then we also\\ninitialize\", mimetype='text/plain', start_char_idx=84804, end_char_idx=88964, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='3d7a2a18-7368-444f-b255-ae2f0b431c9d', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8a96fdc7-a8da-48aa-ab7c-f5534438c890', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='d23062fa5973f2f88422b3c060d742616c14a07d57799db37e3e4cbabeeaa722')}, text=\"the output which is the\\noutput that we want to produce so the\\nwhat is the flashh attention Computing\\nwell the flashh attention um is\\nComputing the following so it's\\nComputing the uh soft\\nMax soft Max of the query multiply by\\nthe transpose of the keys divide by the\\nsome normalization Factor uh multiply\\nthat by V and um so that's what it's\\ngoing to compute and it's going to\\ncomputed this way first of all there is\\nan autor Loop through the queries which\\ncorresponds to the same pelo code that\\nwe have seen before because we want to\\ncompute each block of the output\\nMatrix um in parallel with the with with\\nrespect to the others so basically we\\nwant to compute this output block and\\nthis block all output block\\nindependently this output block here\\ndepends on the query one and all the\\nkeys this output block here depends on\\nthe query 2 and all the keys this output\\nblock here depends on the query three\\nand all the keys where query 1 is not\\nthe first query but it's the first group\\nof queries or first block of queries\\nquery two is not the first um query two\\nis not the second row of the query\\nMatrix but it's the second block of the\\nquery Matrix etc etc um so that's why we\\nhave this outer um outer uh iteration\\namong all the blocks because we want to\\ncompute all those blocks of the output\\nMatrix in parallel but to compute each\\nof these output block we need to go to\\nan iteration among all the keys that's\\nwhy we have an inner loop on the keys\\nand we do exactly the same operation\\nthat we have done so far uh by hand so\\nfirst we compute the S Matrix which is\\nwhat the each block of query with the\\ncorresponding block of the keys then we\\ncompute the local maximum to the current\\ns block this is the local maximum and we\\num compare it with the maximum of the\\nprevious iteration because that's what\\nwe do in the online softmax then we\\ncompute the\\nP the P block which is the softmax star\\nof the S block minus the local maximum\\nof the S\\nblock then we compute the uh\\nnormalization Factor what is the\\nnormalization factor it is the summation\\nof all the exponential of the soft Max\\nstar but um uh by fixing the\\nnormalization factor of the previous\\nstep and we know how to fix the\\nnormalization factor because we just\\nmultiply by an exponential which is the\\nprevious maximum minus the current\\nmaximum that's what this factor is and\\nthen we computed the output exactly\\nusing the same uh correction factor that\\nwe have seen before which is the\\ndiagonal matrix made up of the diagonal\\num where on the diagonal you have the\\nelements of this Vector here which is\\nthe exponential of the previous maximum\\nminus the current maximum multiplied by\\nthe output of the previous step because\\nwe want to fix the previous step because\\nit was based on the previous P which was\\nusing the maximum of the local previous\\nP plus the current p v which is based on\\nthe current local maximum and it will be\\nfixed by the next\\niteration okay okay and at the end after\\nwe have gone through all the cas so we\\nhave computed all the output block but\\nwe didn't apply the normalization factor\\nand it's applied at the end because\\nwhile going through each key we are\\ncalculating the L normalization factor\\nfor the softmax because inside of this\\nfor Loop we are just Computing the\\nsoftmax star so we are not normalizing\\neach value so at the end someone has to\\nnormalize it and it will be this um this\\nthis um instruction here which is use\\nthe normalization factor that we have\\ncomputed over all the iterations and\\napply it to each element of O because\\nthe difference between the softx star\\nand the actual sofx is just the division\\nby the um the normalization factor and\\nthis instruction here is actually\\ndividing each o with the corresponding\\nnormalization Factor one for each row of\\nthe block each row in the output block\\nthat we are\\nComputing uh later we will see also what\\ndo we do what is what does it what is\\nthis SRAM what is the hbm for now I just\\nwant you to concentrate on the um\\noperations that we are doing and they\\nare exactly the same operations that we\\nhave done so far uh later we will see\\nalso why do we need to save this stuff\\nhere and etc etc but for now you should\\nhave enough knowledge to be able to\\nfollow what is written in the flesh\\nattention paper for with respect to the\\nforward pass algorithm and uh what we\\nare doing basically is just block matrix\\nmultiplication and while Computing this\\nblock we fix the previous block by using\\ntricks of the\\nexponential all\", mimetype='text/plain', start_char_idx=88965, end_char_idx=93408, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='183ecf07-d31b-44f7-a5c3-bf84b9eb2ef1', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='3d7a2a18-7368-444f-b255-ae2f0b431c9d', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='04c6ef4cb97c1eafd0269bd095a72f209b73039381734dc6402358642b05ecf3')}, text=\"right now that we have\\nseen forward Paths of the flashh\\nattention before we can implement it we\\nstill lack a little bit of knowledge\\nbecause we don't know anything about the\\ngpus and we don't know anything about\\nCuda and we don't know anything about\\nTriton so that's what we are going to\\nsee\\nnext all right guys it's time for us to\\nexplore finally the GPU and the Cuda\\nprogramming model well uh let's start by\\ncomparing the CPU and the GPU and this\\nwill let us understand how Cuda works\\nthen so first of all what is the Cuda\\nand what is the GPU the GPU is the\\nhardware unit that we are that we buy\\nand Cuda is a software stocks made by um\\nmade by Nvidia or to write software for\\nthis GPU that they sell AMD has its own\\nsoftware stock and other manufacturer\\nhave their own in this particular video\\nwe will be seeing example of Cuda\\nkernels but the knowledge that you will\\nget can apply also to other gpus now the\\nfirst difference between a CPU and a GPU\\nis its purpose uh the GP the your\\ncomputer is right now running on a CPU\\nand your operating system is interfacing\\nwith the CPU um in using the uh the the\\nso-called scheduler so right now\\nprobably you are running a browser you\\nare also running some other software on\\nyour computer\\non your computer and the scheduler is\\ntasked with switching between them very\\nfast on your CPU in such a way that it\\nlooks like to you that the processes are\\nrunning\\nconcurrently uh this actually is a fake\\nkind of parallelism unless your CPU also\\nhas multiple cores which nowadays CPUs\\ndo have so a CPU usually has one or\\nmultiple cores but not so many of them\\nso usually have dual core of quad core\\nor eight core CPU and each of the this\\ncourse can execute instructions in\\nparallel um the CPU is tasked the the\\nmain purpose of the CPU is to execute\\nmany different task uh and switching\\nbetween them very fast so maybe you have\\na browser that is running a small game\\nand then you have another movie player\\nbut then you have a word processor and\\nthen you maybe have some uh utility to\\nmanage your um to download files Etc so\\nmost of these programs actually are not\\ncomputing intensive are actually iio\\nbound meaning that most of the time they\\nare either waiting for the network or\\nthey are waiting for the dis and they\\nare very different from each other in\\nthe purpose so a browser is completely\\ndifferent from a movie player and it's\\ncompletely different from a word\\nprocessor um so the job of the CPU is to\\nactually reduce the latencies of\\nprocessing all these operations and it's\\nhighly optimized to process to optimize\\neach of these execution unit called the\\ncourse which means that each course\\nhas a part that is tasked to understand\\nfirst of all what is the next\\ninstruction to run or to to predict the\\nbranch of how the what the next uh\\noperation may be based on the conditions\\nthat you are running for example if you\\nhave a if condition the branch predictor\\ncan understand what is the more most\\nlikely next instruction and can do some\\noptimizations uh also the CPU is has a\\nlot of caches to reduce the latencies in\\nloading data from all the devices it can\\ninterface with it can interface with the\\nuh the RAM for sure but it can also\\ninterface with the dis it can also\\ninterface with some peripherals like the\\nprinter like the mouse like the keyboard\\netc etc on the other hand the GPU is not\\ntasked to do many different things at\\nthe same time but it's task to do one\\nthing or few things but on a massive\\namount of data so the operations that we\\ndo on the GPU are uh requires a lot of\\ncomputation and for that for this reason\\nmost of the area so the physical area of\\nthe GPU is dedicated to compute units so\\nthis green stuff that you can see here\\nand these are called\\ncourse um and you can see that the part\\nthat is dedicated to the control area so\\nthe part that is um tusked with\\nunderstanding what is the next\\ninstruction to run or to do some\\noptimization in this the program is very\\nlittle uh you may be thinking well uh\\ndoes it make it does it make the GPU uh\\nless fast compared to the GPU to the CPU\\nwell not really because we have many\\nmore coures that can compensate for this\\num higher\\nlatencies um okay I can give you a lot\\nof knowledge about the GPU from a\\ntheoretical point of view I think the\\nbest way to understand the Cuda\\nprogramming model is just to jump into\\nthe code so we don't get bored okay\\nimagine\", mimetype='text/plain', start_char_idx=93409, end_char_idx=97766, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='f1efa636-27bf-479d-8be9-83ce529fd8c5', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='183ecf07-d31b-44f7-a5c3-bf84b9eb2ef1', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='ad61ebff4d2c6cd5b279ea37c9f10472771c181856ca69a80f331312ce367c36')}, text=\"we have a very simple task and\\nwe have a\\nvector we have two vectors A and B and\\nwe want to calculate the sum of these\\ntwo vectors vectors into and save the\\nresult into another Vector C where each\\nitem is the element wise sum of the\\ncorresponding item of A and B how would\\nyou proceed with this task on the CPU\\nwell you would do a for Loop for example\\nso for example uh you would uh make a\\nfor Loop that starts from the first\\nindex so the index zero and C of0 is\\nequal to a of 0 plus b of0 then C of 1\\nis equal to uh a of 1 plus b of one Etc\\nand you do a for loop on all the\\nelements of this\\nVector in the GPU we want to do the same\\noperation but in parallel because we\\nhave a lot of compute units called\\ncourse and we want all of them to work\\nin parallel so the first thing that we\\nneed to understand is how to divide the\\nwork that we are going to do into\\nsubunits of work and dedicate each core\\nto one subunit one simple subdivision\\nwould be okay the first core should do\\nthis summation the second core should do\\nthis summation the third core should do\\nthe summation etc etc so imagine we have\\na eight element Vector we need eight\\ncourse to do this element wise\\nsummation we will call the course\\nthreads because it should also remind\\nyou of the multi- threading uh that we\\nalready use in operating system so\\nmultiple threads work concurrently on\\nthe same uh on the same or similar job\\nin the GPU let's look at the code now\\nthe code that I am going to show you is\\nAuda kernel and it's written in C but\\nyou don't have to understand C and you\\ndon't have to understand this code what\\nI want you to understand is the\\nintuition behind it because later we\\nwill need this knowledge and convert it\\ninto Tron which is Python and you should\\nalready be familiar with\\npython so let's go to the code and I\\nhave a very simple vector addition uh we\\ncan see it\\nhere okay uh first of all how to do a\\nvector summation usually the GPU is\\ninterfaced with a CPU and the CPU has to\\nfirst of all um tell the GPU what is the\\ndata it he's going to work with so the\\nCPU needs to have these vectors it needs\\nto transfer them to the GPU then the GPU\\nneeds to do this Vector summation then\\nthe CPU has to copy back the information\\nfrom the output from the GPU to the CPU\\nand then make it available to the\\nprogram this is what we are going to do\\nhere so we are going to allocate three\\nvectors of size n one called a one\\ncalled B and one is the output Vector we\\ninitialize their items um randomly so a\\nof I uh is a random number between zero\\nand 100\\nexcluded then we allocate memory on the\\nGPU to hold these vectors and then we\\ncopy them to the GPU so we copyed the a\\nvector to the GPU and the B Vector to\\nthe GPU of course we don't copy the\\nresult because that's what we want the\\nGPU to populate with uh the output so we\\njust allocate it on the GPU what we\\ndon't copy uh our output Vector on the\\nGPU because it's it's made of random\\nvalues then um what we do is we launch\\nthe kernel the launching the kernel\\nmeans that we launch a program that the\\nGPU should execute in parallel on\\nmultiple threads or multiple course each\\nof these threads should do a unit of\\noperation a unit of work that is\\nindependent from the others actually\\nthey can be dependent on the other but\\nwe will not be talking about\\nsynchronization um so we launched this\\nkernel and what we are seeing in this\\nline is launch one block of threads and\\nlater we will see what are blocks but\\nyou can think of you can ignore this one\\nfor now what we are saying here is\\nlaunch n threads so n multi parallel\\noperations on with the following\\narguments so the output where we want to\\nsave data the input array a and the B uh\\ninput B and the number of elements let's\\nsee what happens inside of this method\\nthis method is following a particular\\nsyntax that is um um how to say Cuda uh\\nspecific So This Global is actually\\nadded it's like a superet of the C\\nlanguage where we have some additional\\nkeywords that belong to Cuda so it's not\\nreally C it's Cuda\\nc um so it's a very simple uh method as\\nyou can see and the first thing that we\\nneed to do is\\nCuda cannot know what each thread should\\ndo it's we should tell each\", mimetype='text/plain', start_char_idx=97767, end_char_idx=101908, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='2b2242f4-4485-4bfc-8d4a-4106d2281bc6', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='f1efa636-27bf-479d-8be9-83ce529fd8c5', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='e3ad7c68510b06c381ab84e736da70bccbeb4f846232a4a63ed4aab347ca0c24')}, text=\"thread what\\nto do so the mapping between the data\\nand the what each thread should do it's\\nup to us as software\\nengineer Cuda what we do is when we ask\\nit to launch n threads in parallel it\\nwill allocate n threads and assign a\\nunique identifier to each of these\\nthreads in our simple case we can see it\\nlike this so it will assign the first\\nthread the index zero so we are asking\\nfor example imagine we have a vector of\\neight elements it will assign the first\\nthread index zero here I call it one but\\nit's it's wrong but uh we can write\\nanother number here so this will be\\nactually tread zero this will be tread\\none this will be tread two tread three\\ntread four tread five tread six and\\ntread 7 so let me delete this\\none so we don't get confused\\nand what we are seeing is that the item\\nthat each thread should process is equal\\nto its thread index so this is the\\nthread zero so it should process the\\nitem with index zero this is the thread\\none and it should process the item with\\nindex one this is the thread number two\\nand it should process the item with\\nindex two and this is what we are doing\\nin this line of code we are saying which\\nitem each thread should process which is\\nexactly the it's uh the thread\\nidentifier so the thread ID uh later we\\nwill see why why we have this dot X but\\nthat's for later next thing that you\\nshould see is okay we are doing the\\noutput of the I position is equal to the\\na um Vector at the E position plus the B\\nVector at the E position so it's a very\\nsimple summation element wise you may\\nhave noticed this if statement why do we\\nneed an if statement if we already know\\nthat we are going to launch eight\\nthreads and uh of course I will be\\nbetween um we already know that we are\\ngoing to launch n threads so I should of\\ncourse be less than n because each\\nthread ID will be between zero and N\\nminus one so why do we need this if\\ncondition this is needed because when\\nyou um Cuda when it launches a number of\\nthreads this number of threads is always\\na multiple of um a unit which is 32 in\\nthe case of the Cuda so if we have like\\n34 elements in a vector and we ask Cuda\\nto launch 34 threads Cuda will not\\nlaunch 34 exactly it will launch 64\\nthreads so multiple of 32 uh which is\\nthe warp size by the way um and\\nU what we need to do is we need to ask\\nthese threads to only work for we only\\nneed to ask the threads that have a\\ncorresponding element to work and all\\nthe other that don't have a\\ncorresponding element because the the uh\\nthe vector is not large enough for all\\nof them to not do anything so do not\\nenter this uh uh if\\nstatement there is another thing that we\\nshould learn which is actually the\\nthreads um actually when we have a group\\nof threads in in um Cuda programming\\nmodel but I believe also in other\\ngpus um a group of threads of 32 threads\\nis called a warp and this 32 threads\\nwill share the same um control unit so\\nlet's go back to the slide so as you saw\\nas you can see here we have this yellow\\nunit here in the GPU and a group of\\nthreads will share the same control unit\\nwhich means that what is this control\\nunit it's a part of the hardware of the\\nGPU that is tasked with understanding\\nwhat is the next instruction to run now\\nif the group of threads is sharing the\\nsame unit it means that this group of\\nthread will always execute the same\\nstatement at any time they will always\\nwork in synchrony will always work on\\nthe same instruction it's it cannot be\\nlike this thread is working on one\\ninstruction and this one is working on\\nanother instruction what does this mean\\non a programming level it means that if\\nwhen we launch a group of threads of\\ncourse Cuda will spawn more threads than\\nwe need if the if the number of elements\\nof our Vector is not a multiple of\\n32 this means that when we This Thread\\nthey will first execute this uh\\noperation and each of them will have its\\nown uh value of this thread ID so they\\nwill execute the same instruction but\\nthe data at each instruction may be\\ndifferent because each of them have\\ntheir own registers which means that\\nthey will always they will for example\\nreach this statement here and the first\\nthread will have I equal to zero the\\nsecond thread will have I equal to 1 etc\\netc even if they are executing the same\\ninstruction this programming model is\\ncalled single\", mimetype='text/plain', start_char_idx=101909, end_char_idx=106166, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='4d606fae-7fca-4007-800d-7a8a1ff1690f', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2b2242f4-4485-4bfc-8d4a-4106d2281bc6', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='bef68beb6485d0be5f990dbde418d5ab8222ea1a1191570f527e3cc6b4b7098f')}, text=\"instruction multiple data\\nCuda likes to call it single instruction\\nmultiple thread doesn't matter for us it\\njust means that they will always execute\\nthe same instruction but the value of\\nthe variables may be\\ndifferent then after executing this\\nstatement they will reach this statement\\nhere the if statement and of course some\\nsome of them will evaluate this\\nstatement to true and some of them will\\nexecute the statement to false which\\nalso means that some of them should\\nenter this if statement and some of them\\nshould not enter this if statement\\nhowever because the control unit is the\\nsame for all of them they will be forced\\nto enter this if statement even if they\\nshould not so how Cuda manages this\\ncontrol Divergence it will basically\\nmake work like this all the threads for\\nwhich this if statement is equal to True\\nwill enter this if and will execute the\\ninstructions inside of this if and all\\nthe threads that have this statement\\nequal to false so the condition of this\\nif equal to false they will enter the\\nfor Loop because they cannot not enter\\nit because they should be always\\nexecuting the same instruction at any\\ntime but they will just not do any\\noperations inside of this for Loop they\\nwill just sit\\nidle this is um called control\\nDivergence and it can reduce the um the\\nthe throughput of your program so you\\nwant to minimize it um but you may be\\nwondering why doesn't the GPU dedicate a\\ncontrol unit to each core so that they\\ncan work independently from each other\\nbecause the control unit is expensive to\\nadd in the cheap area of the GPU it's\\nmuch more efficient to add more workers\\ninstead of adding control area control\\nunits for each worker so this is a\\ndesign choice of the GPU and it works\\nfine okay now that we have seen how a\\nkernel works Works let's move forward to\\nanother\\nexample all right the next example that\\nwe are going to see is the following is\\nthe same as the as before so we are\\ngoing to do a vector addition but\\nimagine that we have a very large Vector\\nso imagine that we have a vector with 1\\nmillion elements of course we could do\\nlike before so we launch a kernel with 1\\nmillion threads the problem is Cuda will\\nreject it because it say I don't have 1\\nmillion threads to run in parallel so\\nhow can we proceed in this case because\\nusually we are working with very big\\nmatrices or very big vectors so we need\\nto process a massive amount of data so\\nhow to manage a parallel um uh how to\\nsay parallel computation when we do not\\nhave enough uh computation course one\\nway is to divide the input Vector into\\nblocks of uh elements for example we may\\ndecide for example imagine our um GPU\\nonly has 32 cores in total we may divide\\nour input Vector into blocks of size 32\\nsuch that the first 32 element are the\\nfirst block the next 32 element are the\\nsecond block the third 32 element the\\nthird block and the last 32 element are\\nthe last\\nblock in this way we can ask the GPU to\\nwork on one block at a time so we can\\nsay okay work on the first block and\\nafter it has processed the first block\\nit can work on on the second block and\\nthen the third block and the fourth\\nblock this also allows the GPU itself to\\nmanage subunit of work because imagine\\nnow we have um blocks of 32 elements but\\nwe have a GPU of 64 cores the GPU we can\\nalso schedule two blocks at the same\\ntime because it has enough course so we\\nneed to give some granularity uh we need\\nto reduce the Gran uh increase the\\ngranularity of our data to let the GPU\\ndecide how many blocks to schedule this\\nis the reason we introduce blocks inside\\nof Cuda so let me make a concrete\\nexample but with a very simple\\nassumption imagine our GPU only has two\\ncores or let's say four cores actually\\nuh so we have uh n is equal to eight\\nelements eight and we have four cores in\\ntotal so what we can do for example is\\nto is divide this uh um Vector into\\ngroups of either four cores or even less\\nlet's say two two elements at a time so\\nthis is the block number one this is the\\nblock number two this is the block\\nnumber three and this is the block\\nnumber\\nfour we can ask Cuda to launch a kernel\\nthat is made up of four blocks and where\\neach block is made up of two threads so\\nwhen we launch the um Cuda kernel we can\\nshow the code now\\nwe ask the Cuda where is the\\ninstruction this first instruction tells\\nCuda how many blocks we have and the\\nsecond part of this um in this uh\", mimetype='text/plain', start_char_idx=106167, end_char_idx=110513, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='bcd57652-d3a8-4eae-8a76-f1d444a9cfe3', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4d606fae-7fca-4007-800d-7a8a1ff1690f', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='9fb7effc0fe7e1f3837a3a499991e66ad2b72ce98c2d83d848f83c15e052c52d')}, text=\"um\\nsymbols tells how many threads we have\\nfor each block in our case we want um\\nnide by the block size number of blocks\\nwhere the block size in my picture is\\ntwo so how many blocks we will will have\\nwe will have a number of blocks so the\\nnumber of\\nblocks is n / by two where two is the\\nblock\\nsize so this is the block size and this\\nwill be equal to four blocks each of\\nsize equal to two and this is what we\\nare doing here uh so we are saying that\\nthe number of blocks is okay the ceiling\\nbecause it may not be a multiple of the\\nblock size n of nide by the block size\\nand this tells how many blocks we have\\nand this is will be this will Define our\\ngrid it means the grid is basically\\ntelling how many blocks we have and then\\neach block is made up of block size\\nnumber of threads then the problem is\\nhow do we assign the work to do to each\\nof these threads when we launch a kernel\\nlike this with this configuration so the\\nnumber of blocks and the number of\\nthreads per block Cuda will do the\\nfollowing job it will assign this block\\num each block an index called the block\\nID where the block ID of the first block\\nis zero so let me write here so this\\nwill have the first block will have a\\nblock ID equal to zero and in each block\\nit will assign a thread ID and the\\nthread ID of the first thread of each\\nblock will be the thread zero and the\\nsecond thread will be the thread number\\none the second block will have block\\nID block ID equal to one and the first\\nthread of this block will be the thread\\nnumber zero and the second thread of\\nthis block will be the thread number one\\nthe third block will have a block\\nID block ID equal to two and the first\\nthread will be the thread number zero\\nand the second thread will be thread\\nnumber one Etc until the last block\\nwhich will be equal to three this will\\nbe thread number zero and thread number\\none the problem is now based only on the\\nindex of the block and the index of the\\nthread how can we map it to what element\\nof the vector each thread should work\\nwith one simple assignment would be to\\njust do uh well you can see that in this\\ncase we need the uh this Vector um This\\nThread here to work with element zero\\nthis one should work with element one\\nthis one should work with the element\\nnumber two this one to the element\\nnumber three this one four this one\\nfive six and seven this five is so ugly\\nso let me write it\\nagain how can we find the mapping given\\nonly the block ID and the thread ID how\\ncan we find Which element it should\\ncorrespond to well it's very simple\\nformula so you can see that the\\nelement let's call it the element ID\\nwhich in the code I call it I is equal\\nto the block\\nID multiplied by the size of each block\\nwhich is block size let's call\\nit block size yeah I have it block\\nsize plus the tread\\nID because in the case of the first\\nthread this will be equal to 0 * 2 + 0\\nwhich is 0 in this case it will be equal\\nto 0 * 2 which is 0 + 1 and it will be\\nequal to 1 in this case it will be equal\\nto 1 because block ID is equal 1 1 * 2\\nis equal to 2+ 0 is equal to 2 etc etc\\nand you can see that this formula works\\nfor all the threads so the mapping when\\nwe launch a Cuda kernel we are telling\\nthe GPU how many blocks we want and how\\nmany threads there are in in each block\\nbut Cuda has no notion of how to map\\neach um Cuda has no way of knowing how\\nto map each um thread into the element\\nit should work with that's up to us and\\nthat's what we are doing here when we\\nare creating this um uh Kel here so we\\nare telling that the each element each\\nthread should work with the E element of\\nthe vector where I is calculated as\\nfollow the block ID to which This Thread\\nbelongs multiplied by the block size so\\nhow many threads there are in in each\\nblock plus the thread ID um and this\\nwill tell the I element this particular\\nthread should work with by giving uh in\\nlet's go back to the slides by choosing\\nthe block size equal to two and having\\nfour cores the GPU can choose to run one\\nblock or two block concurrently if it\\nhas enough free course so that's why we\\nwant to work with by block by block\\nbecause it allows the GP you to choose\\nhow it want to\", mimetype='text/plain', start_char_idx=110514, end_char_idx=114616, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='d3dd5e96-bb6b-43d0-b360-de1bf9261a19', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='bcd57652-d3a8-4eae-8a76-f1d444a9cfe3', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='7442c9054fae82b1c38a197b2582d31d250e2310ba7adae5c73f5adbecf4d1e2')}, text=\"parallelize the\\noperations if it has enough course and\\nwe don't need to have n course for n ve\\nn element Vector we can divide it into\\nsmaller blocks and let the GPU manage\\nthe scheduling let's see one last\\nexample and then we move on to\\nTron imagine now we want to do a matrix\\naddition instead of doing a vector\\naddition now in a matrix addition we\\nhave data that we can see on two axes\\none is the rows and one is the\\ncolumns it's usually uh we represent the\\nvertical axis as the y axis and the\\nhorizontal axis as the\\nxaxis by using the same blocked um uh uh\\nintuition that we used before so\\ndividing the data input data into blocks\\nthis is how we can divide the labor of\\nour maxr addition into blocks for\\nexample we can divide our rows into\\nblocks and call this one the block block\\nzero and this one in the block one and\\nthis one is the block two the same we\\ncan do on the xaxis so we can choose\\nthis one as the block zero this one as\\nthe block one and this one as the block\\ntwo on the x axis with X is the column\\naxis and the Y is the row axis we don't\\neven have to choose the same block size\\nfor the rows and the columns we can even\\nchoose the to group together three\\ncolumns and two rows instead of doing\\ntwo and two in this case we need to find\\nbecause as we said before when we launch\\na Cuda kernel Cuda will just assign IDs\\nto the blocks and the threads in each\\nblock then it's up to us understanding\\nwhat to how to map the ID of the block\\nand its corresponding thread ID into the\\ndata element that this particular thread\\nshould work it should work with so in\\nthe case of metrix addition we could say\\nthat each thread should work with one\\noutput element of the output Matrix C so\\nit will become the um the the sum of the\\na element plus the B element and it\\nshould map it to the C Matrix uh output\\nMatrix so how to do it imagine we have\\nsix rows and we have six columns one\\neasy way would be to divide this rows\\ninto three blocks each made up of two\\nrows and each column into three blocks\\num each block made up of two columns\\nCuda will launch\\nuh uh as many blocks as there are the\\ncombinations of the rows and column\\nblocks so in this case we have uh three\\nblocks for the columns and three blocks\\nfor the um uh rows so it will launch uh\\nnine blocks so this is the block number\\n00 because it's Cuda will identify the\\ndimensions of the block based on the um\\nuh axis in which we have divided it so\\nwe will call this the X Dimension the\\ncolumns and the rows we will call it the\\nY Dimension so it will launch as many\\nblocks as there are combinations of X\\nand y's in this case we have nine so\\nthis will be the block 0 0 this will be\\nthe block 01 this will be the block 02\\nthis one will be the block 1 Z 1 one and\\none two etc etc inside of each block we\\nwill also divide the threads into X\\nthreads and Y threads along the two\\nDimensions so this will be the thread\\nzero and the thread one along the X axis\\nin the X block and this will be the\\nthread zero and the thread one in the Y\\nuh in the in the block zero of the y-\\nAIS and each block will have two threads\\nand they will be identified as thread\\nzero and thread\\none so let's look at how the launch grid\\nWorks in this case um so imagine we have\\na matrix with number uh num rows number\\nof rows and num columns num calls number\\nof columns and we want to to divide each\\nrow the rows into block size number of\\nrows and calls block size number of\\ncolumns um we define basically the\\nnumber of blocks that we need is this\\none so this is just a fency way of\\nwriting the ceiling of the num Rose\\ndivide by The Rose block size and this\\nis just a feny way of writing the\\nceiling of the number of columns divide\\nby the col's block size this tells us\\nhow many blocks we will have on the rows\\nand how many we will have on the columns\\nthe grid you can see here which tells us\\nhow many blocks we have is a tole that\\naccepts three values which tells how\\nmany blocks we want on the X Dimension\\nhow many we want on the Y Dimension and\\nhow many we want on the Z Dimension we\\nare not going to use the Z Dimension\\nbecause we only have a matrix then\\ninside of each block how many threads we\\nwant for the\", mimetype='text/plain', start_char_idx=114617, end_char_idx=118746, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='9b5e1480-a763-40fb-b03d-e64cff39c70f', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='d3dd5e96-bb6b-43d0-b360-de1bf9261a19', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='1990a16979fac9c6c7f2360cdd91ea8d9882c1e9a4b8988b2f288cafaa49b985')}, text=\"X Dimension and for the Y\\nDimension as the X Dimension we have\\nchosen The Columns so we are saying how\\nmany blocks we want the columns and how\\nmany blocks we want for the rows and\\nthen inside of each block how many\\nthreads we want for the column block and\\nhow many threads we want for the row\\nblock this will Define our launch grid\\nand what Cuda will do it will just\\nlaunch this following configuration so\\nit will launch as many blocks as there\\nare combinations of X and y's and inside\\nof each X and Y it will assign a thread\\nID in such a way that the thread zero on\\nthe X axxis is uh so there will be two\\nthreads on the x- axis and two threads\\non the y- axis of each block now let's\\ntry to understand how to map just based\\non the block ID on the x- axis just\\nbased on the block ID on the y axis and\\nthe thread ID on the X and y- axis how\\nto map it to the one element of the\\noutput\\nMatrix uh let's look at the code so\\nfirst we can use the following formula\\nto identify which row this element\\nshould work with uh which which uh the\\nuh which because each element of a\\nmatrix is identified by two in this say\\none is the row identifier and one is the\\ncolumn identifier the row identifier we\\ncan look at it like the block ID\\nmultiplied by the block size plus the\\nthread ID uh let's see why it makes\\nsense so in this case for example this\\nuh thread will work with the row zero\\nbecause the block ID is on the y- axis\\nis zero and the thread i z zero so it's\\nblock ID multiplied by the block size so\\n0 plus 0 it will be zero so this element\\nwill be working with the row number zero\\nand which column it will be working with\\nwell it will be working with the block\\nID zero multiplied by the block size on\\nthe column which is again zero I mean\\nthe block size is two but multiply by Z\\nit will be zero plus the thread zero so\\nit will be zero this element here on the\\nhere it will be the block ID of the Y y\\nAIS multiplied by the block size plus\\nthe thread so it will be the element\\nzero on the row and for the columns it\\nwill be the element one let's see\\nanother one for example here uh for\\nexample this element here so this um how\\nthis uh thread will uh which element it\\nwill work with well it will be the block\\nsize on the Y AIS multiplied by the the\\nblock ID on the y axis multiplied by the\\nblock size so it will be 1 multiplied by\\ntwo so that will be our row so the row\\nnumber\\ntwo uh which makes sense because it's\\nthe um this is the row zero this is the\\nrow one and this is the row two and the\\ncolumn will be the uh block ID on the x\\naxis which in this case it's equal to 1\\nmultiplied by the block size which is\\nequal to two so 2 + 1 is equal to 3 so\\nthis uh thread here will work with the\\nelement number two three and this\\nformula now makes sense so this is how\\nwe use the block ID and the thread ID\\ninside of each block to map it to which\\nelement this particular thread should\\nwork with so as I said before Cuda has\\nno notion of knowing which element this\\nparticular thread should work with this\\nis up to us just based on the block ID\\nand the thread ID that CA\\nassigns then we make sure that the uh\\nrow index is less than the number of row\\nand the column index is less than number\\nof columns why because as I said before\\nwhen we launch um blocks and threads\\nCuda will round up that number to a\\nmultiple of 32 in the case of the\\nthreads so which means that some of\\nthese threads should not work with any\\ndata so we make sure that all the\\nthreads that should not have the\\ncorresponding element to work with they\\nshould be just sit idle inside of this\\nif\\nstatement but the one that have it they\\nshould go enter and do some job so we\\ncalculate the index of the element of\\nthe Matrix that this particular thread\\nshould work with as follows which is the\\nrow index multiplied by the number of\\ncolumns plus the column index uh this is\\njust another way of\\nwriting uh a uh or for example this is\\njust another way of writing a of row\\nindex call index but the way we allocate\\narrays in C or C++ is a flattened array\\nwhere all the rows are one after another\\nso we need to identify the element\\ninside of the array based on its row\\nindex and column index and this\", mimetype='text/plain', start_char_idx=118747, end_char_idx=122887, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='56e16e77-16f4-4b9d-8e34-ca20d8a0fd5c', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9b5e1480-a763-40fb-b03d-e64cff39c70f', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='d7dc88013b8837469201612cd9a2584e85f38ae01b4ebaa2d3e68cec3f6ff020')}, text=\"is the\\nformula that we use to identify it if\\nyou have never worked with um uh arrays\\nin C++ or C then it doesn't matter\\nbecause later we will see tensor layouts\\nand this will be much more clear but if\\nyou have already worked with then you\\nalready know how to uh index an element\\ninside of a multi-dimensional array in\\nC++ and then we compute the output as uh\\nas usual so I know that this has been a\\nlot of information so what should we\\nshould we remember from this the first\\nthing that we should remember is that we\\ndecide how to divide the work on\\nwhatever Matrix we are working with or\\nwhatever thread we are working whatever\\nVector we are working with we tell Cuda\\nhow many blocks we want and we tell Cuda\\nhow many threads we want in each block\\nbased on the identifier of the block ID\\nand the thread ID we should come up with\\na strategy on how to map it to a subunit\\nof work so which part of the Matrix or\\nwhich part of the vector that particular\\nthread should work with um now the next\\nstep for us is to understand the tensor\\nlayouts because we are going to work\\nwith the tensors and we need to\\nunderstand how the tensors are lay out\\nin the memory of the GPU or in the CPU\\nas well actually so we need to\\nunderstand what is the row column row\\nmajor layout and the column major layout\\nwhat is the stride Etc and convert all\\nthe knowledge that we have about Cuda\\ninto Triton so that we can then code\\nwith Triton our kernel so let's\\ngo all right guys finally it's time for\\nus to explore tensor layouts now why do\\nwe need to explore tensor layouts\\nbecause before we we have seen some\\nexamples of Cuda kernels and when you\\ngive a matrix to Cuda or to a Cuda Kel\\nor a vector to Cuda kernel Cuda will not\\ngive you will not give you the entire\\nmetrix like like in Python where you can\\naccess each element by its index Cuda\\nwill just give you a pointer a pointer\\nto the starting element of that\\nparticular Matrix or the starting\\nelement of that particular Vector then\\nit's up to you to calculate the memory\\naddress of all the remaining elements so\\nsuppose that we have a simple Vector in\\npy torch this simple Vector could be the\\nfollowing which is a vector of shape\\nseven because it's a tensor with only\\none dimension with shape seven which is\\nthe number of elements in the First\\nDimension uh for now ignore this\\nproperty called the slide and later I\\nwill explain it what is it uh how this\\ntensor will be saved in the memory of\\nthe CPU or in the GPU it will be saved\\nas follows suppose that the starting\\naddress of the first element is the\\naddress 100 and suppose that each\\nelement is made up of a floating point\\nof 16 bit so it means that each element\\nwill occupy two bytes so the start\\naddress of the second element will be\\nthe address 102 and the third element\\nwill be 104 and the fourth element will\\nbe 106 etc etc etc so uh this is exactly\\nwhat you get when you in see you get um\\nyou allocate a vector or a matrix with\\nMalo so when you allocate in C um vector\\nor a memory with maloc C or the memory\\nallocator will just allocate enough\\nmemory to store all the elements and it\\nwill give you a pointer to the start\\naddress of this memory then it's up to\\nyou to understand where each of this\\nelement is stored in that block of\\nmemory and this is to to do this we\\nintroduce a property called stride The\\nStride tells us how many elements we\\nneed to skip to arrive to the next\\nelement in the particular dimension in\\nthis case for example in the case of a\\nvector we only have one dimension which\\nis the X\\nDimension uh or the columns Dimension\\nyou can think of it so this is the first\\ncolumn this is the second the third the\\nfourth fifth etc etc um so in order to\\narrive from one element to the next we\\njust need to skip one element so to go\\nfrom here we need to just increase our\\npointer by one element and then to go\\nhere we need to increase again pointer\\nby one element Etc this allow us to do a\\nfor loop on this tensor let's look at a\\nmore complicated case like the Matrix so\\nthe Matrix is\\ntwo-dimensional and suppose we have the\\nfollowing Matrix which is made up of six\\nelement with two rows and three Colum\\ncolumns so the shape of this tensor will\\nbe 2x3 because it we have two rows and\\nthree columns uh how this Matrix will be\\nsaved in the memory in the memory\", mimetype='text/plain', start_char_idx=122888, end_char_idx=127145, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='2a535da0-2b35-4f44-8e13-0d8d9a5a93fe', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='56e16e77-16f4-4b9d-8e34-ca20d8a0fd5c', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='5055034f32d534a52964a34e9e52aef93c7d1b40c328825e44eabecb45026df6')}, text=\"it\\nwill be just a\\nflattened um Matrix it means and this is\\ncalled the row major layout but there is\\nalso another one called column major\\nlayout that we will not be discussing so\\nhow it will be stored in the memory is\\nas follows it will be the first elements\\nof the first row so the elements of the\\nthe first row followed immediately by\\nthe elements of the second row so that\\nthe memory address imagine with this is\\nthe memory address of the first element\\nis 62 to go to the next element we need\\nto increase the memory address by the\\nnumber of bytes that each element\\noccupies which is two bytes so the the\\naddress of the second element will be 64\\nthe third element will be 66 and the\\nnext row will start immediately after\\nthe end of the first\\nrow let's introduce this property stri\\nso the stride is what the stride tells\\nus how many elements you need to skip in\\neach Dimension to arrive to the next\\nelement of that dimension for example\\nimagine we want to um uh address um we\\nwant to get the element so all the\\nelements of the first\\nrow um so let's call this tensor here\\nlet's call it t so T of zero and um this\\nbasically this indexing here says give\\nme all the elements of the first row so\\nin the first row select the all only the\\nfirst row and give me all the elements\\nof that row how to how does this\\nindexing work well by starting from the\\npointer to the first element it will\\nselect only the first row and then it\\nwill move the the index here one element\\nafter another so it will select the\\nfirst one the second one the third one\\nhow does it know that it needs to move\\none element by one element because in in\\nthis Dimension the stride is one so the\\nstride tells us how many elements you\\nneed to skip to arrive to the next\\nelement in that Dimension imagine now\\nthat we want to uh get the T of let's\\nsay zero and\\none well in this case the T let's say t\\nof one actually and all the elements of\\nthe first row it will first of all it\\nneeds to skip some elements from the\\nfirst First Dimension it needs to skip\\nthe element zero because we don't we are\\nnot selecting it we only want to select\\nthe element one of the First Dimension\\nwhich basically means the row with index\\none so because it will start from the\\nfirst pointer to the first element it\\nwill it needs to know how many elements\\nto skip and how many element to skip is\\ngiven by The Stride so the stride tells\\nus how many elements you need to skip to\\narrive to the next element of the First\\nDimension so in this case it will take\\nthe pointer to the first element skip\\nthree elements and it will be starting\\nwith the second r go and then inside\\nthis row it will go through the second\\nin the the the index of the second\\ndimension in which the stride is one so\\nit will just go one after another and it\\nwill return only this part of the memory\\nso to rehearse the stride is just\\na a number that tells us how many\\nelements you need to skip in each\\nDimension to arrive to the next index in\\nthat Dimension so it means that to go\\nfrom one row to the other we need to\\nskip three elements to go from one\\ncolumn to the other we need to skip one\\nelement um why is the stride useful well\\nthe stride is useful because it allow us\\nto reshape tensors very easily and\\nwithout doing any computation let's see\\num okay imagine we want to reshape a\\nmatrix imagine initially the shape of\\nthis Matrix is 2x3 so we have two row by\\nthree columns and we have a stride\\ncalculated as follow it means that to go\\nfrom one row to the other you need to\\nskip three elements and to go from one\\ncolumn uh one row to the other you need\\nto skip three elements and to go from\\none uh column to the next you need to uh\\nskip one element so you need to jump by\\none element if we want to reshape it\\ninto this shape so 3x two basically we\\nwant to um have three rows and two\\ncolumns uh the the we can reshape it\\nwithout actually changing its memory\\nlayout just by changing the stride\\nbecause look at this physical\\nconfiguration of the tensor and we can\\naccess this same tensor as this shape or\\nas this shape exactly by using the same\\nphysical view because to go from one row\\nto the next here the stride is three so\\nwe need to skip three elements it means\\nthat the starting address the starting\\nelement of the second row is given by\\nthe start point plus three elements so\\nexactly here the second row will\", mimetype='text/plain', start_char_idx=127146, end_char_idx=131461, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='e6699ec6-370a-4f1b-b7b6-86128a506457', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2a535da0-2b35-4f44-8e13-0d8d9a5a93fe', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='5a97a7330540039256c2d54ae9cc0d99ba004f25e47844c7db3a5ef5ad4e2aac')}, text=\"start\\nand each element of the second row is\\none after another because the stride of\\nthe second dimension is one so you can\\nsee that to get the second row we can\\njust start from here and then go one\\nafter another and get all these elements\\nwhich is exactly the second row suppose\\nwe want to obtain the second row of this\\nview here of this shape of this reshaped\\nMatrix how to do that let's look at the\\nstride the stri now is two in the row it\\nmeans that to go from one row to the\\nnext we need to skip two elements so if\\nwe want to select this row here we go\\nfrom the starting point of the uh memory\\nso this start pointer we skip um the\\nfirst two elements because the stride\\nsays that to go from one row to the next\\nyou need to skip two elements so we\\narrive here and then we select exactly\\ntwo elements which are one after another\\nbecause the stride in the second\\ndimension is one so the um this stride\\nallow us to reshape the tensor without\\nchanging the physical layout on how it\\nis stored in the memory\\nmoreover The Stride also allow us to get\\nthe transpose of a matrix without\\nchanging the shape of how it is stored\\nin the memory so without changing the\\narrangement of the elements in the\\nmemory and this is very cool because we\\ncan view the same Matrix as with without\\nthe transpose and also the transpose\\nversion of The Matrix without changing\\nanything in the memory so it comes for\\nfree just by working with the index and\\nthe stride so to trans The Matrix along\\ntwo Dimensions we just need to swap the\\nstride along this two Dimensions that we\\nwant to transpose so in this case for\\nexample imagine we want to get the\\ntranspose of this Matrix we just need to\\nswap the strides so if we want to get\\nthe second row of the transposed Matrix\\nhow to get that well you we always have\\nthe pointer to the first element where\\nthe tensor is stored so at the beginning\\nof where the tensor is stored in the\\nmemory and it says that in order to go\\nto from one row to the next we need to\\nskip one element which is correct\\nbecause as you can see the second\\nelement is exactly the second element\\nalso in the memory so we just Skip by\\none and we get the starting point of the\\nsecond row and then to go from one\\nelement to the next in within the same\\nrow we need to skip three elements so\\nthe second element of the second row\\nwill be after three elements uh um after\\nthe first element of the second row so\\nafter two we need to skip three elements\\nso we skip this one we skip this one and\\nwe arrive to this one eight which is\\nexactly the second column of the SE uh\\nof the second row so basically the the\\nthe stride as you can see allow us to do\\ntwo things one is it allow us to reshape\\nthe tensor without having to reallocate\\nit in another configuration in the\\nmemory secondly it allow us to transpose\\na matrix without having to rearrange the\\nelements in the memory which is great\\nbecause moving memory around is\\nexpensive\\nuh and rearranging the memory is\\nexpensive so that's it's great that this\\nthis stuff comes for free\\nbasically um another thing okay for\\nexample do um if you try to you know\\nthat in py torch there are two methods\\nto reshape a tensor one is called the\\nreshape method and one is called The\\nView method the after transposing a\\nmatrix by swiping by swiping The Stride\\nof the two Dimensions that you want to\\ntranspose you cannot reshape for free\\nthe tensor anymore because um the tensor\\nbasically what is the stride The Stride\\nhow it is computed The Stride is just\\nthe uh let me show you with a concrete\\nExample The Stride is just the product\\nof all the shape uh after um in the\\nfuture Dimensions so the stride of the\\nzero Dimension is just the product of\\nthe elements in the shape of the future\\nDimension so so the stride of zero is\\njust the product of all the shape\\nstarting from the index number one uh\\nit's not easy to see with the two The\\nMatrix because we don't have enough\\nelements so let's do it with a three 3D\\nMatrix so this is a tensor with the\\nthree dimensions so it is a shape of two\\n4 three which means that we have two\\nmatrices each Matrix is made up of four\\nrows and each M and three colums The\\nStride is calculated as follows so the\\nzero Dimension stride is just the the\\nproduct of 4x3 and this three here comes\\nthe with the product of just a three\\nwith it with one because we don't have\\nany future dimension of the\", mimetype='text/plain', start_char_idx=131462, end_char_idx=135793, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='1cfce9c8-cf92-488a-902d-720516870db7', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e6699ec6-370a-4f1b-b7b6-86128a506457', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='db2d7eff04bf23860d3a8f7268acf4777711fcf9e664e45a1fe72b80b0a9c500')}, text=\"tree so when\\nwe transpose this stri property is lost\\nand we cannot um after transposing this\\nmatrix by swapping the strides we cannot\\ndo further reshaping operations so\\nbasically the the tensor is not log\\ncontigous so this is a very Advanced\\nokay property if you it doesn't matter\\nif you know it or not but if you are\\ncurious basically in a pytorch you\\ncannot um view a tensor after it has\\nbeen uh transposed because pytorch to\\ntranspose a tensor will just swap the\\ntwo strides but it loses the stride\\nproperty which is basically the stride\\nwill not be anymore the product of the\\nfuture shapes so this is not anym two\\nthis should be two for example and this\\nshould be one but after transposing this\\nproperty lost so you need to actually\\nreallocate the tensor if you want to\\nreshape it after it has been transposed\\ndoesn't matter if you remember this it's\\njust a curiosity anyway so what is the\\ntranspose what is the stride used for is\\nthe St The Stride is used for two things\\nfirst of all it it is used to understand\\nhow to Index this tensor so just by\\nhaving a pointer to the first to the\\nstarting address of this tensor we can\\nIndex this tensor however we like so we\\nwe can access any row any column uh\\nmoreover it allow us to reshape this\\ntensor for free so without rearranging\\nthe elements inside the memory and third\\nit allow us to transpose the tensor\\nhowever we like just by swapping the\\nstrides of two uh the two Dimensions\\nthat we want to\\ntranspose now that we have seen also how\\nthe tensor is stored in the memory we\\ncan finally go to see\\nTron um and see some examples all all\\nright guys now that we have seen how uh\\ntensors work Tor leat Works how Cuda\\nworks now we can see some examples of\\nTron kernels to see how Triton differs\\nfrom Cuda now if you go on the Tron uh\\nTron website you will find some\\ntutorials like in this section here and\\nLet's do let's work one tutorial\\ntogether to understand how Tron is\\ndifferent from Cuda um so if you go to\\nthe tutorial there are many examples so\\nfirst of all the code that I will be\\ncoding for Flash attention is based on\\nthis tutorial here fused attention that\\nyou can see here but with some\\nmodifications because I simplified the\\ncode a lot I removed for example the fp8\\nimplementation I also for example um\\nthis code here on the fuse detention\\nonly works in the backward pass only for\\nthe causal attention while my code will\\nwork for the causal and non-causal\\nattention uh the second another\\nmodification I did is instead of using\\nthe exponential two that they use here\\nto make things faster probably because\\nthe exponential two is implemented uh\\nwith a faster unit uh I I I use the the\\noriginal implementation of flash\\nattention which used the exponential\\nwith the base e etc etc so I simplified\\nmy code as much as possible to make it\\nsimple to follow instead of making it\\noptimized so for sure my code will be\\nslower than the the fused attention that\\nyou see here but mine should be more\\ncomprehensible more easy to follow\\nanyway let's go to the vector addition\\ntutorial and if you go to the vector\\naddition tutorial there are some\\nexamples on how to do a vector addition\\nwith Tron uh this should allow you to\\nget into the mindset of how to write\\nkernels with Tron uh instead of writing\\nfirst the the kernel and then calling it\\nlet's do the opposite so let's see how\\nto call this kernel and let's explore\\nhow it works so I have already copied\\nthe tutorial vector addition from the\\nwebsite so let's look at first of all\\nwhat we want to achieve we have an input\\nVector X and an input Vector y and we\\nwant to compute the vector addition\\nwhich means that with the torch we want\\nto do the following operation and also\\nwe want to do the same operation also\\nwith the Triton by calling this method\\nadd and then we want to compare the two\\nvectors output and they should be equal\\nor at least their difference should be\\nvery very small because of course there\\nis always some rounding error in case\\nyou are working with floating Point\\nnumbers the size of this Vector is\\n98,\", mimetype='text/plain', start_char_idx=135794, end_char_idx=139813, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='5cb6ff9d-a0c6-478e-adee-ed1da4df5adf', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1cfce9c8-cf92-488a-902d-720516870db7', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='b484d7d17fdc64247eba656e28ef8d44c83bcda77cde5d4408f9584a126237b5')}, text=\"000 elements and um we want to work\\nin a blocked way so as you remember\\nbefore with the Cuda you can do vector\\naddition by spawning a lot of number of\\nthreads each doing one operation but\\nwhen the number of threads that you have\\nis not enough then you need to divide\\nthe input Vector into blocks and this is\\nwhat we are going to do here so let's\\nlook at this add method so this add\\nmethod basically will first of all\\nallocate the necessary memory for the\\noutput Vector then it will compute the\\nlaunch grid the launch grid tells Tron\\njust like in Cuda how many um kernels we\\nwant to how many blocks we want to\\nlaunch how many blocks of threads we\\nwant to launch uh if you remember in the\\nCuda kernel we specify how many blocks\\nwe want and then how many Treads we want\\nfor each block in the case of uh\\nTriton we tell how many um blocks we\\nwant and then we don't force how many\\nthreads to launch it will be Tron that\\nwill choose how many threads to Launch\\num we just tell what each group of\\nthreads should do so in this case for\\nexample we divide our number of elements\\nso n so which is 98,\", mimetype='text/plain', start_char_idx=139813, end_char_idx=140900, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='547392eb-c6b9-4b4d-be2f-8465998ac904', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='5cb6ff9d-a0c6-478e-adee-ed1da4df5adf', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='1539344f40a8db44efe6d40fcc57302547702f98e272d824b1735712bea25bd6')}, text=\"000 into blocks of\\nsize block size which is initialized as\\n1224 this is basically saying take the\\nto calculate the grid size you do the\\nceiling division so basically this means\\nceiling of oh oops seal of n elements\\ndivided by block size this is the\\nmeaning of this one so how many blocks\\nwe want now what each block should do is\\ninside of the kernel so let's go to the\\nkernel and when we launch the the kernel\\nwe we can specify that the launch Grid\\nin this uh Square parenthesis and then\\nin the uh round parenthesis we specify\\nthe arguments of this kernel so let's go\\nto the\\nkernel we see that python uh Tron will\\nnot give us access to the tensor X it\\nwill give us a pointer to the first\\nelement of this tensor and this takes us\\nback to the tensor layouts so the reason\\nwe studied the tensor layouts and the\\nstrides and all the stuff is because\\nTron this code uh this add kernel will\\nrun on the GPU and the GPU cannot um\\ndoes not index tensors like P torch by\\nusing all the dimension and with the uh\\nbroadcast casting and all this fancy\\nstuff the GPU will just give you the\\npointer to the first element of this\\ntensor in the memory and then it's up to\\nyou to compute all the indexes of all\\nthe elements that you want to access so\\nthis x PTR is the pointer to the first\\nelement of the X Vector this y pointer\\nis the first the pointer to the first\\nelement of the Y uh Vector then we have\\nthe pointer to the output Vector where\\nwe want to store the result of this\\nMatrix addition we specify how many many\\nelements our vectors have and what is\\nthe block size so how many uh items each\\nblock should process which may not\\ncorrespond to how many threads each um\\neach kernel will\\nhave you may be confused because okay uh\\nin Tron in Cuda we specified how many\\nthreads each um block should have so the\\ngranularity that we manage is the thread\\nlevel here we are saying it's a group of\\nthread that should work with this\\nquantity of data then it's up to Tron to\\noptimize the number of threads that it\\nwill actually use actually there are\\ntricks there there are ways to say how\\nmany threads we actually want by\\nspecifying the number of wordss but we\\nwill see that later for now just\\nremember that this thread this Kel here\\nwill process a number of elements in the\\ninput vectors how many number how many\\nelements block size number of elements\\nfirst of all we need to identify which\\nblock we are um we are in Cuda we use\\nthe the variable called block ID dox to\\nidentify the identifier of the block\\nwhich tells us which group of elements\\nwe should be working with in Tryon you\\ndo the same by using program ID and in\\nuh Cuda the block ID can be along the X\\nY and Z axis in Tron these are called\\nthe dimension zero one and two uh two\\nhere we have onedimensional data so we\\nonly use one AIS to specify the block\\nindex so we get the block index which is\\nthe P ID in this the Tron this called\\nthe program ID it's more intuitive to\\nthink of as the program like this is a\\nkind of a program that is running in par\\nwith other programs that will have\\ndifferent program ID and based on the\\nprogram ID we can understand what is the\\nstarting element this program should\\nwork with so this blue block of threads\\nshould work with and to get that is just\\nthe P ID multiplied by the block size so\\nthe p 0 should be working with the\\nelements that starts from the element\\nzero the P id1 should start with the\\nelement 1024 and the P2 should start\\nfrom the element 248 so it should skip\\nthe first 248 elements and starts with\\nthe element with index\\n2048 next we Define how to load these\\nelements based on the\\npointer in which of the X and the Y uh\\nvector\\nto do that we specify a list of offsets\\nwith respect to the starting address\\nthat we want to load so because each\\nprogram in Tron works with a group of uh\\num of data so not one single element but\\na block of elements we we need to\\nunderstand which elements to load so the\\noffset of this elements in the case of\\nthe program ID zero it will load the\\nblock start so zero plus the elements\\nfrom index 0 to 100\\n1,424\\nexcluded with the program element\\num one this basically will result in a\\nvector that is uh well the program start\\nwith PID equal to 1 will be24 then\\n1025\\n1,\", mimetype='text/plain', start_char_idx=140900, end_char_idx=145083, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='7abf63fe-bfcf-4ba9-804c-ae2614db4ed2', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='547392eb-c6b9-4b4d-be2f-8465998ac904', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='1fdab1cc61ee4b15cbfdd4866deac91c3210ab432d2d2e46a1ba35c55ee14a76')}, text=\"26 1027 etc etc until\\n2047 um with the program number let's\\nsay\\ntwo this uh this offsets will be the\\nelements\\n248\\n249 blah blah blah until 3,000 and\\nsomething um now we also as you remember\\nwhen we create\\num when we launch a grid the number of\\nthreads is not always based on the\\nnumber of elements in the block or the\\nnumber of elements in your vector it is\\nalways a multiple of a base number which\\nis usually 32 which means that the grid\\nthis program may have more threads that\\nit needs so some threads should not be\\ndoing anything so should not be loading\\nany data and should not be Computing any\\nsummation so what we this is what why we\\nneed this mask this means that if um all\\nthese offsets that we are loading should\\nbe at most up to n elements because\\nimagine you have not 1 2,000 imagine you\\nhave Vector of\\n260 Elements which means that this\\noffset for um the the third program of\\nthis kernel will load the offset that go\\nfrom\\n2048 2049 blah blah blah 20 60 and then\\nalso 2061 2062 etc etc but we said that\\nwe only have 260 elements so all the\\nelements 261 62 Etc until 300 and\\nsomething they don't exist so we need to\\ntell somehow that all the threads that\\nare working with this elements should\\nnot load anything that's why we need\\nthis mask this mask tells load among all\\nthe offsets that this block should work\\nwith only those elements that actually\\nexist for which this mask is true then\\nwe load the elements of this current\\nprogram which is a group of elements\\ndefined by these offsets\\nand only the one that for which this\\nmask is true so only the one that\\nactually exist all the other should be\\nignored and we can also specify what it\\nshould load in case this um the mask is\\nfalse um with another parameter but we\\nwill not seeing that here we also load\\nthe group of elements of the Y vector\\nand then we compute the output x + y so\\nif you remember previously in Cuda we we\\ndid something like this like the output\\nof I is equal to the x of I plus the Y\\nof I so we did it one element at the\\ntime because each thread was working\\nwith one index here we are working with\\na group of elements so this x is a group\\nof elements is a block of elements at\\nmost of size block\\nsize actually of size block size and\\nit's this Y is a group of elements from\\nthe Y vector and we are Computing the\\noutput\\nGroup by group so this this is summing a\\ngroup of elements of X with the\\ncorresponding group in y and writing it\\nin output then we need to restore this\\noutput we need to store it in the output\\ntensor output PTR that you can see here\\nwhich is a pointer to the first element\\nof the output vector and we say that\\nwhere should we store this output Vector\\nwhich is of size shape of this Vector\\nhere is block size where should we save\\nit well in the same offset to where\\nwhich we loaded X so if this uh program\\nwork with the index 2048 2049 etc etc\\nthen all this output should be written\\nin the same offset uh 2048 2049 Etc up\\nto 3,000 and something using the mask as\\nwell because we don't want to write all\\nthe values of this block size because\\nmaybe we don't have enough elements so\\nonly write the one that are actually\\npresent in the vector so the reason we\\nneed this mask is because Cuda will\\nlaunch a number of thread that is always\\na multiple of a base unit that may not\\nbe a a multiple of the vector size that\\nwe are working with so we need to find a\\nway to tell some threads to not do\\nanything for those that the data is not\\navilable so let's rehearse what you have\\nseen so far in Cuda the program that we\\nwrite is at the thread level so each\\nthread what it should do in Tron it's a\\nthis block of data we work with a block\\nof threads what data this block of\\nthread should work\\nwith all right guys the final finally\\nthe moment has come so uh we are going\\nto quote the flashh attention for our\\npass right now in Tron but let's\\nrehearse the algorithm so the goal of\\nthe attention mechanism in specifically\\nin Triton uh in flesh attention is to\\ncompute the attention output which is we\\nwant to compute the output of the\\nfollowing formula so the query\\nmultiplied by the transpose of the key\\ndivide by the square root of the head\\nDimension all\", mimetype='text/plain', start_char_idx=145083, end_char_idx=149220, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='82fd6775-9d7b-492a-b5f6-7be75d8cbb7e', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='7abf63fe-bfcf-4ba9-804c-ae2614db4ed2', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='bd45cdc84c0609c53b3b10bf50d16b48877bcadab7257d477676bf528eba4a55')}, text=\"multiply we apply the soft\\nMax and then all um multiplied by\\nV now um we in this video we will be\\ncoding the forward pass and also the\\nbackward pass but before coding the\\nbackward pass we need to understand how\\nthe autograd works we need to understand\\nwhat is the gradient what is the\\nJacobian how to derive the gradient of\\nthe softmax operation how to derive the\\ngradient of the matrix multiplication\\noperation etc etc so that is going to be\\nanother part of the video for now let's\\nconcentrate on the forward pass right\\nnow we have some tools so we know that\\nwe have this thing called the GPU that\\ncan parallelize operation among multiple\\ncores we know that in Cuda we can\\nparallelize operations by telling by\\nwriting a program that is the definition\\nof what each thread should do or we can\\nfollow the Tron programming mode which\\nis telling in Python what each group of\\nthreads should do\\nthe mapping between the what each thread\\nshould do and the which element that\\nshould thread work with is up to us to\\nthe programmers and the same happens in\\nTron we tell we how many blocks of\\nthreads we want how much data each\\nthread should block of thread should\\nprocess so that's the block size that we\\nsaw in the vector addition but then the\\nmapping between the elements of the\\nvector and the um the the identity of\\neach group of threads so the program ID\\nthat we saw is up to us and the same\\nwill happen when we recode flashh\\nattention let's see what can we par\\nparallelize in this flashh attention so\\nfirst of all this code that you see the\\nforward pass of the flashh attention is\\ntakes as input query key and value that\\nis a vector that is a matrices of n by D\\nhowever usually in a Transformer Network\\nwe don't have only one sequence made up\\nof D Dimensions we have many sequences\\nmade up of D dimensions and this D is\\nthe lower case D which is the the number\\nof Dimensions dedicated for each head\\nbut we don't have only one head we have\\nmultiple head so the algorithm that you\\nsee here is what each head should work\\nso each uh head of each batch should do\\nmoreover we have have seen before when\\ntalking about block matrix\\nmultiplication that we can parallelize\\nthe computation of the output because\\nthis output block here depends on the\\nquery one and all the keys this one here\\ndepends on the query group block of\\nquery two with all the keys and this one\\nhere is the query three with all the\\nkeys Etc so because this one only\\ndepends on query the group The Block\\nquery one and this one only depends on\\nthe Block query to they can work\\nindependently from each other by sharing\\nof course work the the\\nkeys another thing that we need to\\nunderstand about a Tron is the shared\\nmemory so um the in the GPU we have the\\nhigh bandwidth memory and which is the\\nlike kind of the ram so the when you buy\\nan a100 they tell you that it has 4 40\\nGB that's the amount of memory in the\\nhigh bandwidth memory so the D Ram so\\nlet's look at actually the structure of\\nthe\\nGPU uh which is here we have this dram\\nwhich is the big memory uh that we that\\nthe GPU has and then each um streaming\\nmultiprocessor so it's a let's call it\\nBL block of threads uh actually also\\nhave a shared memory so inside of the\\nGPU actually we have we have these\\nstreaming multiprocessors and these\\nstreaming multiprocessors have a part of\\nmemory called the shared memory which is\\nmuch smaller than the dram like much\\nmuch much smaller what changes between\\nthese two mem\\nthe access to the dram is very slow and\\nthe access to the shared memory is very\\nvery very fast so one thing that is\\ndifferent between Cuda and Tron is that\\nwhenever you load some information in\\nCuda you are loading that information\\ndirectly from the global memory because\\nwhen we launch a Cuda kernel first of\\nall as you remember in my C C++ code we\\nfirst copy the tensors from or the\\nvectors from the CPU to the GPU and they\\nreside in the global memory of the GPU\\nthen we load these elements directly\\nfrom the global memory but the access to\\nthe global memory usually it's much much\\nmuch slower so what happens with the\\nflesh attention is that the flesh\\nattention computation in its the\\nattention computation in its naive\\nversion the one that we can do with the\\ntorch is very slow because access to the\\nglobal memory is very slow so we want to\\nuse as much as possible the shared\\nmemory so we want to reuse the elements\\nloaded from the\", mimetype='text/plain', start_char_idx=149221, end_char_idx=153578, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='d27d4f55-f57b-4954-b758-c5376d4e6854', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='82fd6775-9d7b-492a-b5f6-7be75d8cbb7e', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='0bc734563a50fec1b1b56a735ed9d1cf11ccfa183b36eca554d9cda29ad58c72')}, text=\"global memory into the\\nshared memory so that we don't need to\\naccess the global memory every time to\\nload elements from the vectors or the\\nmatrices and um this is what happens\\nalso in Tron so in Tryon whenever you\\nload some data you are copying the\\ninformation from the global memory to\\nthe shared memory then whatever\\noperations that you're doing is done on\\nthe shared memory and then when you\\nstore the information you are copying\\nthe data from the shared memory to the\\nglobal memory um this makes it much\\nfaster so we always work with elements\\nthat have been Lo in the shared memory\\nand this shared memory basically it's\\nshared for all the threads that belong\\nto the same uh block uh in Tron we have\\nan obstruction level that doesn't make\\nus work directly with the threads so we\\nalways work with a group of threads that\\nbelong to the same block that share this\\nshared memory so in Tron we are copying\\ninformation from the global memory to\\nthe shared memory we do some operation\\nwith it and then we store back to the\\nglobal memory and this is what we are\\ngoing to do with flash attention now\\nlet's review the algorithm of Flesh\\nattention so in flesh attention we have\\nto go an out for Loop that is among all\\nthe between all the keys and then an\\ninner loop that is sorry between all the\\nquery blocks and then an inner loop that\\nis um through all the key\\nBlock in the original flash attention\\nalgorithm the flashh attention one the\\nouter block was on the keys and the\\ninner block was on the queries this made\\nit less parallelizable why because the\\nOuter Loop is on the queries and we have\\nseen before that the the output of this\\num attention can be computed\\nindependently for each block of queries\\nso it's much easier to parallelize so\\nthis outer for Loop actually we don't\\nhave to run a for Loop we just spawn\\nmany kernels each working with one\\niteration of this outer for Loop so each\\nworking with a different query block of\\nthis outer for Loop and the inner for\\nLoop is something that we have to\\niterate through so each Trion kernel\\nwill work with one query block and then\\niterate through all the key blocks um\\nand inside of this key block we have\\nalready seen the operations that we are\\ngoing to do which the we we we explored\\nbefore and at the end of this for Loop\\nwe need to store back the output in the\\nhigh bandwidth\\nmemory um and this is how it's going to\\nwe are going to work another thing that\\nwe should notice is that this s value\\nare n byd so as I said before but\\nusually in a um in\\na Transformer model we don't have only\\none sequence we have many sequences so\\nwe can also parallelize on the number of\\nsequences that we have in the batch\\nbecause each batch can work\\nindependently from each other and inside\\neach um and each head each sequence has\\nmultiple heads so each head also can\\nwork independently from each other\\nbecause that we know from the attention\\nis all un need paper that's what's the\\nmeaning of head that's what's the\\nmeaning of multi head attention so that\\neach head can compute the attention\\nindependently from each other so we will\\nalso parallelize along the head\\nDimension and moreover if you look at\\nthis definition of the query block we\\ncan also split the query into blocks and\\neach query block can work independently\\nfrom the other query blocks by in\\nproducing one output block this is how\\nwe are going to parallelize so we are\\ngoing to parallelize each sequence in\\nthe patch in inside of each sequence we\\nare going to paraliz each head and\\ninside of each head we are going to\\nparalyze each query block so how many\\nprograms we we will have working in\\nparallel at most it will be uh the sequ\\nthe number of batches so the bch the\\nnumber of sequences in the batch so the\\nbatch\\nsize it will be the batch\\nsize multiplied by the number of\\nheads uh multiplied by the number of\\nblocks that we will divide the query\\nsequence into so let's call it the I\\ndon't know block size\\nQ the block\\nsize\\nQ all right now that we have seen this\\none let's go actually code it\\nso I have already introduced a little\\nbit of the differences between my\\nimplementation of The Flash attention\\nand the one that you can find on the\\nTron documentation which is a first of\\nall I don't work with fp8 because I\\nbelieve this is unnecessary for our\\nexplanation it's of course much faster\\nbecause the recent gpus also support\\nfb8 Second difference is that in the um\\nIn the Flesh attention on the\", mimetype='text/plain', start_char_idx=153579, end_char_idx=157968, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='a30a5bc8-5b24-44d3-9cad-3b28b5207a53', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='d27d4f55-f57b-4954-b758-c5376d4e6854', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='96856acb4d6ce34b883f19811b511a81e5b0b65a20b6a09231f7521589f4023d')}, text=\"Triton\\nwebsite the backward pass is only\\nimplemented for the causal attention but\\nin my case I implement it for the causal\\nand the non-causal attention even if\\nit's slower and later I actually I want\\nto give you an exercise on how to\\nimprove it um and the third difference\\nuh main difference is that I made make\\nexplicit use of the soft Max scale so I\\nactually use the scale when needed\\nanother difference is that in the online\\nuh Tron computation of The Flash\\nattention is this X is not really e to\\nthe power of X but it's two to the power\\nof X and then they compensate it with by\\nby using the\\nlogarithm however uh because probably\\nthe implementation of two to the power\\nof X is faster than the e to the power\\nof X but in my case I retain the\\noriginal exponential because I want to\\nfollow the original algorithm to make it\\nsimpler to visualize the code along with\\nthe algorithm as in the flash ration\\npaper\\nso uh I know I have created a lot of\\nhype so let's do it uh let's start by\\ncreating a new um file let's call it\\nprogram. piy uh just like before when I\\nintroduced Tron I will start by coding\\nfirst the code that will use our Kel and\\nthen we code the Kel and we will only be\\ncoding the forward P of the Kel\\nso let's start\\nby importing what we need to import\\nwhich is just the torch and the Triton\\nand secondly let's start by let me check\\nokay the co-pilot is already off so I\\ndon't have to worry about that let's\\nstart to implement the code that will\\ntest our implementation of the Triton\\nand compare it with the naive\\nimplementation of the attention\\nmechanism so we in uh we create our\\nquery key and SE uh query key and value\\nsequence for test thing which is if you\\nremember it's query is the BET size it\\nhas the dimension bet size because we\\nhave a multiple\\nsequences each sequence has a number of\\nheads and it's made up of s tokens and\\neach token is identified by a head dim\\nnumber of\\nDimensions uh if you in then this is\\nbecause we have already split each token\\ninto smaller tokens each each with its\\nown head Dimension if you remove the N\\nheads Dimension then you put back you\\nconcatenate all the dimensions of this\\nhead\\ndim um we initialize the query key and\\nvalue sequence by using a normal\\ndistribution this code I already took\\nfrom the tutorial of Tron so it's\\nnothing different and we require the\\ngrad the gradient because we want to\\ncompute the gradient with respect to\\nquery key and value and um we will see\\nlater why because because we want to\\nimplement the back we want to test also\\nthe backb pass even though we will not\\nbe coding it now so the first thing that\\nwe do is we Define our soft Max\\nscale which is as you remember the\\nformula is U query multiplied by the\\ntranspose of the keys and then divided\\nby the square root of head\\nDimension so DK or the DHE head sometime\\nit's called and then we need to so we\\nneed to compute this one we can already\\ncompute it it's this this is one over\\nthe square root of the head dimension\\num and then we also Define d o and later\\nwe will see what is this but this is\\nbasically we will be needed needed for\\nthe backward\\npass um don't worry if you don't\\nunderstand what is doo later we will see\\nit let's do the naive implementation of\\nthe attention which is very simple which\\nis first we Define the mask and we use\\nthis mask only if the attention we are\\nComputing is Cal so as you can see we\\npass this parameter called causal that\\ntells if we want to compute the causal\\nattention or the not causal attention\\nand the D type which is float 16 because\\nwe want to work directly with 16 bit\\nfloating Point numbers we will not be\\nworking with fp8 just because we we\\ndon't we don't want to implement uh my\\nimplementation is actually not as fast\\nas the one in the tutorial of the Tron\\nwebsite but I believe it's much more\\neasier to\\ncomprehend uh so we Define the mask\\nwe compute the the the product query\\nmultiply by the transpose of the key\\ndivide by the square root of the head\\nDimension so that's why we are\\nmultiplying by Soft mask scale if the\\nattention we're Computing is Kaa then we\\nuse this mask that we have computed so\\nwe replace all the points all the dot\\nproducts where this mask is equal to\\nzero with minus Infinities and then the\\nsoft mask will\", mimetype='text/plain', start_char_idx=157969, end_char_idx=162176, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='bd829e19-8439-4d2f-807e-e22fae1fe2e4', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a30a5bc8-5b24-44d3-9cad-3b28b5207a53', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='da8fb8e9eeff2790bc667c1f4aaa0a7cdc8e19b130f4e04cc97d63ec13ca97f4')}, text=\"replace this minus\\nInfinities with zeros because then we\\nare applying the soft Max and the soft\\nMax is applied by rows just like the\\nnormal attention we\\ncompute okay the second thing that we do\\nis we want to um so the output is the\\nproduct of the output of the softmax\\nwith the V so this is the reference\\noutput on the naive implementation of um\\nflash of the attention mechanism then we\\nwant to compute we want to also derive\\nthe gradients of the output with respect\\nto the um\\ninputs and in this case it's the\\nthe the V the K and the Q later we will\\nsee what are we doing here then we want\\nalso to we want to compare this\\nreference implementation with our Tron\\nimplementation so let's do it so our\\nTron implementation will be implemented\\nas a class called Tron attention that we\\nwill call using this method called apply\\nand later we will see what is this\\nmethod in which we pass the query key\\nand value if we want to compute the Cal\\nattention the soft Mark scale that it\\nshould be using and it should prod\\nproduce some output which is the output\\nof the output of the softw multiplied by\\nV then we can run also the backward and\\nthis backward will be the the the the\\nsame backward that we will compute with\\nthe um Tron\\nattention and then we\\ncompare okay and then we can compare uh\\nthe result of our implementation so this\\nTron attention not apply with the\\nreference implementation which is this\\none here and they should be uh we use\\nthe the function all clause which\\nbasically compares the elements of two\\ntensors and make sure that their\\nabsolute difference is no more than this\\none we are not using the relative\\ndistance we are just using the absolute\\ndistance between the two elements\\ncorresponding elements of two vectors\\nthis uh implementation that we have that\\nwe will build will work with the causal\\nattention and also with not causal\\nattention while the the one that we saw\\nin the website of Tron it only works\\nwith the uh the forward pass actually\\nworks with the causal and non causal\\nwhile the backward pass only works in\\nthe case of the Cal attention um okay\\nbut it's highly optimized the one online\\nso if you want to learn a little more\\ntricks on how to optimize Triton kernels\\nthere is a lot of knowledge there anyway\\nguys now let's try to uh implement this\\nTron atten at least the forward pass so\\nlet's go to implement this Triton\\nattention\\nclass um okay here every time you want\\nto introduce a new operation into torch\\nyou need to uh derive the um you need to\\nimplement your operation by deriving\\nfrom this autograd do function class so\\nevery operation in torch actually if\\nit's the soft Max or it's the I don't\\nknow the the ru or the zgo or whatever\\nthere is it is always implemented as a\\nas a function is a class that derives\\nfrom this function and it should provide\\ntwo method one called the forward pass\\nand one called the backward pass the\\nforward should produce the output of\\nthis operation and the backward should\\ncompute the\\ngradient um the gradient with of the\\nloss with respect to that the the input\\nof that function and later we will see\\nhow that works for now let's concentrate\\non the forward pass to implement the\\nforward pass we need to create a static\\nmethod that is called\\nforward which takes as input one thing\\ncalled the context so as you know in the\\nautograd in when training AAL networks\\nwe have the forward pass and the\\nbackward when Computing the backward\\npass we need to reuse the activations of\\neach of the computation nodes during the\\nforward pass and this context basically\\nallow us to save the information to uh\\nfor the necessary activations that we\\nwill need during the backward pass and\\nlater we will see in the tron um in the\\nflash attention algorithm what\\ninformation we need to save in order to\\ncompute the backward pass for example\\nwhat we will need to save during the\\nbackward pass we will need to recompute\\non the Fly the soft the query multiply\\nby the transpose of the keys for each\\nblock but we don't want to recompute the\\nnormalization factor or the maximum\\nvalue for each row so we will save those\\ntwo values and actually we will not save\\ntwo values we will save one value with a\\ntrick called The Log sum X log sum X\\ntrick that we will see later anyway this\\ncontext is just a kind of a storage area\\nwhere we can save some stuff that will\\nbe necessary for us to recompute the\\nbackward and you can see whatever you\\nlike then we have the input of this um\\noperation which is the query key and\\nvalue which is three tensors with\", mimetype='text/plain', start_char_idx=162177, end_char_idx=166651, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='394e1164-fca2-4408-8d7e-d264cc265a2a', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='bd829e19-8439-4d2f-807e-e22fae1fe2e4', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='8c9dab42578eee4502a3a8ec90f6191af284247e6347a4a48c540b43605d7cec')}, text=\"the\\nCal if we are going to compute the Cal\\nattention and the soft Mark scale that\\nwe should apply based on the one over\\nthe square root of the uh head\\nDimension uh which we could also compute\\nit on the fly actually by the way by by\\nchecking the shape of this but okay it\\ndoesn't matter anyway so um the first\\nthing that we are going to do is to\\nextract the shapes of this objects and\\nmake sure all the shapes are what we\\nexpect them to be so the shape of the\\nquery key and value is a bch size by\\nnumber of heads by sequence length by\\nhead Dimension we make sure that the\\nhead Dimension matches for the query key\\nand value uh they should match because\\neach Vector should should be of the same\\nsize uh and then we declare what we\\npre-allocate the output Vector so where\\nwe should save our output so as you\\nremember the output in and the attention\\nmechanism has the same same shape as the\\nquery key and value sequence where theer\\nQuan value sequence I want to remind you\\nis not the query value of the input of\\nthe attention which is a sequence of\\ntokens but it's the output already of\\nthe WQ WK and WV because flesh attention\\nis not concerned with optimizing those\\nmetrix multiplication but only the\\noutput of the WQ WK and\\nWB so we pre-allocate the output tensor\\nwhere we will store this output which\\nhas the same shape as the query key and\\nsequence uh um Matrix\\nactually actually no not true actually\\nit has the same shape as the query but\\nit may not be the same as the key and\\nvalue why because there is this thing\\ncalled cross attention where the query\\nkey and value are transposition are\\ndifferent projection through WQ WK WV\\nnot of the same input sequence but of\\ntwo sequence so cross attention happens\\nwhen we have a query that comes from one\\nuh sequence and the key and value come\\nfrom another sequence and they pass\\nthrough their own WK and WV and they may\\nnot have the same sequence length so the\\nshapes of the output of the tension only\\ndepends on the shape of the query\\nsequence not of the key and value\\nsequence this is happens during cross\\nattention but usually in language models\\nwe always work with the self attention\\nso that should not happen um at least in\\nthe Cal language models um then we have\\num the stage and later we will see what\\nis this stage uh basically the stage\\nit's just a number that that tells if\\nthe um operation that we are going to do\\nlater is for the causal attention or for\\nthe not causal attention and then we\\nneed to La Define our launch grid the\\nlaunch grid tells us how many parallel\\nprocess we need to be launched by Tron\\nactually they will be launched by Cuda\\nbut by we always work with the Triton as\\nan interface to Cuda so by\\nTriton so in Tron as I said before we\\nwant to parallelize along the batch\\nDimension so each batch each sequence in\\nthe batch should work independently from\\neach other not only each inside of each\\nsequence in the batch each head should\\nwork independently from each other so at\\nleast we have a bch size multiplied by\\nnumber of heads\\nprograms and for each of these program\\nwe have another uh Dimension called the\\nwe divide the query into blocks of\\nqueries so um as you remember when\\ntalking about a block matrix\\nmultiplication we don't work with the\\nquery as the original Matrix query\\nMatrix so where each query is one vector\\nor one token we work with group of\\nqueries so each block of queries is a\\ngroup of tokens in the query\\nsequence so we are saying that we want\\nto launch at a number of um um kernels\\nor blocks of threads or a group of\\nthreads along two Dimensions just like\\nthe Cuda kernel can be launched along\\nOng two Dimension X and Y here we are\\nlaunching programs along two Dimensions\\none Dimensions that tells us which batch\\nwhich head of which batch we are going\\nto work with so which head of which uh\\nbatch element are we going to work with\\nand inside this we are going to say okay\\nthis is a sequence which group of\\nqueries are we going to work\\nwith are we going\\nto going to work with so overall and the\\ngroup of queries is what is the sequence\\nlength divided by the number of queries\\nthat we want to group together so the\\nblock size Cube tells us how many\\nqueries are there in each block of\\nqueries so this CD is just the ceiling\\ndivision so it is equal to let me write\\nit here this is equal to ceiling of\\nsequence length divide by the block size\\nQ\\nthis tells us uh how many blocks of Q we\\nhave so let's\", mimetype='text/plain', start_char_idx=166652, end_char_idx=171036, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='8c2b0c74-7a6a-4f29-b4ed-07be4f5fa264', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='394e1164-fca2-4408-8d7e-d264cc265a2a', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='f1be9286cbe21cda17ebc7ef1f5824ac653a6b4b62d7ba0bfa3618b183663cb7')}, text=\"rehearse we have a tensor\\nthat is q that is B size by number of\\nheads and each flashh attention\\nalgorithm will work with the following\\nthe sequence length head Dimension\\nmoreover we have seen that the flesh\\nattention has two Loops one is the outer\\nloop among all the query blocks one is\\nis the inner loop along all the key\\nblock we have seen that the query block\\ncan work independently from each other\\nso we can spawn as many programs in\\nparallel as there are number of blocks\\nof Q because they can work in parallel\\nso this grid tells us how many programs\\nthere are that can work in parallel then\\nit will be the GPU that based on its\\nresources will decide how many program\\nactually to work in parallel if it has\\nenough resources to make them all work\\nin parallel wonderful if it doesn't have\\nenough resources to make them work in\\npar it will launch them sequentially one\\nafter another and the last Dimension is\\nthis is like the Z dimension in the\\nCuda in the Cuda launch grid and we\\ndon't want to use it because we don't\\nwant an additional um level of\\nparallelism all right this is our launch\\ngrid so we will launch a number of um\\nprograms that is this one number of\\nprograms of parallel programs or number\\nof parallel Kels and each kernel in Tron\\nwork is a group of\\nthreads which is a batch size multiplied\\nby number of\\nheads multiplied by number of blocks of\\nQ so how many blocks we have in we\\ndivided the Q sequence\\ninto okay let's continue so the we will\\nsee what is this one so this m is\\nanother Matrix that we will need and\\nit's the log sum Expo for the backward\\npass and we will see at the end of this\\nvideo what it not at the end of this\\nvideo but at the end of the forward pass\\nwhat it's needed for but basically this\\nis you can think of it as the maximum\\nfor each row um you we to to recompute\\nthe query multiply by the key in the\\nbackward pass we should also have if we\\ndon't want to recompute the maximum for\\nhro and the normalization factor of the\\nsoftmax we should save two things one is\\nthe maximum for each row and one is the\\nuh the normalization factor however by\\nusing the log sum X trick we can only\\nsave one value which is the as you can\\nsee in the um algorithm of FL attention\\nit's this stuff here which is uh let's\\nsee here it's this stuff here so this Li\\nwhich is the maximum for each row plus\\nthe logarithm of the um of the\\nnormalization\\nfactor um and\\nbasically in when Computing the back\\npass we need to recompute on the Fly\\nthis block here so the square multip by\\nthe transpose of but to apply the soft\\nMax as you remember we need to have the\\nmaximum for each row and the\\nnormalization factor so uh we don't um\\nwe don't recompute them during the\\nbackward because we have already\\ncomputed them during the forward so we\\nsave this information but we don't need\\nto save this two information separately\\nwe can aggregate it into one single\\nvalue called Li and later we will see\\nhow we can use\\nit all\\nright so we have defined find also this\\none and we can proceed further so now we\\nlaunch our grid our\\nkernel don't be scared it's going to be\\na little long so here so we are\\nlaunching the the kernel for the forward\\npass by defining what is the launch grid\\nso how many of this program should run\\nin parallel at most we are passing the\\nquery we are passing the key we are\\npassing the values we are passing the\\nsoft Mark scale the M which is the\\ninformation that we need to save for the\\nbackward pass it's actually the L in the\\ncode of the pseo code of The Flash\\nattention algorithm here I call it m I\\nthink because also in the original code\\nit was called\\nM um the O where the our kernel should\\nsave its\\noutput and then as you remember uh we\\ndon't get all the nice access by um\\nindexing tensor like we are used to in\\ntorch we only get a pointer to the\\nstarting element of q a pointer to the\\nstarting element of K and to the\\nstarting element of v and then we have\\nto figure out all the index in the\\nmemory of the other elements how to\\ncalculate the index we need the stride\\nbecause the stride tells us how many\\nelements to skip to go from one\\ndimension to the other and that's why we\\nare passing the stride for each\\ndimension of each tensor actually in our\\ncase uh we are only working with q k and\\nV that are actually of the same D\", mimetype='text/plain', start_char_idx=171037, end_char_idx=175283, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='58412425-2cd2-4fbd-874c-40ae354cab80', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8c2b0c74-7a6a-4f29-b4ed-07be4f5fa264', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='e976c374bd77228413c0590a7faed14030965faf7061f49aa1c22b12bfd4c155')}, text=\"type\\nand of the same shape so we should not\\nneed actually to pass all all the stride\\nfor each of these uh\\ntensors um because they should have the\\nsame strides however in the original\\ncode I believe they they were passing it\\nso I kept it so the stride allow will\\nallow us to index these pointers to\\nunderstand um to access the elements of\\nof this tensor just by using its\\nstarting uh the pointer to the starting\\nelement and then the strides we will be\\nable to index any element we want in the\\ntensor then we pass the information of\\nthese shapes so the bch size the number\\nof heads the sequence length and the\\nhead\\nDimension and U which is the same for\\nall of them and then the stage the stage\\nindicates if we are going to compute\\ncausal attention or not causal attention\\nso let's not implement it and let's\\ncontinue writing this method so the then\\nthen we need to save some information\\nthat we will be needed for the backward\\npass which is this context variable that\\nI told you before so we save some\\ninformation for the backward pass which\\nis the query key and value uh which are\\nthe tensor for which we want to compute\\nthe gradient during the backward pass um\\nand then um um we need to store also\\nthis m tensor and this o\\ntensor um then we can we need to also\\nstore the causal uh variable so because\\nif if we computed the caal attention\\nduring the for forward pass then during\\nthe backward pass we need to um have\\nthis information because we need to mask\\nout the things that we don't want to\\ncontribute to the gradient but we will\\nsee that later when Computing the\\nbackward pass for now let's concentrate\\non this attention\\nforward so we need to implement this\\nforward kernel that you can see so uh\\nunderscore attention underscore forward\\nmethod now a Tron kernel is just a\\npython method with a particular\\ndecorator called Tron go.get so we copy\\nand paste the signature so this is what\\nmakes a method become a Tron kernel and\\nas you can see here we pass the query\\nkey and value Matrix along with other\\ninformation the M Matrix please don't\\nconfuse the M Matrix with the mask that\\nwe will apply um uh on the Fly we will\\ngenerate it on the Fly because we are\\nonly concerned in this case with a cal\\nattention or not causal attention we do\\nnot accept custom masks\\nhere um then we pass the strides of all\\nthese tensors the B size the number\\nnumber of heads the sequence length the\\nhead Dimension which is the shape of\\neach of these uh tensors um and the\\nblock size Q and The Block size KV the\\nblock size Q indicates how many queries\\nwe want to group together to make one\\nblock of the Q Matrix and how the KV\\nindicates how many keys and values we\\nwant to put together to make one block\\nof the K and V Matrix which is what we\\ndo when we do block matrix\\nmultiplication this stage is a number\\nthat indicates if it's a caal or um not\\ncausal attention we are doing so it will\\nbe three in case it's a caal and one in\\ncase it's not\\ncausal okay the first thing that we do\\nis to verify some information so we\\nverify that the um the block size of the\\nKV is less than or equal to the Head\\nDimension to be honest I don't think we\\nneed it with my code because I removed\\nmost of the constraints so this uh this\\ncheck was also present in the original\\ncode so I kept it but it all depends on\\nhow we are later we will see what is the\\nautot tuning process and later we will\\nsee uh what variables we are going to\\nautotune for and how many stages we will\\nchoose how many warps we will choose etc\\netc so let's leave it for later you can\\ncomment it or you can keep it it\\nshouldn't\\nmatter um the first thing that we do as\\nI said before we launch a grid so a grid\\nis a series of programs where we will\\nhave some identifiers like in the coda\\nwe had an identifier for the blocks on\\nthe x axis and on the y axis in Tron we\\nget this identifier for the programs we\\nlaunched um\\num uh sequence length divide by block\\nsize Q number of programs along the zero\\naxis and the B size multiplied by number\\nof heads on along the first axis of the\\nuh green grid of the launch\\ngrid which will help us identify which\\num part of the query we are going to\\nwork with in this program in this uh\\nkernel and also in which batch and on\\nwhich head this program should work with\\nso that's what we are going to do now\", mimetype='text/plain', start_char_idx=175284, end_char_idx=179543, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='0a932420-1ce1-4376-964b-39f429942815', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='58412425-2cd2-4fbd-874c-40ae354cab80', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='afdfc3e3e76c52d103565e49feabb8a4a9af314fbb5fe766b95f0fd52c90ce1d')}, text=\"we\\nare trying to understand what part of\\nthe input we should work with based on\\nthe IDS of the program which corresponds\\nto the block ID in\\nCuda uh so let me copy so the program ID\\nzero indicates it's this stuff here\\ntells us which part of the queries so\\nwhich block of the queries we are going\\nto work with why do we have a block on\\nthe query because as we saw before the\\noutput can be computed independently for\\neach block of the queries while each\\nblock of the query has to iterate\\nthrough all the key and\\nvalues um so this is what will tell us\\nwhat is the index of the block of the\\nqueries we are going to work with in\\nthis particular\\nprogram then we can understand also\\nwhich index which batch and which head\\nthis program is associated with the\\nprogram ID number one is the product of\\nthe uh batch size and number of heads it\\nmeans that we will have as many programs\\non the axis number one as there are uh\\nindicated by this product so this\\nproduct lets us understand that this uh\\nproduct will tell us which batch and\\nwhich head this particular program is\\nassociated with so uh to get the uh ID\\nof the batch we just divide this number\\nby the number of heads and it will give\\nus the head index and to get the head\\nindex inside this batch we just do the\\num this number here uh modulus the\\nnumber of heads\\nokay uh the next thing that we need to\\ndo uh we need to okay first of all when\\nwe pass a tensor because as you can see\\nhere the Q parameter to this attention\\nforward method is a tensor because it's\\nthe input of this function forward\\nfunction and this forward function is\\ncalled here when we do attention do\\napply and it's this Q stuff here and\\nthis Q stuff here has been created as a\\ntensor so when we pass a tensor to a\\nTriton kernel it's not really a tensor\\nit is a pointer to the first element of\\nthat tensor in the memory now we need to\\nunderstand because now we know which\\nbatch we are going to work with and\\nwhich head we are going to work with we\\nneed to Index this tensor to select the\\nright batch and the right head inside of\\nthe right batch which means that\\nbasically we have this Q uh tensor so we\\nneed to do some some sort of like some\\nstuff like this like Q of the index\\nbatch and uh number of the number of\\nheads in the case the the which head we\\nare going to work with so it should be\\nindex of head and we need to select\\nevery everything that is inside this\\nindices so we are we need to enter the\\ntensor at the right location where the\\nparticular sequence length and head\\ndimension for this batch and for this\\nhead starts for that we need to generate\\nan offset in which we need to move this\\ntensor from because this T this pointer\\nsorry this not tensor this pointer from\\nbecause this pointer is pointing at the\\nbeginning of the entire tensor so we\\nneed to move in the bch size dimension\\nand in the number of heads dimension\\nto do that we generate the following\\noffset which will tell us where this uh\\nwhere this particular batch and where\\nthis particular head starts in this\\ntensor and to do that we need to do the\\nstrides we need to use the strides so\\nwhat we are going to do is we are\\ncreate going to create the qkv offset uh\\nthis is should be the sequence\\nlength um which will be the index batch\\nmultiplied by The\\nStride for the batch Dimension which\\nwill tell do how many elements we need\\nto skip to get to the next batch and\\nit's based and we multiply it by the\\nindex of the batch that we want so for\\nthe zero batch we don't skip anything\\nbecause we already pointing to the first\\nelement of that batch but if we are at\\nthe batch one we will skip that many\\nnumber of elements plus we also need to\\nskip the some heads how many head we\\nneed to skip based on which head we are\\ngoing to work with and what tells us how\\nhow to go from one head to the next The\\nStride of the head dimension so we\\nmultiply the index head so the head that\\nwe should be working with with the\\nstride Q\\nhead all right then um we select now\\nTron helps us with a new function that I\\nthink it was quite recent that helps us\\nindex element inside of a tensor without\\nhaving to deal with all the complex um\\nindexing maths that can be confusing for\\nbeginners so I will be using few methods\\nto help us with this um uh with this\\nindexing and this function is called\\nmake block pointer and it's this\\nfollowing so basically this make\", mimetype='text/plain', start_char_idx=179544, end_char_idx=183864, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='a1375460-1f0a-4320-93c2-118c333944d8', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='0a932420-1ce1-4376-964b-39f429942815', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='0150681459aa0e467d3fdbc71eeb7549e735e660a56efd0d4b46185bc3358734')}, text=\"block\\npointer takes as input a\\nvector and sorry a pointer not a vector\\ntakes as input a pointer in this case we\\nare\\nsaying create a\\nblock that has the following shape that\\nis sequence length by head dimmension so\\nlet me do it one by one actually I don't\\nwant to confuse you guys with all this\\nstuff Al together okay so take start um\\nthere is a pointer that is right now\\npointing at Q Plus Q KV offset so right\\nnow it is not pointing at the first\\nbatch but it's pointing exactly to our\\nbatch so the the batch that this\\nparticular program should be working\\nwith and inside this badge to the\\nparticular head that this program should\\nbe working with which is is basically\\nsaying that we have um we are pointing\\nto a tensor that is as follows so we are\\npointing to the following tensors which\\nis the right\\nhead the right uh sorry the right batch\\nand the right head and then we are\\nselecting everything inside so it's\\npointing to the first element of this\\nparticular\\ntensor this tensor particular tensor\\nbecause we have already selected the\\nbatch and the head it is a\\ntwo-dimensional tensor with this the\\nfollowing shape because the the\\nfollowing dimensions are sequence length\\nand head dim so we are saying take this\\npointer which contains a tensor of the\\nfollowing shape sequence length and head\\nDimension and I'm also giving you the\\nstrides of this Dimensions that are in\\nthis pointer so the the the two\\nDimensions that are that we need are the\\nsequence Dimension and the head dim\\nDimension which is this one for the Q\\ntensor and\\num and in this um in this\\nquery uh tensor we want to\\nselect a block of queries based on the\\nquery on the block of queries that this\\nprogram should be working with so I\\nthink I need to maybe probably use the\\niPad otherwise it can be very confusing\\nto visualize so uh let's do it actually\\nso let me see if I can create another\\nanother here and let's use the\\niPad all\\nright okay so we have a Q Vector Q\\ntensor because this construct we will be\\nusing it for all the other tensor so if\\nyou understand it for one tensor you\\nunderstand it for all the others we have\\na Q tensor that is a\\nbatch uh by number of\\nheads number of heads then the sequence\\nlength and then the head\\nDimension with the following line so the\\nthis line here so when we create Q Plus\\nuh qkv offset we are already selecting\\nthe right batch Dimension and already\\nthe right head Dimension which means\\nthat we have already forwarded our Q to\\nnot point to the first batch and the\\nfirst head but to point to the exact\\nbatch that this program is working with\\nand the exact head that this program is\\nworking with which basically means that\\nright now it is pointing at a tensor\\nthat is made up of these two\\nDimensions now inside of this tensor we\\nalso need to select the right block of\\nquery that this program should work with\\nand this Dimension here so the sequence\\nDimension is all the queries so we need\\nto select select the right queries so we\\nneed to skip some queries how to skip\\nsome queries well we say that we need to\\nskip block index multiplied by block\\nsize Q number of queries because they\\nwill be processed by another um by\\nanother uh program that will have this\\nnumber here the program ID will be\\ndifferent so we are selecting with this\\nline not only inside of the que the\\nright index and the head but also the\\nright position in this dimension in the\\nsequence length Dimension that will\\npoint to the exact to the starting point\\nof the exact query block that this\\nparticular program should be working\\nwith this is what it's happening and we\\nare also creating this block basically\\nlater we will see how it can be used um\\nto uh to create a block of the shape uh\\nwe are telling what is the the size of\\nthis tensor so this tensor has two\\nDimensions because we are pointing to\\nthe beginning of the right query\\nsequence so it has only two Dimensions\\nthe sequence Dimension and the head dim\\nDimension so it's the last\\nDimension um and we are already pointing\\nto the right beginning of the sequence\\nuh Dimension because we have already\\nskipped some queries why we are skipping\\nsome queries because these queries will\\nbe handled by another program that will\\nhave a block index Q to some other\\nvalues um and this order uh actually I\\ndon't know what is this order you can\\ntry to put 01 and one two I think it's\\nsome optimization that Triton does I\\nhave read the online documentation and I\\ncouldn't find anything about it so this\\nis something that I will investigate but\\nactually even if you put\", mimetype='text/plain', start_char_idx=183865, end_char_idx=188343, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='d64e42c3-d9d4-45b9-9d5d-c9ac840c737e', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a1375460-1f0a-4320-93c2-118c333944d8', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='b57e5fcfccf6ec110a1d02f73fdbe0b94c3f06420478a1cfcfe37422dcf3c5f9')}, text=\"a 01 it doesn't\\nmatter so I think it's something that\\nyou tell Triton uh if this you want the\\ntransposed of this block or you want the\\nnot transposed version of this block and\\nlater we will see actually how we can\\ntranspose the key block without doing\\nany transpose operation actually we we\\nwill just change the strides like we\\nhave seen before so um now this make\\nblock pointer is not something that is\\nnecessary but it makes our life easier\\nwhen we will Index this particular\\npointer so we can treat this pointer\\nnearly as um nearly in the same way when\\nwe work with the tensor in pytorch we\\nwill be able to skip one uh increase uh\\none index in one dimension without\\nhaving to do the computation of the\\nstrides later when doing the backward\\npass I will not use this one and do all\\nthe pointer indexing by hand so you can\\ncheck the differences of indexing a\\ntensor by using make block pointer and\\nnot by using it anyway to rehearse what\\nare we creating we are creating a\\npointer to the right index in the batch\\nto the right index in the head Dimension\\nand we are already skipping some queries\\nbased on the Block index que so this\\npointer is already point to the right\\nblock of queries that this particular\\nprogram should be working with let's\\nlook Instead at the V and the K block\\nnow so let's copy the V block now which\\nis similar to the query but we are not\\ngoing inside we are only indexing by the\\nindex badge and the index head so what\\nthis one actually let me write it here\\nis already skipping\\nso this amount of\\nqueries this is what we are indexing\\nwith this make block pointer so we are\\nin the right batch in the right head and\\nwe are skipping some\\nqueries here we are just indexing by\\nbatch and by head so we are doing V of\\nindex batch index head and we are not\\nselecting we are not skipping anything\\nbecause you see this offset is equal to\\nzero in the First Dimension in the\\nsecond dimension so we are not skipping\\nanything on the sequence length and we\\nare not SK anything in the head\\ndimmension Dimension head Dimension\\nDimension um all right so let's look at\\nthe K block pointer and this is\\ndifferent because as you know when\\nComputing the flashh attention algorithm\\nwe need to have access to the block of\\nqueries and all the block of the key\\ntransposed so when accessing the key we\\nshouldn't access it like we are\\naccessing Q we should invert the two uh\\nDi Dimensions that we want to transpose\\nfor and that's very simple with make\\nblock PTR and you can see it here we say\\nthat we want to point to the right index\\nand to the right head and the tensor be\\ninside of it so let's let me write it\\nhere so later I can explain in line by\\nline so what we are doing here is go to\\nthe K tensor select the right batch\\nselect the right head select everything\\nthat is inside so it's a tensor of two\\nDimensions with the sequence length and\\nthe head dim because we we you can see\\nhere um here uh sequence length and head\\ndims Etc but we don't want first\\nsequence length and then head dim we\\nwant first head dim and then sequence\\nlength so we want to transpose it how to\\ntranspose it we just say that you need\\nto read this tensor with the two strides\\ntransposed so we are saying first use\\nthe stride of the dimension Dimension\\nand then use the stride of the sequence\\nDimension and the shape of this uh\\ntensor is not sequence uh head dim it's\\nhead dim sequence and it's a block of\\nKVs um why we are not putting directly\\nthe sequence Dimension here because we\\nwant to skip block by block later so we\\nare not selecting all the sequence\\nlength in the sequence Dimension we are\\njust selecting a block of KVs and later\\nwe will use another method to go to the\\nnext block\\nso I hope that by showing you the\\nindexing like this it's a little easier\\nto follow the indexing so for each\\ntensor we are going in the right batch\\nin the right head Dimension and for the\\nquery we are skipping some query blocks\\nbecause each each program will work with\\na small different um query block but for\\nthe key and value each program needs to\\niterate through all the key and value so\\nwe just point it to the first key and\\nvalue block and then we will advance by\\none block by um we will advance uh one\\nblock uh by one during the for Loop that\\nwe will do\\nlater then in the output also we need we\\ncan make um a tensor block\\ntensor this basically creates a pointer\\njust like in the query key and\", mimetype='text/plain', start_char_idx=188344, end_char_idx=192702, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='a55f222c-970b-4ad0-814e-e9bbfbd5a06c', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='d64e42c3-d9d4-45b9-9d5d-c9ac840c737e', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='67256e03bd97fcdb0dce6367823fe8975ee7d0f30b103a9c58ef9e9474d5bdc6')}, text=\"value\\ncase in which we select the right index\\nbatch so what we are doing is we are\\nindexing by\\nbatch we are indexing by head and we are\\nselecting everything that inside Unown\\nwe are not cting everything inside we\\nare skipping also in this case some\\nblocks of queries uh because as I said\\nbefore the output has the same shape as\\nthe query so um the this particular\\nblock this particular program that we\\nthat will have this particular block\\nindex Q will only work with one block of\\nuh the queries which will produce only\\none block of the output Matrix and we\\nneed to select exactly that one so we we\\ncan point this pointer exactly to the\\npoint where we should start writing so\\nlet's skip also in this case block index\\nQ multiplied by block size Q um\\nrows so we select exactly the block that\\nour um our program this particular\\nprogram will produce when I speak about\\nthis particular program I mean the\\nprogram that is identified by this\\nprogram ID in the xzero axis and this\\nprogram ID in the first axis because\\neach of these program will run in\\nparallel hopefully and each of them will\\nhave a different value for the block\\nindex q and index batch\\nhead okay now we have pointed our\\npointers to the right position where\\nthey should either read some information\\nor they should either write some\\ninformation by using make block pointer\\nthese uh pointers can also be treated\\ndirectly as tensors so that's why we\\nspecify the shapes of these tensors\\nbecause python uh Tryon right now\\nprovides some methods to work directly\\nwith uh blocks of um to work directly\\nwith pointers like they are we are\\naccessing um tensors so we can index\\nthem like\\ntensors all right so basically just try\\non doing some calculation for you based\\non the strides so you don't have to do\\nit by hand but later when we do the back\\npart T we will avoid using make block\\npointer and we will see the indexing\\ndone by\\nhand all right uh um as you know we are\\nprocessing a single block of queries so\\nlet let's go back to the um algorithm\\notherwise we we lose uh the site of what\\nwe are\\ndoing so let's go here and let's show my\\niPad all right so as you know each\\nprogram we will parallelize along the\\nquery block Dimension so each program\\nwill work with a different query block\\nand then we need to do a for loop on all\\nthe key and values\\nblocks right now we just uh moved our\\npointers to the right position to select\\nthe right query block that we should\\nwork with and to the beginning of the\\nkeys and values block that we should\\nwork with based on which index and which\\nhead this particular program should be\\nworking\\nwith all right now that we have pointed\\nour pointers to the right position in\\nwhich our program should be working it\\ninside of the big pointers that are um\\ninside of the big tensors that are the\\nthat have the batch Dimension the number\\nof heads Dimension the sequence length\\nDimension and the head Dimension we have\\nbecause we are pointing to the right\\nbatch and we are pointing to the right\\nhead these tensors have become two\\ndimensional tensors so they only work on\\nthe they are only tensors on the\\nsequence length and on the head\\nDimension now we need some mod some more\\ninformation that we will use\\nlater the first information that we we\\nneed is the offsets of each query inside\\nof the current block of queries that\\nthis particular program should be\\nworking with and that is given by the\\nfollowing line so let me copy and paste\\nwhich is this one so the offsets of the\\nqueries are the first of all they are\\nhow many of them block size Q because\\neach block of queries is made up of\\nblock size Q number of queries what is\\neach where it's a token and it's on the\\nhead Dimension is um the dim Dimension\\nis not the all the embedding of the\\ntoken but a part of the embedding of\\neach token which part the part\\ncorresponding to the head that this\\nparticular program is going to work with\\nso we are generating the offsets that\\nwill load this particular number of uh\\nthis particular queries from the big\\ntensor that contains all queries and we\\nknow that our queries start at the Block\\nindex cu multiplied by block size Q\\nposition so if this is the program\\nnumber zero they will the Imagine block\\nsize is equal to four they will be the\\nquery with index 0 1 two and three but\\nimagine we are the program number three\\nwhich means that we need to skip um\\nthree multiplied by four so 12 so it\\nwill point to the query number 13 14 15\\nand 16 etc etc\\netc all right and we\", mimetype='text/plain', start_char_idx=192703, end_char_idx=197131, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='499615e9-d6a1-41a4-ba1b-bff874e65f3d', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a55f222c-970b-4ad0-814e-e9bbfbd5a06c', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='3ebc750380d233ca8423d8dca39c4e49aa3fc763fab4d41fbc484b71a2225e16')}, text=\"do the same for the\\nkey and values initially the key and\\nvalues um is a range of keys and values\\nthat we need at e each iteration and at\\nthe beginning because our pointer for\\nthe K andv is pointing to the beginning\\nof the sequence of key and value for\\nthis particular badge and for this\\nparticular head we are pointing to the\\nfirst block of key and value so we are\\nnot skipping anything in the query case\\nwe are skipping because our program will\\nonly work with one single block of\\nqueries in this case we don't skip\\nanything because we need to iterate\\nthrough these key and values so we are\\npointing to the first uh block of key\\nvalues so imagine block size KV is equal\\nto four so this stuff here will be equal\\nto zero 1 2 and\\nthree all right now we need as you\\nremember inside of the flashh atten\\nalgorithm we need to compute a block of\\nquery multiplied by the transpose of the\\nkeys and to each of this block we need\\nto apply the soft Max star if you\\nremember what is the soft Max star is\\nthe soft Max of without the\\nnormalization so while Computing the\\nsoft Max star we also actually compute\\nthe normalization factor without\\napplying it and we apply the\\nnormalization factor at the end so for\\neach block of query multiplied by\\ntranspose of the keys we need to have\\nthe maximum for each row in this\\nparticular block and the normalization\\nfactor for each row so that's why why we\\nneed these two following\\nstatistics which is this one and this\\nthis is basically um a block uh it's a\\nblock of numbers how many based on how\\nmany queries we have in our block of\\nqueries each one initialize with minus\\ninfinity just like in my algorithm that\\nI show before so let me go back to the\\nslides in case we forgot um or actually\\nyou can also check the flash attention\\nalgorithm we initialize it with minus\\nInfinities so so far we are creating\\nthis stuff here so we are initializing\\nthe Mi we are we will be initializing\\nthe LI I and we will initializing the o\\nand then we will show the inner loop\\nhere um and this is exactly the\\nalgorithm that we have seen before so we\\ninitialize M with minus\\nInfinities now we initialize also the\\nL's so let me go back to the\\ncode uh all\\nright so the L's are initialized with\\nthis number here so\\nhere in the O blocks as we can see from\\nthe flashh algorithm they are the O\\nBlock is initialized with zeros so\\nthat's why we initialize a block this is\\nthe output block that this particular\\nprogram will compute which is based on\\nthe position in the batch and the\\nposition in the indexes so it is one\\nblock of the size block size Q so how\\nmany queries there are in this block by\\nhead\\nDimension um which if you want to\\nvisualize it let's go back to the slides\\nit is equal\\nto one block of this Matrix here so it's\\none one block of the output Matrix so\\none row of blocks uh one block of\\nrows uh okay so let's go back to the\\ncode\\nnow all right so now we have initialized\\nlittle stuff here so the output the Mi\\nand Li where Mi is the maximum for each\\nrow in this particular query block and\\nthe LI is the normalization factor for\\neach of the um items in the query for\\neach of\\nthe rows in our query\\nblock now we need to do the for Loop the\\ninner loop in the flashh attention\\nalgorithm uh we will create a separate\\nmethod that will run the inner loop so\\nlet's let me copy\\nit here and I am following the same\\nstructure of the code that you see in\\nthe tutorial of the Tron\\nwebsite so um basically if we are\\nrunning the Cal attention or even if we\\nare not running the Cal attention we\\nmake this for Loop and then we will make\\nanother for Loop and I will show you why\\nso let me first write it and then we\\nwill see so this function here will be\\nthe inner loop this inner loop needs to\\ngo through all key and value blocks one\\nby one and for each query and value\\nblock it needs to fix the previous\\ncalculated block of uh the the previous\\nsoftmax star block so basically what we\\nare doing here we will need to create a\\nfunction as the following where we are\\ngoing to iterate on all the key value\\nblock we will need to compute the query\\nmultiply by the transpose of the keys\\nusing the query block that is fixed for\\nthis program and the key is block is the\\none that we are iterating it through and\\nfor each of these queries we need to\\ncalculate what is the maximum for each\\nrow we need to compute\", mimetype='text/plain', start_char_idx=197132, end_char_idx=201427, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='04c33712-80fa-48f3-abfe-84d13f34972e', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='499615e9-d6a1-41a4-ba1b-bff874e65f3d', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='8649f9f76623d95f4a5d6d0f3ee431f97a628c241cf2909c8cde24376f9ac607')}, text=\"the softmax star\\nso the softmax without the normalization\\nfactor\\nwe need to keep the statistics L which\\nis the normalization factor that we will\\napply at the end of the iteration of the\\nfor Loop and at the same time we need to\\nupdate the output so as you remember the\\noutput is P11 * by V1 plus p12 * by vs2\\nbut we need to fix the previous P11 so\\nto fix that we need to every time we sum\\nto O to the output we need to fix the\\noutput of the previous\\niteration um and then we increase\\nintroduce the p and v block of the\\ncurrent iteration so um here the author\\nof The the code for the the one that you\\nsee on the Tron website decided to split\\nthis for Loop into two steps why because\\nin the causal attention we need to uh\\nwhen we have a caal attention we have a\\ngroup of um we we don't we don't want\\nthe query to attend keys that come after\\nit while in the non-causal attention we\\nlet all the queries attend to all the\\nkeys which also means that we will need\\nto have some kind of if statement inside\\nof this if U inside of this for Loop\\nthrough all the key and values in which\\nwe need to check if the this particular\\nquery that we are working with is comes\\nbefore or after the key and value in\\ncase we are doing the caal attention so\\ninstead of uh iterating through all the\\nkeyan values also in the case of the Cal\\nattention by splitting it into two uh\\nsteps we are saying uh first let's\\niterate through all the keyan values for\\nwhich the index is smaller than the\\ncurrent queries block and for this we\\nneed to compute the attention in the\\ncase of the causal and non-causal case\\nthen for all the elements on the right\\nof this block so for which the key index\\nis more than the Q index in the case of\\ncausal attention we don't need to\\ncompute anything because it will be\\nmasked out because in the soft Max it\\nwill become zeros so it will not\\ncontribute to the output so we don't\\neven have to compute\\nit um this is why we split this this for\\nLoop into two steps so first we iterate\\nto all the parts that are left to the\\ndiagonal of the query multiplied by the\\nkey\\nMatrix so for all the values for which\\nthe query index is less than the key\\nindex then we um and then we skip all\\nthe parts to the right of this diagonal\\nin case we are working with a cal mask\\nbut in case of the non-causal Mask we\\ncompute the left part and the right part\\nof this\\ndiagonal all right don't worry when we\\nrecorde this for Loop it will be more\\nclear so I just wanted to give a little\\nintroduction so let's go uh code this\\ninner loop what will this inner loop do\\nit will work with this particular query\\nblock that we have found so this Q block\\nit will uh why I don't see the Q block\\nbecause I didn't load it well yeah uh\\nlet's load it so we need to load the\\nquery block actually we forgot to load\\nit so as you remember in Tron we we load\\num data from the high bandwidth memory\\nto the SRAM so to the shared memory by\\nusing the load statement and we are\\ntelling load the query block that we\\nshould be working with because this\\npointer Q block PTR is already pointing\\nto the right block that we should be\\nworking with so it's already skipping\\nall the blocks that other programs\\nshould be working with and it will load\\na uh a tensor of size of block size Q\\nhad dim so the right block of\\nqueries and we pass it to this inner\\nloop to which we pass the output so\\nwhere it should write this output the Li\\nand Mi which are the statistics for the\\nrows and for for the maximum for each\\nrow of each query and the LI I which is\\nthe normalization factor for each query\\nand the query block this program should\\nbe working with the beginning of the key\\nand value block pointer because we need\\nto iterate through them so we just point\\nit to the beginning and then inside the\\nfor inner for Loop we will iterate\\nthrough them then the soft Max scale\\nthat we should use when Computing query\\nmultiplied by the transpose of the keys\\nthe block size so how many queries we\\nhave in each block of Q and how many key\\nand value we have in each block of KV\\nuh this is a stage that tells us what uh\\nif we are on the left side of the\\ndiagonal or on the right side of the\\ndiagonal so it will tell us if we need\\nto apply the Cal mask or not based\", mimetype='text/plain', start_char_idx=201428, end_char_idx=205609, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='4a16f9f0-56d5-4705-bdf5-f4cea645fce7', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='04c33712-80fa-48f3-abfe-84d13f34972e', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c62098609ac060c8a9fad9e2ec5aaccea57c3ea6f7999273d26cee5e3366c0c4')}, text=\"on\\nwhere we are and if we are need to apply\\nthe Cal\\nmask um the offset q and the offset KV\\nare just the offsets of the query and\\nkey inside of each q and KV block which\\nis a list of indices that tells us um\\nhow many queries we have\\nuh and then the sequence length the\\nentire sequence length because in the\\nfor Loop we need to iterate to all the\\nsequence length block by block so block\\nof KV block of KV block of KV all right\\nlet's write this me let's write this\\nmethod and later we need actually need\\nto continue this method again so let's\\ngo and let me go\\nhere all right\\nso uh this method we have already seen\\nthe signature so it's just another\\nkernel so it can be called by the first\\nkernel and this is something you can\\nalso do in Cuda you can actually call\\ncall one Cuda kernel from another Cuda\\nkernel um and then we based on the stage\\nof this inner loop we decide what we\\nneed to do so when we are using caal um\\ncausal attention so we only want to\\napply the um attention to the queries\\nfor which the index is less than or\\nequal to the key so we only want the\\nquery to never attend to key and value\\nthat come after it then um we pass the\\nvalue three for the stage parameter now\\nwhen we in the Cal case this will become\\n4 minus 3 it is equal to 1 so what will\\nhappen is that we will only work with\\nthe range of um keys and values that are\\nare from zero up to the current block of\\nQ so all the keys that whose index is\\nless than or less than the the the index\\nof the queries we are working with so to\\nthe left part of the Cal mask let me\\ndraw it otherwise I think it's going to\\nbe very difficult to follow so let's do\\nit actually so let's open a new one and\\nlet's go here all right so we have been\\nusing this one before so we can do it\\nagain clear page all right in this now I\\nI want you to think of the following uh\\nMatrix as a block Matrix so let's draw\\nit in pink because I have been drawing\\nit all in pink we know that in the rows\\nof this query multiplied by the\\ntranspose of the keys we have a uh the\\nqueries blocks of queries so we are not\\nwatching one single block we are\\nwatching all the blocks right now so\\nthis is the query block one this is the\\nquery block two this is the query block\\nthree this is the query block four each\\nof this query block is made up of\\nmultiple tokens of queries and then we\\nhave the key the key\\nblocks uh let's do it like this very\\nugly but okay uh key 1 key block two key\\nblock three key block four when apply\\ncalculating the attention when you\\ncalculate the caal attention so um like\\nwith the causal mask you want only the\\nquery to attend to keys that come before\\nit so when we apply the causal mask this\\nstuff here will be made up of zeros this\\nstuff here will be made up of zeros this\\nstuff here will be made up of zeros and\\nthis stuff here and this stuff here and\\nthis stuff here all made up of\\nzeros we never have to mask out anything\\nwhen we are in this case\\nbecause well when we are in this\\nparticular scenario actually in this\\nparticular scenario we don't need to\\nmask out anything for sure why because\\nall the key um keys in this block so in\\nthis block of keys keys will have an\\nindex that is smaller than the index of\\nthe corresponding queries in case the uh\\nthe key the block size of the query and\\nthe key matches so imagine each query is\\nmade up of three queries so each block\\nof query is made up of three queries so\\nthis is the query number 0 1 and two\\nthis is the query number 3 4 five really\\n3 4 five yeah this will be the number uh\\nsix 7 and eight and this will be the\\nquery number nine 10 10 and 11 in total\\nwe have 12 queries we will have the same\\nindices also for the keys in case we\\nchoose the same uh size for the blocks\\nso this key key block here will be the\\nkey number 0 1 and\\ntwo this will be the key number three\\nfour five this will be the six six 7 and\\neight etc etc ET now what happens is\\nthat in this case as you can see the key\\nin this in indices of the keys are\\nalways smaller than the indices of the\\nqueries so we don't need to mask out\\nanything even in the case of the Cal\\nmask because\", mimetype='text/plain', start_char_idx=205610, end_char_idx=209666, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='2987e925-0354-4db2-bf78-30e14637cc13', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4a16f9f0-56d5-4705-bdf5-f4cea645fce7', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='fc7f6913a246d1f1dbcdeb2729014811ec4ce620b743a626995e05353b559f1d')}, text=\"we are sure that in this\\ncase all of these dot products will\\nnever be masked out also in this case\\nall these dot products will never be\\nmasked out and also in this case we'll\\nnever be masked out we'll never be\\nmasked out and we'll never be masked out\\nand in this case however along the\\ndiagonal some of the queries will be\\nmore have will have an index that is\\nbigger than than that of the keys and\\nsome of them will not be uh will not\\nhave an index that is bigger than that\\nof the keys because these are blocks of\\nqueries and blocks of keys some of them\\nneed to be masked out and some of them\\ndon't need to be masked out so we are\\ndividing our for Loop into multiple\\nsteps the first step that we are doing\\nis all to the left of this diagonal in\\nwhich we don't need to mask out anything\\nthen we will see another step here in\\nwhich we um uh we need to mask out and\\nthen everything to the right of this\\nwill be we will not even compute in the\\ncase of cation because we already know\\nit's made up of zero so it will not comp\\nso the product query multiplied by the\\nby transpose of the keys after the\\nsoftmax will be made up of zeros so if\\nyou look at the flesh attention\\nalgorithm so um this stuff here the\\ncontribution will be zero because we are\\nmultiplying zero with v it will be zero\\nso we don't need to change the output so\\nwhy even compute this part of the Matrix\\nif we already know it's not going to\\ncontribute to the output so we just skip\\nall those iterations and this is why we\\nare splitting the for loop I hope now\\nit's much more\\nclear all right so let's go back um okay\\nso uh we are now to the left part of the\\ndiagonal in case of the stage number one\\nin the case of the stage number two it's\\nthe part in exactly on the diagonal so\\nin which we need to do some dot products\\nand some other dot products we don't\\nneed to do and then for the non-causal\\nattention we just go through the from\\nzero to the sequence length without\\ndoing this\\nmulti-step\\num because we don't need to mask out\\nanything so this is why we have this\\nstage this tells us what is the lower\\nand higher index of the key block that\\nthis particular stage should be working\\nwith all right um now this function here\\nmultiple of is just telling Tron that\\nthis number here is a multiple of this\\nnumber so Tron can make some\\noptimizations so the stage one happens\\nwhen\\nwhen we are doing a causal attention so\\nstage number three in this function and\\n4 minus 3 will become one so imagine we\\nare in the causal attention we will go\\nthrough the key and value block that are\\nto the left of the uh diagonal with\\nrespect to the query block that we are\\nworking\\nwith um in the case we are doing not\\ncausal attention in this first call to\\nthe uh inner function this the stage\\nwill be one so the uh four minus stage\\nwill be equal to three so we will\\nexecute this part of the if statement so\\nwe will go through all the key and\\nvalues in\\ncase uh for the Cal attention only as\\nyou can see here we will do another\\niteration here that will only be done\\nalong the diagonal in which we need to\\nmask out something and we don't need to\\nmask out something because inside of\\neach blocks there will be some keys that\\nhave the index uh below the index of the\\nquery and some that have above the index\\nof the query quy so only in the Cal\\nattention we will call this function\\ntwice the first time with Stage equal to\\none and the second time with Stage equal\\nto two and the second time we will only\\niterate through the group of kyv blocks\\nthat are exactly on the diagonal of the\\num Matrix S multiply by transpose of the\\nkeys the big Matrix that is made up of\\nall the\\nblocks all right now that this should be\\nclear let's proceed further so let's um\\nbecause we need to do the for Loop the\\ninner for Loop of the flashh attention\\nlet's go and load the first blocks of\\nkey and values which is exactly the one\\nthat the key and V blocks are currently\\npointing at which is the Zer z00\\nblock so uh we we defined the the\\npointers basically um we we we point the\\nkey and value blocks to the first uh key\\nand value block that this uh for Loop\\nshould be working with which will be\\nbased on the stage so if it's the first\\ncall to this function they will be\\npointing to the\", mimetype='text/plain', start_char_idx=209667, end_char_idx=213860, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='13fd283d-f1be-43bd-8682-3fc353b7eee8', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2987e925-0354-4db2-bf78-30e14637cc13', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='f2a66d8324dd6a5c643390dd39269d64fa5f9e970e64b34e4a4e78aed551d973')}, text=\"first block in the case\\nof the uh causal and non causal if it's\\nthe second call to this function which\\nonly happs happens in the case of the\\ncausal attention they will be pointing\\nexactly to the key and value block to\\nthe\\ndiagonal all right then we need to make\\nthe for\\nLoop so let's Loop over all the for Loop\\nso let's do it\\nso Loop over the key and value and what\\nwe do is um okay we we let the compiler\\nknow that this number here the start KV\\nwill always be a multiple of the block\\nsize KV because we will be moving from\\none KV block to the next KV block block\\nby block so we let the compiler know\\nthat this number here start KV is a\\nmultiple of block size KV it doesn't\\nchange anything from a logic point of\\nview we are just telling giving some\\nhint to the compiler so it can do some\\nother optimization that Tron does um now\\nthe first thing that we see in the flash\\nattention algorithm is we need to\\ncompute the product of the query so this\\nthe particular block of the query that\\nwe are working with with the current KV\\nBlock in this iteration so let's do it\\nso we compute K andv so we load the the\\nthe query have already been loaded by\\nthe color of this function we have\\nloaded it\\nhere here we have a already loaded the\\nquery but we need to load the current\\nblock of K so we load the current block\\nof K indicated by the K pointer and we\\nmulti we do the matrix multiplication of\\nthe current block of query the the block\\nof query with the current block of K\\nwhich is already transposed because when\\nwe loaded this k k when we defined the K\\nblock pointer we defined it already with\\nthe stride changed so we are reading the\\nT already already transposed so we are\\ndoing the query multipli by the\\ntranspose of the keys\\nbasically okay now let's do\\nhere this part here basically\\nsaying okay if the stage is to when the\\nstages to is when we are exactly on the\\ndiagonal we know that some of the\\nqueries will have an index that is\\nbigger than that of the keys and some of\\nthem we have an index that is smaller\\nthan that of the keys so we need to\\napply the Cal mask only in this case so\\nuh basically what we do is we Define the\\nmask that we should be applying so the\\nmask will mask out all the values for\\nwhich this mask is not true so when this\\nmask is true when the index of the query\\nis more than the index of the K and vs\\nand um we uh okay we apply the soft Max\\nscale so as you remember we here we only\\ncomputed query multiplied by the\\ntranspose of the keys but we also need\\nto divide by the square root of head\\nDimension and we do it\\nhere um and then we because we already\\ncomputed\\nthe uh the the product we can calculate\\nthe maximum for each\\nrow and then we uh we we substract\\nbecause uh when later in the flashh\\nattention algorithm we have another\\noperation which is the which I call the\\nsoftmax star and as you remember the\\nsoftmax star needs to to do uh each row\\neach element of the S Matrix so the\\nquery multiplied by the transpose of the\\nkeys minus the maximum for each row so\\nwe can already compute the maximum for\\neach row and we can also before\\nComputing the maximum for each row we\\nneed to mask out all the elements that\\nwill be masked out in the stage number\\ntwo which is along the\\ndiagonal and how to mask out we just\\nreplace with minus infinity before\\napplying the soft Max all the values for\\nwhich the mask is false um so right now\\nwe are we have computed what we have\\ncomputed the query multiplied by\\ntransport of the keys we have masked out\\nin case we need to mask and when we need\\nto mask only when we are along the\\ndiagonal in all the other cases we don't\\nneed to mask out anything we just\\nmultiply by the soft Max scale and then\\nwe um we subtract the MJ the MJ is the\\nmaximum value for each row because we\\nneed to compute the softmax star\\noperation which is the softmax with the\\nnormalization which in the flash\\nattention algorithm is exactly this\\noperation which will produce the\\nP okay so let's go here so now we can\\ncompute the P block which is this stuff\\nhere which is the exponential of the\\nquery KV block variable here which have\\nalready substracted the m so we have\\nalready substracted this uh Mi at the\\nprevious instruction so now we can just\\napply the exponential and this is what\\nwe are doing here uh okay then we need\\nto compute\", mimetype='text/plain', start_char_idx=213861, end_char_idx=218117, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='14954bc3-190c-4ba3-821b-1e22732a1563', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='13fd283d-f1be-43bd-8682-3fc353b7eee8', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='2c7430672c0fe47fb90c26bcf513c091b9ae401c964893f8124d5909689a4a14')}, text=\"the uh sum of the the row for\\nthe before the um uh normalization\\nFactor so for the current block we will\\nhave a a list of uh we we will have the\\nP block for the current KV block to\\ncompute the normalization factor for the\\nsoftmax we need to keep summing up these\\nexponentials and later we will fix the\\nexponentials the um the normalization\\nfactor that we computed at the previous\\nstep but we will do that later so now we\\njust computed the normalization factor\\nfor the current block which is just the\\nsum of all the values on a single row uh\\nwhich is the same as what we did before\\nhere as you can see\\nhere when I show you the algorithm so\\nfor each um for each block we do the row\\nsum as you can see here\\nof the P Matrix what is the P Matrix is\\nthe exponential of the S minus M and for\\nnow we didn't apply the the the\\ncorrection to the previous block that's\\nit so we computed the L J for the\\ncurrent K andv block and then we comput\\nthe correction factor for the previous\\nblock so the correction factor for the\\nprevious block if you remember the\\nformula from the paper is this one is\\nthe exponential of the previous estimate\\nof the maximum minus the current\\nestimate of the maximum which is exactly\\nthis one so the previous estimate of the\\nmaximum minus the current estimate of\\nthe maximum uh we will see later why Mi\\nis the previous estimate of the maximum\\nand what m j is the current estimate of\\nthe maximum because it is coming from\\nthe current block that we are Computing\\nMi is the let's say the the one that um\\nit is the the one of the previous\\niteration because later we will override\\nMi with m j but I'm just following the\\nflashh attention gthm so far so I am\\nComputing the correction factor of the\\nprevious Li which in the flash attention\\nalgorithm is let me show you uh this\\nstuff here so it is this stuff here this\\none\\nhere um okay and then we apply it so\\napply the correction factor so we apply\\nit so we apply the previous Li with the\\ncorrection factor plus the current Li\\nwhich is the one coming from the current\\nP block the one that we computed with\\nthe current K andv with the current\\niteration and right now we are doing\\nthis operation so Li is equal to the\\nprevious Li multiplied by the correction\\nfactor all right and then what we need\\nto do okay we need to as you remember\\nthe formula is um we um calculate the P\\nblock and then we need to multiply by\\nthe V block so we need to load the V\\nblock so let's load it\\nwe loaded the V block based on the\\npointer of the V block to which this um\\nto to which the pointer V is is pointing\\nto at the beginning of this iteration in\\ncase we are in stage number three so in\\ncase we are doing for example not Cal\\nattention it will be pointing to the\\nfirst k v block uh V block and then okay\\nhere there is just a type conversion so\\nwe make sure this is in floating Point\\n16 and then we\\ncompute the output block so we are\\nComputing the following so we just take\\nv p multiply it by V and we add it to\\nthe output and this is what we are doing\\nhere we take P we multiply it by V and\\nwe add it to the O Block uh let's go\\nactually to this line one by one so\\nfirst of all we need to fix the previous\\nAus block with the correction factor\\ncorrection factor that we have here so\\nwe can fix the previous block with this\\nAlpha term here which is the correction\\nfactor for the previous\\nblock and so we just fixed the previous\\nblock for now but we didn't add the new\\nPV so to add the new PV we do the dot\\nproduct of p and v and this third\\nargument tells the dot this not DOT\\nproduct it's actually the matrix\\nmultiplication uh tell this matrix\\nmultiplication to use this element here\\nas uh the accumulator so this is exactly\\nthe same as doing uh P block multiplied\\nby the V block\\nuh O Block Plus equal to P block\\nmultiplied by the V block uh this is\\njust optimized because anyway this dot\\nuh function here needs some place where\\nto store the intermediate results so why\\nnot just store it where it should\\nactually go and because it um the dot\\nthe the the matrix multiplication is\\njust a DOT product and the dot product\\nis just a repeated sum this accumulator\\nwill be will this dot will keep summing\\nthe result to this block here which will\\nexactly result in this uh um instruction\\nlike we have done the matrix\\nmultiplication separately and we added\\nit to the O\", mimetype='text/plain', start_char_idx=218118, end_char_idx=222410, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='311eafdd-9d2d-4238-8d0e-a740246d87f0', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='14954bc3-190c-4ba3-821b-1e22732a1563', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='94758c2152bfb3c97c8a2f9fc57612e1f604ffd9545923e9062e98b835a6952a')}, text=\"Block so this is uh that's\\nwhy this argument is called the\\naccumulator all right so we have also\\ncomputed the output and then we save the\\nnew estimation of the maximum for the\\ncurrent iteration and it becomes Mi so\\nat the next we can use it to calculate\\nthe correction\\nfactor and then we have finished for the\\ncurrent block and then we can move on to\\nthe next block so we advance our K and V\\npointers by one block of K and v um we\\nadvance it differently because we know\\nthat the V block is a pointer to a\\ntensor of shape let me write it here\\nthis is a tensor of shape uh sequence\\nlength head dim\\nso we need to increase the sequence\\nlength by one KV uh the block size KV uh\\nwhile the K block is actually the K\\ntranspose block so we need to and it is\\ntransposed because we have exchanged the\\nstrides and the shape so it is head\\nDimension head Dimension sequence length\\nso we don't change the head Dimension we\\njust Advance the sequence length by\\nsequence uh block size KB so basically\\nwe are just going to point to the next\\nblock of K and to the next block of a\\nv I hope to you were able to follow the\\nalgorithm of flash atation I try to use\\nthe same names I try to use the more or\\nless the same logic and always writing\\nthe formula that I am referring to so\\nhopefully you didn't get lost I think\\nthe only difference that there is\\nbetween the flashh attention algorithm\\nas written on the paper and this code is\\nprobably this Alpha which is the\\ncorrection factor but I hope it's easily\\nunderstandable anyway um then we just\\nreturn the O Block so O\\nBlock Li I which is the um the\\nnormalization factor for each row in the\\ncurrent output block which is also a q\\nblock because we are working with one Q\\nblock independently from the other\\nprograms and Mi is the maximum value for\\neach row which will be needed for the\\nbackward pass because when in the\\nbackward pass we will compute the Q quy\\nquery multip by transport of the key\\nblck on the Fly we need to also apply\\nthe soft Max but instead of recomputing\\nthe stuff which we already computed\\nduring the forward pass we just save\\nthem and reuse them during the backward\\npass which will save us some\\ncomputation um now I know it's time to\\ntalk about the log some X trick because\\nwe are going to use it so let's go back\\nto the old method so let's go here all\\nright so we have computed two calls of\\nthis function in case we are working\\nwith caal attention in case of the we\\nare Computing Cal attention we call this\\nfunction once to work with all the query\\nblocks that are to the left side of the\\ndiagonal of the query key Matrix then we\\ndo another call of this function to work\\nonly with those blocks of keys that\\nexactly lie on the diagonal of the query\\nkey uh\\nMatrix uh because in this case some of\\nthe values need to be masked out and\\nsome of them do not need to be masked\\nout moreover by doing this we can avoid\\ncomputer in the dot products for all\\nthose values in the Cal m in the causal\\ncase for which the key is index of the\\nkey is higher than the index of the\\nquery saving some computation because\\nanyway they will be resulting after the\\nsoft Max in zeros and they will not\\ncontribute to the output so it should be\\nfaster uh okay now let's go back to the\\nthis method here so calling method and\\nthere is one last thing that we need to\\ndo which is uh we need to compute the\\nlog some exp and now I will show you\\nwhat is it so in order for the backward\\npass to recompute the soft Max without\\nhaving to recalculate the normalization\\nfactor and the maximum value for each\\nrow we should be actually saving two\\ndifferent stuff one is the maximum for\\neach row in the query block and one is\\nthe normalization factor for each query\\nin the query block however there is a\\ntrick and the trick is okay it's not\\nreally called log some X trick because\\nthe logm X trick is used for another\\npurpose but let's call it logm X trick\\nnumber two so um the logm X trick number\\ntwo is something as like this so let me\\nopen the slides so when we do um query\\nmultiply by transpose with the keys we\\nget a matrix that is made up of dot\\nproducts so something like this like\\nthis is one dot product so let's call it\\nquery one transpose the key1 query one\\ntranspose the key2 this is a query two\\ntranspose the key one one and this is\\nquery 2 transpose the\", mimetype='text/plain', start_char_idx=222411, end_char_idx=226679, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='efcd06e9-4ee1-48f0-8db5-206ba1330561', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='311eafdd-9d2d-4238-8d0e-a740246d87f0', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='0341846be256d4f3c061e5a1022090c27ba43ffc9d205f98792b4198a698aa89')}, text=\"key2 then we need\\nto apply the soft Max right so the soft\\nMax is what is the let's write the\\nformula of the soft Max for each of\\nthese vectors so this is a vector and\\nthis is a vector because we applied it\\nby rows for each of these vectors it\\nwill modify element wise each element as\\nfollows so the soft Max of x i is equal\\nto the exponential of x i minus oh my\\nGod I didn't leave enough space so let's\\nmove this stuff here\\nback and this stuff here a little left\\nall right it will be the soft Max of um\\nthe exponential of each element minus\\nthe maximum for the current Vector to\\nwhich we are applying the soft\\nMax divided by the normalization factor\\nwhich is the summation over all possible\\nJS where n in this case is equal to two\\nbecause we have each Vector is made up\\nof two elements of the exponential of x\\ni- x\\nmax now imagine we already have X Max\\nand we already have this summation in\\nthe flashh alation algorithm in the\\nforward pass this stuff here is called\\nLi and this stuff here is called\\nMi what we are going to save in the code\\nyou can see here we are saving actually\\nnot Mi and Li separately we will be\\nsaving Mi I plus the logarithm of Li I\\nso we are going to save Mi plus the log\\nof Li I so what will happen is that when\\nwe will um compute the um compute the\\nbackward pass we need to recreate this\\nMatrix here on the Fly which means that\\nwe need to recompute the query multiply\\nby the transpose of the keys and we to\\num and then we should apply the softmax\\nto apply the softmax we should need this\\nstuff and this stuff here but we have\\nonly this stuff here so so this is the\\nMi plus the logarithm of Li so when we\\nComputing the softmax we will compute\\nthe following so we will compute the\\nsoftmax as follows uh we will Define\\nlet's call it a new softmax so let me\\nuse another color\\nuh\\nhere we will apply the softmax as\\nfollows so\\nsoftmax of\\nXI let's call it the soft Max 2\\nbecause it's I don't want to confuse\\nsoft Max is equal to the exponential of\\neach\\nelement minus we will substract this\\nvalue here the one corresponding to the\\ncurrent row to which we are applying the\\nsoft Max so it will be the exponential\\nof x i minus m i minus the log of Li\\nI if we expand this expression this will\\nbecome the\\nexponential of\\nbecause exponential the sum of two expon\\nthe exponential of the sum is equal to\\nthe product of the two exponentials we\\ncan also write it like this so it will\\nbe the exponential of x i minus m i\\ndivided\\nby the\\nexponential\\num the exponential of the log of Li I\\nwhich guess what it is equal to the\\nexponential of x i minus m i\\ndivided by Li I which is exactly the\\nnormalization factor and we also have Mi\\nso instead of saving two values we save\\nonly one value and when we apply it the\\nexponentials properties will take care\\nof actually also normalizing each value\\nto which we apply it um if you don't\\nremember the properties of the\\nexponential it is very simple so the\\nexponential of a MTI plus b is equal to\\nthe exponential of a multiplied by the\\nexponential of B and the exponential of\\na uh not exponential it's the\\nexponential a minus B is equal to the\\nexponential of a divided by the\\nexponential of B and this is the the\\ntrick that we're using so that's why we\\ndon't need to save two different values\\nwe just need to save one value and then\\nwhen we apply it it will automatically\\nbe taken care will take care of\\nnormalizing because of the properties of\\nthe\\nexponential all right let's move forward\\nso we have also created this um value\\nthat we will use during the backward\\npass now uh as you remember in the flash\\nattention algorithm we don't normalize\\neach block while Computing it we\\nnormalize the output at the end and this\\nis exactly what we are going to do here\\nso we normalize the block at the end\\nafter we have computed all the\\nnormalization factors that we need for\\nall the rows that belong to the current\\noutput\\nblock um we save this Mi so we save it\\num this Mi is what is the normalization\\nfactor and the maximum for each row that\\nwe will need for the backward pass so we\\nneed to save it in a tensor that we will\\nuse during the backward pass so we need\\nto understand which tensor is this and\\nit's the tensor that we called M\", mimetype='text/plain', start_char_idx=226680, end_char_idx=230846, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='10cd6933-4b8f-4cf8-8446-3bf55fe484b1', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='efcd06e9-4ee1-48f0-8db5-206ba1330561', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='ddf433519f77cc3f04f58a314629205e424cebbe19e8c1eefae0bb0515706f3e')}, text=\"which\\nis a tensor of a b size numb heads and\\nsequence length Dimensions so we need to\\nselect the right point in this tensor to\\nselect to where we should save this Mi\\nvalues uh so we need to select the right\\nB size index and the right number of uh\\nhead\\nindex uh so we advance this pointer by\\nthe following offset which is M\\nplus um the index batch head because\\neach um index okay the index batch head\\nis what is the index of the current\\nprogram that includes information about\\nwhich head we are working working with\\nand which batch we are working with\\nbecause each of this um for each batch\\nand for each head we have a sequence\\nlength we can skip n uh a number of\\nsequence length based on which index is\\num okay what we are doing is basically\\nwe are\\nskipping uh for each uh batch and for\\neach head we will have a sequence length\\nbecause each to token in the sequence\\nhas a maximum value and each token in\\nthe sequence will have a normalization\\nvalue So based on the current\\ncombination of batch and head we can\\nskip a number of sequence length that\\nother programs will process so uh\\nbecause in this uh tensor we have the\\nsequence length as the last Dimension\\nand we have what is the combined index\\nof the batch size and number of head\\nsize we can skip a number of sequence of\\nlength based on the combined in index\\nwhich is given by the program index\\nnumber one which is the index batch head\\nthat we have here and this is why we\\nskip here a sequence length number um\\nmultiplied by the index batch head this\\nm is pointing to the\\nfirst uh element of the entire tensor so\\nwe we are skipping the heads and the\\nbatch based on the combined index index\\nbatch head that this particular program\\nis working with and then we have off\\nCube of skew is because each um of this\\nKels the attention forward method will\\nwork with one um query block uh each\\nquery block has some indices for the\\nexact queries it includes and this is\\ngiven by off skew variable that you can\\nsee here which is how many blocks of\\nqueries we need to skip because they\\nwill be processed by other programs plus\\nthe range of queries that this\\nparticular that Noti this that a\\nparticular block of queries has so uh\\nimagine this particular program is\\nworking with the queries that go from I\\ndon't know from uh 12 to 16 then this\\nwill be 12 13 14 15 so the normalization\\nfactor and the maximum value for each\\nrow we only have that for the this for\\nthis indices of query queries so 12 13\\n14 and 15 and that's why we need to also\\nskip the number of queries that this\\nparticular program works with which is\\nalready included in this offset offs Q\\nvariable all right so now we can store\\nthe Mi so because we have the point to\\nwhich where it should be saved and we\\ncan also store the output which was\\ncomputed of by our inner for Loop and\\nthis guys is the forward step of the\\nattention flashh\\nattention now we should go forward which\\nis we should compute the backward path\\nwe also have all the ingredients for\\ncomputing the backward pass because we\\nhave already seen this trick which is\\nthe logm X trick so we already know what\\num how to use it to compute the query\\nkey block during the backward pass on\\nthe\\nFly what we miss to understand the\\nbackward pass well we need to understand\\nwhat is the first of all what is the\\nbackward pass why do we even need a\\nbackward pass we need to understand what\\nis the autograd of py torch how does it\\nwork how to compute the gradient what is\\nthe gradient how to compute do we need\\nto what is the Jacobian when Computing\\nthe gradient on the backward pass do we\\neven need to compute that so we need to\\nderive all the formulas of the backward\\npass by hand so if you are in for the\\nchallenge let's continue all right so\\nnow before looking at the flesh\\nattentions backward pass at at the\\nalgorithm we need to understand why we\\neven need a backward pass and to\\nunderstand why we even need a backward\\npass so before looking at the autograd\\nof P torch we should be looking at what\\nis what are derivatives what are\\ngradients what are jaian so that when we\\ntalk about derivatives gradients and\\njaian we don't feel lost so I will do a\\nvery fast uh let's say rehearsal of what\\nthese topics are now what is the\\nderivative when you have a function that\\ntakes as input a real value and outputs\\na real value we talk about derivatives\\nwhich is defined as follows the\\nderivative of the function with respect\\nto\", mimetype='text/plain', start_char_idx=230847, end_char_idx=235222, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='e6057be4-5b23-405b-8bed-aa3dc0d939d0', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='10cd6933-4b8f-4cf8-8446-3bf55fe484b1', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='1ca422aa3d451e755ef7a5c373f298958c65028a0801428710fe1177bd0caf2b')}, text=\"its variable uh X is defined as the\\nlimit for a step size that goes to zero\\nof the function evaluated at X Plus H so\\nX plus the step size minus F evaluated\\nat The\\nX at x divided by the step size so\\nintuitively we are saying is the ratio\\nof how much the output change for a\\nsmall change for how much the input has\\nchanged in the\\nfunction that this also gives you the\\nintuitive um intuition uh of why the\\ngradient is the derivative is also the\\ntells you the inclination of the tangent\\nline of the um to the function at the\\npoint in which it's\\nevaluated I will use also the following\\nnotation to denote the derivative so the\\nderivative I am used to write it as like\\nthis so frime of X but it's also\\npossible to write it as D of f ofx with\\nrespect to DX or D of Y where Y is the\\noutput of the function with respect to X\\nand they are all equal to the same thing\\nwhich is the definition\\nabove if we invert this form here and we\\ntake H to the left side we can also\\nwrite the follows so if we want to\\nevaluate the function um at at the\\nposition X+ H we can also evaluate it as\\nF Prime of H so the derivative of the\\nfunction in the point x multiplied by H\\nwhich is the step size plus f of x this\\nis actually also how we derive the ler\\nrule uh for computing the differential\\nequations but that's not the topic of\\ntoday so this H we can also call it\\nDelta X so f of x plus Delta X is more\\nor less because here we have a limit\\nthat says when this only happens when H\\nis very very very small so that's why we\\nput this more or less approximately so f\\nof x plus Delta X is more or less equal\\nto frime of X multip by Delta X Plus F\\nofx this you can also read it as follows\\nthat if by inverting this formula um if\\nx changes by a little amount and this\\nlittle amount is Delta X how much y will\\nchange y will change by this exact\\namount which is the derivative of y with\\nrespect to X so Dy with respect to DX\\nmultiplied by how much X has changed so\\nthis Dy DX tells us how much y will\\nchange with a small change of x if we\\nmultiply with the actual change of X it\\nwill tell us by how exactly y will be\\naffected um I don't want to use stay too\\nmuch on this but I I would like to use\\nthis intuition to introduce the chain\\nrule because imagine we have a function\\nof a function so imagine we have Z is\\nequal to F of G of\\nX we can think of X being mapped into a\\nvariable y through the function G and\\nthen y being mapped to into variable Z\\nthrough the function f if x changes by a\\nlittle bit and by a little bit I mean\\nDelta X how much y will change well y\\nwill change by Delta Y what is Delta y\\nDelta Y is the derivative of y with\\nrespect to X multiply by the step size\\nof\\nX but if y changes it will also affect Z\\nbecause there is a direct mapping\\nbetween Y and Z so how much Z will\\nchange for a small change in y let's see\\nso if y changes from the old y by a\\nsmall step Delta y then Z will also\\nchange by some Delta Z and this Delta Z\\nis the DZ of on Dy multip by Delta y if\\nwe replace this Delta by with the Delta\\ny that we have computed in the\\nexpression\\nabove we arrive to the chain rule it\\nwill tell us how Z will be affected so\\nthis is Delta Z what is um the effect on\\nZ for a small change on X and it's the\\nproduct of the two derivatives one with\\nof Y with respect to S and one Z with\\nrespect to Y and this is the chain rule\\nthat we study in high school so it is if\\nyou want to compute DZ on DX it is DZ on\\nDy mtip Dy DX uh which is very intuitive\\nif you think about the following example\\nso you can think of Z as the price of uh\\nC\\nand X as the price of the oil how much\\nwill the a small change in the price of\\noil affect the price of a car well the\\nsmall change in the price of the oil\\nwill affect for example a variable Y\\nwhich could be the price of uh\\nelectricity so if how much the price of\\nelectricity will affect the price of a\\ncar it's through the derivative of the\\nprice of electricity with respect to the\\nuh the price of the car with respect to\\nthe um electricity so to to get the\\neffect of the price of oil\", mimetype='text/plain', start_char_idx=235223, end_char_idx=239221, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='fdabfb7f-164d-4713-be7f-3f44b6fd5f34', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e6057be4-5b23-405b-8bed-aa3dc0d939d0', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='392358ef134cd7ceeebaec28fd007eba17afa1a93231c5f7d1b5016cade98306')}, text=\"on the price\\nof the car we just multiply the two\\neffects and this is the intuition behind\\nthe chain rule anyway let's talk about\\ngradients so when we have a function\\nthat as input takes a vector and\\nproduces a scalar we talk not anymore\\nabout derivatives we talk about\\ngradients so imagine we have a function\\nthat takes as input a vector made up of\\ntwo Dimensions but n dimension in\\ngeneral and it produces a scolar when do\\nwe have to deal with this kind of\\nfunction for example loss functions loss\\nfunctions are something that are always\\na scalar as output and as input they\\ntake tensors so um for example imagine\\nthe crossentropy loss it will take\\na a sequence of tokens each tokens with\\nits own Logics and it will compute one\\nsingle number which is the\\nloss so how to uh View the effect on the\\noutput with respect to the input in this\\ncase well if x changes by a little\\namount and this little amount is not\\nanymore a number but it's a vector so if\\nchange the X the old the X Plus Delta x\\nuh is a vector sum then y will also be\\naffected by what y will be affected by d\\ny on DX multiply by Delta X however this\\nDelta X is not a number anymore it's a\\nvector because X1 may change by a little\\nbit X2 will change by a little bit X3\\nwill change by a little bit X4 blah blah\\nblah until xn will change by a little\\nbit so this is actually a DOT product of\\nthis Vector multiplied by this Vector\\nwhy a DOT product because y will be\\naffected by the change in X1 it will be\\naffected by the change in X2 it will be\\nchange affected by the change in X3 up\\nto xn and each of the contribution of\\nthe contribution of X1 will the partial\\nderivative of y with respect to X1\\nmultiply by how much X1 has changed plus\\nthe contribution FX2 will be the partial\\nderivative of y with respect to X2\\nmultiply by how much X2 has changed blah\\nblah blah until the last uh contribution\\nof\\nxn so and the chain rule in this case\\nalso applies in the same way as in the\\nscalar case so the formula does not\\nchange also for the change rule here I\\njust want you to to to remind that in\\nthis case we are talking about a\\ngradient and the gradient is just a\\nvector um made up of all the partial\\nderivatives of the output with respect\\neach of the input variables that are in\\nthe input\\nVector when we talk about a function\\nthat have as input a vector and produces\\na vector then we don't talk about\\ngradient anymore we talk about jacobians\\nso if our input X the input X of this\\nfunction changes by a little amount and\\nthis Delta X is a vector then the output\\ny will also change and this output y\\nwill change by a Delta y that is not a\\nnumber anymore it is a vector and this\\nVector is the result of this quantity Dy\\non DX multiplied by Delta X Delta X is a\\nvector so this one to be a vector it has\\nthis one here has to be a matrix and\\nthis Matrix is called the Jacobian it is\\na matrix that has as many rows\\nlater we will talk about the denotations\\nso it has as many rows as there are\\noutput variables and as many columns as\\nthere are input variables the first row\\nwill be the partial derivative of the\\nfirst output variable with respect to\\nall the input variables the second row\\nwill be the partial derivative of the\\nsecond output variable with respect to\\nall the input variables and the last row\\nwill be the partial derivatives of the\\nlast output uh variable with respect to\\nall the input variable in the input\\nVector um now let's talk about notations\\nthe Jacobian that I have written here is\\nis is written according to the uh\\nnumerator layout this is called the\\nnumerator layout and there is another um\\nconvention called the oh not layout\\nsorry guys it's called the numerator\\nconvention and there is another\\nconvention called denominator convention\\nor notation in which um the rows are not\\nthe\\nthe number of rows is not the equivalent\\nto the number of output variables but\\nequal to the number of input variables\\nso um the fact that I have we we choose\\nto write the Jacobian as follows is\\nbased on a convention you can also write\\nthe the Jacobian according to the\\ndenominator convention just by\\ntransposing this Jacobian here and also\\nthe formula for the chain rule changes\\naccordingly for now I want to keep the\\nformula for the chain rule just like the\\none for the scolar case so that's why I\\nam using this notation here but later\", mimetype='text/plain', start_char_idx=239222, end_char_idx=243509, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='01fc603f-7dfc-4073-a32c-e705401eff4a', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='fdabfb7f-164d-4713-be7f-3f44b6fd5f34', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='8960a116ed06303706029d835b0e2bf9604531ecf28ac20d7c8b2f43b4b676c1')}, text=\"we\\ncan change between one notation to the\\nother just by doing a\\ntransposition okay now that we have\\nreviewed what is derivative what is a\\ngradient and what is a Jacobian let's\\ntalk about um what happens when we take\\nderivatives with respect to tensors of a\\ntensor with respect to another tensor in\\nthis case we talk about the Jacobian but\\nit's called the generalized Jacobian so\\nif we have the function that is add\\ninput takes a tensor of DX\\nDimensions where the first shape this is\\nkind of the shape of the tensor so the\\nfirst element of the shape is N1 the\\nsecond element of the shape of the input\\nVector is N2 etc etc until n DX and it\\nproduces an output tensor that has this\\nshape so M1 M2 blah blah blah m d y in\\nthis case the formula for the chain rule\\ndoesn't change and\\nif x changes by little amount so by\\nDelta X which is a\\ntensor y will also be affected by how\\nmuch by Dy on DX multip by Delta X and\\nthis is a tensor product it will be a\\nJacobian uh this is called generalized\\nJacobian with the following shape so all\\nthe dimensions of the output multiplied\\nby all the dimensions of the\\ninput all right this is very abstract\\nfor now we will see actually a concrete\\ncase of this one because we will be\\nderiving the the gradient of the output\\nof a matrix\\nmultiplication uh the the gradient of\\nthe loss when Computing backward pass\\nwith respect to each of the input of the\\nmatrix multiplication operation and we\\nwill do it also for the soft Max and we\\nwill do it also for the attention so I\\ndon't want to jump to too many topics I\\njust wanted us to get into the right\\nmindset so we know that derivatives when\\nwe have scalar functions gradients when\\nthe output is scolar input is a vector\\nJacobian when the input and output are\\nboth vectors generalized Jacobian when\\nthe input and the output are tensors the\\nchain rule always works in the same way\\nall right let's talk about Auto autog\\ngradient I will do the scalar case and\\nthen we will extend it to the tensor\\ncase so imagine we have a very simple\\ncomputation graph why we have\\ncomputation graph because we are talking\\nabout neural networks and neural\\nnetworks are nothing more than\\ncomputation graphs where we have some\\ninput with we have some parameters and\\nwe do some operations with this input\\nand parameters suppose that you have an\\ninput a and this input a is multiplied\\nby a weight a parameter weight it's just\\na scalar U and it produces an output y1\\nthis y1 is then summed up with another\\nnumber called B1 and it produces Y2 this\\nY2 is then raised to the power of two so\\nthis Z to the^ of two is just the power\\nof two of the input and it produces Y3\\nand this Y3 becomes our loss function so\\nit's a scalar now uh what we uh want to\\ndo to apply gradient descent is we want\\nto compute the gradient of the um loss\\nfunction with respect to each of the\\ninput of this computation graph so each\\nof the leaves of these computation\\ngraphs what are the leaves it's this\\nnode here so the parameter nodes and the\\ninput\\nnodes um and to do that there are two\\nways one is if you have access to the uh\\nexpression that relates Direct the input\\nto the uh uh output so the to the loss\\nthen you can directly compute the the\\ngradient the derivative in this case\\nbecause it's not a gradient it's a\\nscalar versus scalar so in this case\\nimagine we want to compute the\\nderivative of the loss with respect to\\nW1 imagine we have access to the um\\nexact um expression that relates the W1\\nto uh to the five which is our loss we\\ncan compute it as follows so we just\\nderive this expression with respect to\\nW1 which is two * because this is the\\npower of two of a function so it is two\\nmultiplied by the function multiplied by\\nthe the derivative of the content of\\nthis function with respect to the\\nvariable that we are deriving so it will\\nbecome the following expression there is\\nanother way which is by using the chain\\nrule so we can use the derivative of\\nfive with respect to Y W1 is the\\nderivative of I with respect to Y3 which\\nis the previous uh output of the\\nprevious node then the derivative of 53\\nwith respect to the previous the output\\nof the previous node so and then the\\nmultiplied by the derivative of Y2 with\\nrespect to the output of the previous\\nnode and then the derivative of y1 with\\nrespect to W1 if we do all this chain\", mimetype='text/plain', start_char_idx=243510, end_char_idx=247777, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='c19fe637-323f-4ca9-9f43-09f90dbf8280', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='01fc603f-7dfc-4073-a32c-e705401eff4a', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='8aad720a7bb915b7dc9334096c301ecfa938ffe305c2470230578ebf3bb42971')}, text=\"of\\nmultiplication we will obtain the same\\nresult and you can see that here this\\nstuff here is exactly equal to this\\nstuff\\nhere by doing this procedure here we\\nwill note something that is\\nI want to zoom out a little bit okay to\\ncompute the the derivative of five with\\nrespect to W1 we are doing all this\\nchain of multiplication but what is each\\nitem in what is each um uh factor in\\nthis sequence of multiplications well\\nthis stuff here is nothing more than the\\nderivative of five with respect to Y2\\nthese multiplications here are nothing\\nmore than the derivative of five with\\nrespect to to W uh to respect to y1 and\\nall of them combined are the derivative\\nof f with respect to W1 what pytorch\\nwill do it will do the following pytorch\\nwill do the backward pass because\\npytorch knows what is the computation\\ngraph that relates the output so the\\nloss function in this case and the\\nvariable for which we want to compute\\nthe gradient right now we are talking\\nabout derivatives so uh it's not\\ngradient but the mechanism is exactly\\nthe same so pytorch will say uh it will\\npy is like a person that knocks the door\\nof this operation and\\nsays um hey\\noperation expon power of two if I give\\nyou the gradient of the loss with\\nrespect to Y3 which is one because loss\\nand Y3 are actually the same can you\\ngive me the gradient of the loss with\\nrespect to Y2 because a Pythor actually\\ndoes not Implement an autograd system in\\nin the sense that it does not know the\\nsymbolic operations that led to the\\noutput it just knows what are the\\nfunctions that computed the output and\\neach function has a function each\\nfunction is a class in Python that\\nimplements two methods one is the\\nforward step and one is the backward\\nstep the forward steps takes the input\\nso in this case Y2 and computes the\\noutput Y3 the back First Step will take\\nthe gradient of the loss with respect to\\nits output and needs to compute the\\ngradient of the loss with respect to its\\ninput how can we do that well it's very\\nsimple because pytorch will knock the\\ndoor as let me copy it this stuff here\\notherwise it's not easy to go back and\\nforth\\nso okay and let's past it here pytorch\\nwill knock the door of this function\\nhere and will say hey if I give you\\nthe loss of the Lo the the gradient of\\nthe loss function with respect to your\\noutput can you give me the gradient of\\nthe loss function with respect to your\\ninput yes the function can do it why\\nbecause of the chain rule this operator\\nhere this function here can just do take\\nthe loss uh the gradient of the loss\\nfunction with respect to its output\\nmultiply it by the\\nJacobian uh or in this case the\\nderivative of its output with respect to\\nits input and it will be equal to the\\ngradient of the loss with respect to its\\ninput then pytorch will take this one\\nand knock the door at the next operator\\nwhich is this one this summation and\\nwe'll say hey if I give you the gradient\\nof the loss with respect to your output\\ncan you give me the gradient of the loss\\nwith respect to your input yes this\\noperator can do it because this operator\\njust needs to apply the chain rule so it\\nwill take the gradient of the loss with\\nrespect to um to Y2 which is provided by\\npytorch and by multiplying it with the\\nthe Jacobian in this case it's the\\nderivative the derivative of the its\\noutput with respect to its input it can\\ncompute the uh the gradient of the loss\\nwith respect to its input then pytorch\\nwill take this output of this backward\\npass and will knock the door of the next\\noperator which is this product and we\\nask again the same question hey if I\\ngive you the gradient of the loss with\\nrespect to your output can you give me\\nthe gradient of the loss with respect to\\nyour input yes this will do the same\\nexact job it will take the gradient of\\nthe loss with respect to the output\\nmultiplied by the Jacobian of the output\\nwith respect to the input and obtain the\\ngradient of the loss with respect to the\\ninput and this is how py toch runs the\\nbackward step it runs one operator at\\nthe time backwards in the computation\\ngraph knocking the door of each operator\\nand asking always the same question if I\\ngive you the output the gradient of the\\nloss with respect to your output can you\\ngive me the gradient of the loss with\\nrespect to your input and each operator\\nwill just apply the chain rule to to to\\nget this um to get this gradient to\\ncalculate this gradient that pych\", mimetype='text/plain', start_char_idx=247778, end_char_idx=252139, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='6bc1f28f-1f28-4afe-a54d-1f627c78f0fd', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c19fe637-323f-4ca9-9f43-09f90dbf8280', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='d65e97cb9e238513b708546344b621e42e5a0fc0ea470b1271a2b723321a305b')}, text=\"needs\\nwhy pytorch cannot do it by itself\\nbecause pytorch does not do symbolic um\\nmathematics it does not have access to\\nthe exact expression that each function\\nis Computing it just uses the function\\nas a blackbox that computes forward and\\nbackward however with the Jacobian we\\nhave a problem and let's see what is the\\nproblem all right so up to now we have\\nbeen working with a compostition graph\\nthat is made up of Scholars but the\\nthings that we have said they work in\\nthe scolar case but also in the tensor\\ncase so let's go back see what is our\\ncomputation graph we have seen that\\npytorch will go Operator by operator\\nasking always the same question if I\\ngive you the gradient of the loss with\\nrespect to your output can you compute\\nme the gradient of the loss with respect\\nto your input and each operator can just\\napply the chain rule to compute that uh\\nimagine now that all of these operators\\nare working not with scolars but are\\nworking with tensors which means that\\nthe derivative of the output with\\nrespect to the input of each operator is\\nnot a derivative it will be a Jacobian\\nbecause the output will be a tensor a\\ngeneralized Jacobian and input will be a\\ntensor which means also that this\\nquantity here so the derivative of the\\nloss with respect to the input in this\\ncase will not be a derivative it will be\\na gradient because the output the loss\\nis a number always while the input in\\nthis case y1 will be a tensor so number\\noutput input is a tensor then we talk\\nabout gradients so this will be a\\ngradient the and we will call it the\\ndownstream gradient that the operator\\nneeds to compute this will be the\\nUpstream gradient that pytorch will give\\nto the each of these operators so the\\ngradient of the loss with respect to the\\noutput of each operator and each\\noperator needs to come up with this\\nDownstream gradient by using the\\nJacobian however the Jacobian has a\\nproblem let's see so uh imagine we are\\nimplementing a simple operation that is\\nthe matrix multiplication and the matrix\\nmultiplication is takes as input X\\ntensor it multiplies it by a w Matrix\\nmade up of parameters and produces a y\\nMatrix as output suppose that X is let's\\ncall it n by D Matrix W is uh let's say\\nd by m\\nMatrix and so y will be a n by uh M\\nMatrix usually the input X is a sequence\\nof T of let's say vectors each of each\\nwith d Dimensions so you can think of it\\nas a sequence of tokens each token is a\\nvector made up of the dimensions usually\\nwe have many tokens so suppose that n\\nusually is at least\\n1,24 at least in the most recent\\nlanguage models we even have millions of\\ntokens actually so uh ND is also\\nactually quite big it usually it is at\\nleast 1,24 also so also this one\\nis24 um D and M M is also at least 1024\\nso we can actually become 22 2048 let's\\nsay so I I like the powers of two by the\\nway so the problem of the Jacobian is\\nthis if we compute want to compute this\\nDownstream gradient by multiplying the\\nUpstream gradient with the Jacobian this\\nJacobian matrix is huge because look at\\nthe dimensions here this will be a\\nmatrix that\\nis um it will be well n by\\nm multiplied so it will be a generalized\\nJacobian so it will be a tensor that has\\na shape n m uh and then the input is X\\nso it is n by D so how many elements it\\nwill have well it will have\\n1,24 multiplied by m which is\\n248 multip by 1024 multi by D which is\\n1,24 so it is at least wow it's billions\\nmore than 1 billion\\nelements so it is impossible actually to\\nmaterialize this Matrix here in the\\nmemory because in the ram of the GPU\\nbecause it will be too\\nbig so but we need to compute this down\\nscreen gradient because pytorch needs it\\nto continue calculating the gradient uh\\nof the loss function with respect to\\neach of the nodes in the computation\\ngraph so how can we proceed the first\\nthing that we should notice is that this\\nG ve this uh Jacobian is actually a\\nsparse Matrix and I want to show you why\\nit is a actually is a super super super\\nsparse Matrix because um if if you look\\nat the input what is the effect of the\\ninput on the output the input is a\\nsequence of\\ntokens so this is the token number one\\nit's a vector of some Dimensions 1,24\\ndimensions then we have another token as\\ninput then we have another tokens as\\ninput then we\", mimetype='text/plain', start_char_idx=252140, end_char_idx=256343, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='083138af-e6b4-4209-89e6-d81aff076e8c', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='6bc1f28f-1f28-4afe-a54d-1f627c78f0fd', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='34128a6c569851d6ec511a145580b838234965f0ad071c134041dfde8a570448')}, text=\"have another tokens as\\ninput and we multiply by the W Matrix\\nwhich is made up of some columns uh some\\nuh columns so this one is n by D\\nright yes and W is uh d by m so d by m\\nthis will produce a matrix that is n by\\nm so it will be also a sequence of\\ntokens each made up of M Dimensions so\\nit will be a matrix like this so this\\nwill be the first output token this will\\nbe the second output token this will be\\nthe third output token and this will be\\nthe fourth output\\ntoken now this output row here\\nis the dotproduct of this input row with\\nall the columns so the derivative of\\neach of these Dimensions with respect to\\nthe dimensions of all the other tokens\\nwill be zero because they do not\\ncontribute to this output so the\\nJacobian will have zeros every time the\\nwe are calculating the derivative of\\nthis First Dimension with respect to any\\nother element of other tokens that's why\\nwe always can come up with a better\\nformula for computing this Downstream\\ngradient that does not involve the\\nmaterialization of the Jacobian because\\nthe M the Jacobian itself is sparse so\\nlet's see how we can optimize this uh\\ncomputation without materializing the\\nJacobian in the case of matrix\\nmultiplication because we need it for\\nflesh\\nattention all right guys so before\\nproceeding to the backward uh watch the\\nformulas of the backward path of the\\nflesh attention uh let's look at how to\\ncompute the gradient of the matrix\\nmultiplication operation with respect to\\nits input so imagine we are create okay\\nPythor already have actually how to\\ncompute the the the the gradient of the\\nuh inputs of the matrix multiplication\\nwith the gradient of the loss with\\nrespect to the input of the matrix\\nmultiplication operation but in Flash\\nattention we are creating a custom\\nkernel which means that the custom\\nkernel is fusing multiple operations\\ninto one operation so when pyro will\\nknock the door of our operator it will\\nask the our operator which is the Tron\\nattention operator that we have built\\nwhat is the gradient of the loss\\nfunction with respect to q k and V\\nbecause that's the input of our function\\nso if we look at the code that we have\\nbuilt so far you can see that our trial\\nrotation will be a node in the\\ncomputation graph that Tak take takes as\\ninput q k and V and produces an output\\nthen pyro will give us the gradient of\\nthe loss with respect to that output so\\nit will will give us d o so the\\nderivative of the loss with the gradient\\nof the loss with respect to O and then\\nwe'll ask this class here so Tron\\nattention to compute the gradient of the\\nloss with respect to Q K and B because\\nwe are fusing multiple operations\\ntogether so we are Computing on the Fly\\nthe soft Max of query query multiply by\\nthe transp of the key and then\\nmultiplying doing the soft Max and\\nmultiplying it by uh V to compute the\\noutput uh we need to compute this\\ngradients internally to compute this um\\nthe gradient of the inputs so because in\\nthis operations that we are doing fusing\\ntogether there is a matrix\\nmultiplication we need to derive by hand\\nthe matrix multiplication uh the\\ngradient of the of the loss function\\nwith respect to the input of the matrix\\nmultiplication operation so that we can\\nuh provide it to P torch that's why we\\nneed to derive this formula uh I will uh\\nderive it in the simp in a very simple\\nway and um\\nand then we will do it for the soft Max\\nas well because these are the two things\\nthat we need to derive by hand to derive\\nthe formula of The Flash attentions\\nbackward\\npass so let's start imagine we have a a\\ncomputation graph uh in od in the\\ncomputation graph called the matrix\\nmultiplication and this node in the\\ncomputation graph is doing a matrix\\nmultiplication so it is Computing the\\nfollowing operation Y is equal to X\\nmultiplied W now what py will give us as\\ninput when Computing the backward path\\nof this node py torch will give us the\\ngradient of the loss so it will give us\\nDF with respect to Dy so the output of\\nthis node and we ask us to compute the\\ngradient of the loss function so the\\ngradient of the loss function with\\nrespect to DX and the gradient of the\\nloss function with respect to the W uh\\nthe easiest one to work with and the one\\nthat I will be showing and the other one\\nI will not show in the video but I will\\nattach the PDF slide on how it is\\ncomputed because they are very similar\\nin the way they are computed so\", mimetype='text/plain', start_char_idx=256344, end_char_idx=260674, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='13646459-7589-442e-84fc-ab4c7970459f', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='083138af-e6b4-4209-89e6-d81aff076e8c', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c75b609d765d197bba9d7b5e327528b458af923f4392b431154562a8a4b0136d')}, text=\"I don't\\nwant to make the video too long for\\nUnnecessary\\nreasons let's compute the gradient of\\nthe uh loss function with respect to the\\num input so with respect to\\nX all right so how to do that by hand\\nwithout materializing the Jacobian\\nbecause as we have seen we cannot just\\nuse the chain rule by materializing the\\nJacobian which would be the easiest way\\nbecause the Jacobian is very big Matrix\\nthat cannot even fit in the memory of\\nthe GPU so we need to find a smarter way\\nwe exploit the fact that the Jacobian is\\nsparse so hopefully we will get formula\\nthat does not involve the\\nmaterialization of a very big sparse\\nJacobian let's see so uh let's see um\\nlet's when dealing with this kind of\\nderivations I always recommend to make\\nsome example tensors so suppose that\\nthat X is a tensor of size let's say n\\nby\\nD and where n let's say n is equal to 1\\nand D is equal to let's say three and um\\nX the W is a tensor also or a matrix\\nwith the shape let's say d by\\nm where m is equal to let's say 4\\num and Y will have as a consequence the\\nshape n by\\nm so it will have the shape um well 1 by\\n4 what pyto will give us pyto will give\\nus the following quantity so it will\\ngive us this stuff here so the gradient\\nof the loss function with respect to the\\noutput of this operator which is y so it\\nwill give us a vector or a tensor\\nactually with the following Dimension\\nwhich is n by\\nm and we need to compute the gradient of\\nthe loss function with respect to X\\nwhich should be a tensor of shape n by D\\nbecause um when dealing with the\\ngradient it always has the shape of the\\ninput variable because it's the output\\nwhich is a scalar with respect to each\\nelement in the input so it has the same\\nshape as the\\ndenominator all right so uh when dealing\\nwith these kind of problems I always\\nrecommend to create example matrices and\\nthen work out what happens to the output\\nand then try to work out the the the the\\nthe the gradient Matrix so let's do it\\nso let's see that what is how is the\\noutput computed well the output will be\\na matrix that is 1\\nby4 computed as follows it will be the\\ninput so 1 by3 so let's call the input X\\nX11\\nX12\\nx13 it will be multiplied by another\\nMatrix W that is has Dimension 3 by 4 so\\nit will be three rows by four columns so\\nit will be W1 1 W12\\nW13\\nw14 then w21 w22 w23\\nW2\\n4 W3 1\\nw32\\nw33 W 3\\n4 if we do this matrix multiplication it\\nwill be well it will produce the\\nfollowing Matrix that is okay this is\\none row by three columns this is three\\ncolumn three rows by four columns so the\\noutput will be a matrix that is 1 by\\nfour so one row by four columns so it\\nwill be uh uh let me write it with a\\nsmaller because otherwise it will never\\nfit here\\nso let's do it like this it will be X11\\nultip by\\nw11 plus\\nX12 * by\\nw21 plus x13 *\\nw31 and this will be the first element\\nof the output the second element of the\\noutput will be um\\nX11 with W12 X11 with\\nW12 plus\\nX12 with one two with\\nw22 plus\\nx13 with\\nw32 this will be the second element of\\nthe output Matrix the third element of\\nthe output Matrix will be let me move\\nthis stuff on the left otherwise it will\\nnever fit\\nso okay I think now it can fit this will\\nbe X I need also to watch this one so\\nX11 with W13\\nX1 X11 with\\nW13 plus X1 2 with W 2 3 plus x 1 3 with\\nW3 3 and then we multiply the same row\\nwith the last column so it will be X11\\nw14 + X12 W2 4 + X1 3 W3\\n4 this will be the output uh y if we do\\nthe matrix multiplication what pyto will\\ngive us it will give us the gradient of\\nthe\\nloss so it will give us Delta fi with\\nrespect to Delta y because it's a\\ngradient it has the same shape as the\\ndenominator so it has a shape that is\\n1x4 let's call it because we don't know\\nwhat this value will be they will be\\nprovided to us by pytorch let's\", mimetype='text/plain', start_char_idx=260675, end_char_idx=264376, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='2af346a7-11da-4cf7-b804-f2202e91f5f7', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='13646459-7589-442e-84fc-ab4c7970459f', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='497fbfa00c1dd999a3d8bb81cd9b81531584e4e5fb3ee62c2070f7742294fa0c')}, text=\"just\\ngive them generic name like\\ndy11\\ndy12\\ndy13 and\\ndy4 like\\nthis now to compute the um uh the\\ndownstream gradient that we need to\\nprovide to py torure we should be\\nComputing the we should be m realizing\\nthe jacoban which\\nis um which is uh okay let's write the\\nchain the chain rule formula so we need\\nto provide Delta fi to with respect to\\nDelta X which is equal to Delta fi with\\nrespect to Delta y this is provided by\\nby torch multiplied by the Jacobian\\nwhich is Delta y with respect to Delta\\nX now instead of materializing this\\nJacobian let's try to do this let's\\nmaterialize it now and let's do the\\nmultiplication of these two L quantities\\nto see if something simplifies so this\\nstuff here will be Dy with respect to DX\\nwhich means the derivative of every\\noutput y with respect to every input X\\nhow many output we have we have four\\nelements as the output which is this\\nstuff here and we have three element as\\ninput in the X\\nMatrix so it will be as follows I I\\ndon't know how to let me copy it because\\nmy screen is not big enough and I\\nremember that X is X11 and xx2\\nso uh Delta y with respect to Delta X\\nwill have the following entries so the\\nuh y1 with respect to X11 and as you can\\nsee y1 only has one X11 appearing as\\nmultiplied by w11 so the derivative with\\nrespect to X11 will be\\nw11 then y11 so this stuff with respect\\nto X12 it will be\\nw21 then X um y11 with respect to x13\\nwill be\\nw31 the second row of this Matrix will\\nbe the derivative\\nof the partial derivative of the second\\noutput so w Y2 with respect to all the X\\ninputs which will be the derivative\\npartial derivatives of this stuff here\\nwith respect to every X which is\\nW12 w22 I guess and\\nw32 now let me check if it's what I'm\\ndoing is correct yes because I've\\nalready done it so I can always double\\ncheck uh uh and then we have W the\\npartial derivatives of this stuff here\\nwith respect to all the\\nX which is\\nW13\\nw23 and\\nw33 then the partial derivatives of the\\nlast output so y4 with respect to all\\nthe X which will be W\\noh\\nw14 W2 4 and\\nw34 we obtain the following\\nJacobian if um but this Jacobian as you\\ncan see it's just equal to W\\ntransposed so we don't need to\\nmaterialize the Jacobian we can just do\\nthe multiplication of whatever uh\\ngradient pytorch is giving us multiply\\nit by W transposed and we will get the\\ndownstream gradient so let me rewrite so\\nwe know have we know what we are doing\\nso D on D DX is equal to\\nD with respect to Y multiplied by Dy on\\nDX but we have seen that Dy on DX is\\njust equal to W transposed so this is\\nequal to D on DX Dy multiplied by W\\ntransposed and this gives us the\\ndownstream gradient so in order to\\nprovide the downstream gradient that\\npytorch need we just need to take\\nwhatever gradient pytorch will give us\\nmultiply it by W transposed and it will\\ngive us the gradient of the loss\\nfunction with respect to the input X of\\nthe matrix\\nmultiplication in the same way we can\\nalso arrive to the formula for the\\ngradient of the loss function with\\nrespect to W and it is equal to X\\ntransposed multiplied by\\nDy with respect to uh DW uh Dy\\nhow to remember these formulas these are\\nthere is a ponic rule which is um these\\nare the only possible ways for this to\\nhave the shape of X and this to have the\\nshape of w because this ones this stuff\\nhere will have the same shape of Y so it\\nwill be\\nu n by m this stuff here will have shape\\nof w transpose W is is d by m so w\\ntranspose should be M by\\nD and the resulting operation of this\\nmatrix multiplication or tensor\\nmultiplication will be n by D which is\\nexactly the same shape as\\nX in this case we will have that XT is\\nthe transpose of T and it is uh n by D\\nso it's D by n\\nmultiplied by D with respect to Dy which\\nis a gradient so it has the same shape\\nas the\\ndenominator uh so it has uh n by\\nm uh and the output will have um shape d\\nby m which is exactly the um the shape\\nof w so if you to to remember them this\\nis the only way this shape work out\\notherwise they\", mimetype='text/plain', start_char_idx=264377, end_char_idx=268288, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='72f18398-7deb-47a8-aee8-7ac1b8c2b1dd', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2af346a7-11da-4cf7-b804-f2202e91f5f7', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='8c0fa3ca1c87e500f23db822bee33e1470f6029f2db2b362f5eff8b2f12d19f7')}, text=\"don't work out so this is\\na nimonic formula on how to remember how\\nto compute the gradient of the inputs of\\na matrix multiplication given the\\ngradient of the loss with respect to the\\noutput of the matrix multiplication and\\nthe inputs to the matrix multiplication\\nare the input Matrix and the parameter\\nMatrix W now we need to derive the\\ngradient of the output of the softmax\\nwith respect to the input of the softmax\\nbecause that's another operation that we\\ndo in our fused attention because we are\\nfusing many operations together which is\\nmatrix multiplication and the soft Max\\nso this is the second ingredient that we\\nneed to understand the backward pass of\\nflash attention so let's do\\nit I will use to make this derivation I\\nwill use the same notation as in the\\nflash attention paper so first of all\\nlet's write the title of this stuff\\nwhich is the\\ngradient\\nthrough the soft\\nMax um the first operation that we do in\\num during computation of the attention\\nis we we compute the product of the\\nquery multipli by the transpose of the\\nkeys we do in a block-wise ways it means\\nthat we do it block by block but it\\ndoesn't matter because the end result is\\nthe same so we can also we can write s =\\nto Q multip by the transpose of the keys\\nand then we apply the soft Max to this\\noperation to the result of this\\noperation and we call this output P\\nwhich is the soft Max\\nof s and after the we have applied the\\nsoft Max we take the output of the soft\\nMax we multiply it by V to obtain the\\noutput so the output is equal to P\\nmultiplied by\\nv um now we need to understand how to uh\\nbecause as I saw as I as I said before\\npytorch autograd works in the following\\nway pytorch will treat our attention\\ncomputation as a black box so we will\\nhave a computation graph like the\\nfollowing\\nwe will have a query input a key input\\nand a value input which are sequences of\\ntokens each one with some embedding\\nDimension these are fed to some black\\nbox called the\\nattention which is our implementation of\\nthe attention which is the function that\\nwe started coding before this will be\\nfed as input to this node in the\\ncomputation graph and the computation\\ngraph will output an output tensor o\\nwhat pyro will give us pyro will give us\\nthe gradient of the loss with respect to\\nthe\\noutput so as you remember py knocks the\\ndoor knocks the door at each operator\\nand says if I give you the gradient of\\nthe loss with respect to your output can\\nyou give me the gradient of the loss\\nwith respect to your inputs and this is\\nwhat we need to figure out so giving the\\ngradient of the loss with respect to the\\noutput we need to understand how to\\ncompute the gradient of the loss\\nwith respect to WQ the gradient of the\\nloss with respect to w k the gradient of\\nthe loss with respect to w v however the\\nthere is no direct connection between q\\nand o or k and o because there are two\\nintermediate operation so one there is a\\nfirst a matrix multiplication then there\\nis a softx then there is an additional\\nmatrix multiplication however we have\\ntools that allow us to understand how\\nthe gradient propagates through multiple\\noperations when they are applied in\\nsequence and that's called The Chain\\nrule however we have seen that applying\\nthe chain rule in its naive way by\\nmaterializing the Jacobian is invisible\\nso we need to understand how to apply\\nthe chain rule without materializing the\\nJacobian and that's what we are going to\\nfigure out for one of the operations\\ninside of this attention computation\\nwhich is the softmax and that's why we\\nare going to do this derivation which I\\npromise is the last one that we will do\\nand then we will finally go to code the\\nbackward path of fles attention we\\ncannot proceed directly to coding the\\nbackward pass of The Flash attention\\nbecause if we look at the formulas on\\nhow it is computed we will not\\nunderstand how the um the derivation\\ncomes\\nout okay now we can\\nstart so let me delete this stuff delete\\nand imagine for Simplicity now we apply\\nthe soft Max to a uh rowwise to this s\\nMatrix so each row is soft maxed\\nindependently from the others\\nso let's see what happens to one single\\nrow of this Matrix and for Simplicity I\\nwe call it s so s is uh a single row of\\nthe S Matrix I could also call it s of I\\nbut if I do it like this we will have to\\ncarry over the index I okay guys just\\njust do it we will carry over the index\\nall right so let's call SI one row\", mimetype='text/plain', start_char_idx=268289, end_char_idx=272642, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='b2063d28-b7b9-4c43-a492-6d710dfd4fd1', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='72f18398-7deb-47a8-aee8-7ac1b8c2b1dd', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='a692327a352be241ae02ab069552e39fda4132fbcd6d98727c363061e5c3f31b')}, text=\"of\\nthe S Matrix so SI is equal to Let's\\nlet's say it's the in tensor notation py\\ntensor notation it would be like this so\\nfrom The Matrix S so from the tensor s\\nwe take the I row and all the columns\\nthis is the definition of Si I know it's\\nvery ugly notation but it helps you\\nunderstand and this is a vector of size\\nand\\ndimensions uh we apply the soft Max to\\nthis uh vector and we will obtain an\\noutput vector and we call it Pi Pi is\\nequal to the uh soft Max soft Max of Si\\nso as we have seen the soft Max\\noperation does not change the shape of\\nthe input it just change element wise\\neach number um so it the output will\\nalso be a vector of size R to the power\\nof n now\\num what is the soft Max so the soft Max\\nis defined as follows the soft Max\\nof uh well p i j so the J element of the\\np i Vector is equal to the exponential\\nof the J element of the SI\\nVector divided by a nor normalization\\nfactor that is computed as follows uh\\nwith uh let's say not J let's use K in\\nthis case not even K let's use l\\nis equal to 1 up to n of e to the power\\nof s i\\nl all right so uh first of all you may\\nbe wondering the soft Max that we are\\nthat we apply during the forward pass of\\nthe computation of the attention is not\\nreally this softmax because in if you\\nremember what we applied before we were\\napplying the soft Marx where each of the\\nargument of the exponential is reduced\\nby the maximum element in the vector to\\nwhich we appli the soft Max so it was\\nmore or less like this so s i minus SI\\nMax so the maximum element in the SI J\\nSI\\nvector and also the argument of the\\ndenominator was reduced by Si\\nMax\\nhowever we also proved that this stuff\\nhere is equivalent to the standard soft\\nMax without this reduction in the\\nargument because this reduction in the\\nargument is only added because we want\\nto make it numerically safe to compute\\nbut there is it's equivalent to do it\\nwithout from a mathematical point of\\nview on the computer of course it will\\nbe become numerically unstable but from\\na mathematical point of view it is the\\nsame thing which also means that doesn't\\ndoesn't matter how you comput the\\nforward pass if it's equivalent to\\nanother mathematical definition you can\\nalways use the other mathematical\\ndefinition to compute the backward pass\\nit will result in the same value if you\\ndidn't understand what I said let me\\ngive you a more simple example which is\\num imagine you have a uh do you remember\\nthe formula from high school this one so\\nCo cosine of s of X Plus sin 2 of X is\\nequal to one now imagine we compute uh\\nan output Y is equal to cosine 2 of X\\nand then we need to compute the\\nderivative\\nof y with respect to X it doesn't matter\\nif you compute it as the derivative of\\ncosine squared of X with respect to X or\\nif you compute it as the derivative of 1\\n- sin 2 of X with respect to to X\\nbecause they will result in exactly the\\nsame result uh because the two\\ndefinitions are equivalent and this is\\nwhy we don't need to add this um this\\nfactor in the exponential uh because the\\ntwo definitions are equivalent\\nmathematically we just use the\\nnumerically save one because when\\ncomputed on the on the computer we need\\nsomething that is numerically save uh\\nstable that will not\\noverflow all right now um what do we\\nwant to obtain\\nso we want to obtain the uh the gradient\\nof the loss with respect to the input\\nVector of the soft Max which is the SI\\nVector given the gradient of the loss\\nwith respect to the output of the\\nsoftmax which is the pi\\nVector multi and we can obtain that with\\nthe chain rule multiply that by the\\nJacobian Pi with respect to uh s\\nnow we uh the chain R is always valid\\nlet's see what does this Jacobian look\\nlike um all right so this Jacobian will\\nbe the pi with respect to Delta\\nSII uh well we need to do it let's look\\nat what each element in this jacoban\\nwill look like so the J element with\\nrespect to to the let's say the K\\nelement so we are\\num we are Computing the\\nthe we are looking at what each element\\nin this Jacobian will look like which is\\nwhat is the Jacobian it's each element\\nin the out in the numerator of the\\nJacobian derived with respect to each\\nelement in the denominator of the\\nJacobian uh in this uh fraction here\", mimetype='text/plain', start_char_idx=272643, end_char_idx=276797, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='c6eafc06-682c-4318-9692-c3fb4ad7ccef', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='b2063d28-b7b9-4c43-a492-6d710dfd4fd1', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='8b3cc4d7113d8740af1936e976e5c0a799a9b5a5d904407fffe72af2ee5f9fc2')}, text=\"so\\nwe are saying for each element in the\\noutput Vector derived with respect to\\neach element in the input Vector this is\\nthe what we are writing here so what is\\nhow is the output Vector obtained well P\\nJ we know that it is equal to by the\\ndefinition of the softmax is obtained as\\nfollows so e to the power of s\\nj divided by the normalization factor\\nlet's call it l\\nis equal to 1 to n e to the power\\nof uh s i\\nl uh all derived with respect to s i\\nk i k so what we are trying to do is we\\nknow that the P Vector is suppose it's a\\nvector with the three elements so this\\nis a p one this is well P11\\n1 one P1 2 and\\np13 the S Vector will be a vector also\\nwith the three elements so it will be\\nthe\\nS11 S12 and\\ns13 what we are trying to do is\\ncalculate what the Jacobian will be the\\nderivative of this one with respect to\\nall the input Vector then then the\\nsecond row of the Jacobian will be the\\nderivative of this one with respect to\\neach of this input element then the\\nthird row of the Jacobian will be this\\nstuff here with respect to derived with\\nrespect to each of the input element of\\nthe S Vector we are trying to understand\\nwhat does the generic element in this\\nJacobian look like based on the J date\\nelement of the output Vector so this J\\nindex refers to the output vector and\\nthe K element in the input\\nVector all right so what can happen when\\nwe do this uh jacoban is that we have a\\nthis one here is the derivative of a\\nfraction of two functions and we know\\nfrom high school that the derivative of\\nthe fraction of two functions is as\\nfollows so the derivative oops the\\nderivative let me write like this of f\\nof x with respect to G of X Prime is\\nequal to with respect to X by the way is\\nequal\\nto uh F Prime oops\\nof x * by G of x minus uh G Prime of x f\\nof\\nx all divided by the uh G of x to the\\npower of 2 like this now let's apply it\\nhere so this will become here we will\\nhave two cases either the variable that\\nwe are deriving with respect to so this\\ns i k has the same index as the variable\\nbeing derived so either we are doing P11\\nwith respect to S11 or we are doing P11\\nwith respect to something else that has\\nnot the same index so like P11 with\\nrespect to S12 or s13 so there are two\\ncases that we need to consider suppose\\nthat we are deriving P11 with respect to\\nS11 or we are deriving p12 with respect\\nto S12 or we are deriving p13 with\\nrespect to s13 so we are deriving the\\nelement of the output with respect to\\nthe same element in the input with the\\nsame\\nindex so in this case the this um this\\nuh derivative will look like the\\nfollowing so it's the derivative of f so\\nthe numerator with respect to the\\ndenominator that has the same index so\\nwe are saying that in this\\ncase uh J is equal to K\\nso uh the numerator with respect to SI J\\nwith respect to uh e to the power of s j\\nwith respect to s j will be e to the\\npower of s j so because e to the power\\nof X1 with respect to X1 will be e to\\nthe power of X1 so this is equal to I am\\nreducing the size\\nnow e to the power of s i j then we need\\nto multiply that by the denominator of\\nthe fraction which is this summation\\nhere so the summation\\nover all possible\\nL of e to^ of s i\\nl uh minus the derivative of the\\ndenominator with respect to the variable\\nbeing derived so this denominator is the\\nsum of all the exponentials of all the\\ninput elements if we derive it with\\nrespect to one particular input element\\nthere will be at least one term that\\ncontains that input element and so the\\nthe all the other terms will result in\\nzero so the only derivative that will\\nsurvive will be the e to the power of s\\ni k with respect to s i\\nk so we write minus e to the power of Si\\ni\\nk multiplied by the numerator which is e\\nto the^ of Si\\nJ all this divided by the denominator to\\nthe power of two which is this summation\\nhere so L = to 1 up to n e^ of s i\\nl all to the power of two and this stuff\\nhere will be equal to well we can see\\nthat the this two\", mimetype='text/plain', start_char_idx=276798, end_char_idx=280686, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='403cb761-0659-4ba3-a3c4-c6b326faeeb5', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c6eafc06-682c-4318-9692-c3fb4ad7ccef', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='23c46b5b6a10e47cfb3f5d0527cd72ec2dc554860bcc4526f03cdf30597751bd')}, text=\"term this one and this\\none have a one term factor in common\\nwhich is e to the power of s j so we can\\ncollect that so e to the power of s i j\\nmultiplied by the\\nsummation minus e to the power of s i\\nk all this divided by the denominator uh\\nwhich is the power of two of this stuff\\nhere so let me just copy and paste it\\nwhich is let me rotate it also because I\\ndon't know why I always write\\nlittle\\nlittle yeah all right and this stuff\\nhere is equal to well uh we can separate\\nthe two terms so we can separate this\\nterm here and this term here because the\\ndenominator is to the power of\\ntwo so we can write it also as e to the\\npower of s\\nj mided by the denominator so which is\\nsummation of l = 1 up to n e to the^ of\\ns i\\nl uh multiplied by this stuff here so\\nthis stuff here divided by the same\\ndenominator so there's summation of l =\\n1 up to n e to the^ of s i\\nl minus E to the^ of s i\\nk i Am s i k divided by the same uh\\ndenominator\\ns i\\nl now this one can be written as this\\nstuff here is nothing more than the\\noutput element P J because this one is\\njust the soft Max applied to the SI J\\nelement which we know that the output of\\nthe softmax applied to the SI element is\\ncalled P because it's one element of the\\noutput Vector which we called P so this\\nstuff here is equal to p i\\nj multiplied by\\nthis stuff here will be equal to 1 minus\\nthis stuff here what is this stuff here\\nis the output of the soft Max appli to\\nthe s i k element so it will be p i k so\\nit is equal to 1 minus p i\\nk okay and this is in the case the the\\nthe variable with respect to which we\\nderive has the same index as the\\nnumerator in this uh fraction\\nhere uh in this derivative here uh the\\nother case is when the two variables so\\nthe the output the index of the output\\nwith respect to the index of the input\\nare not the same in this case we will\\nhave another case so we will have that\\nJ uh let me write it again so this stuff\\nhere hope I can copy it all\\nwithout in the other case in which s is\\nnot equal to\\nJ uh not s it's j not equal to K so J is\\nnot equal to K what\\nhappens in this case it will be well uh\\nthe derivative of the numerator because\\nwe need to apply again this formula here\\nso derivative of the numerator with\\nrespect to something that is not the\\nsame variable it will be zero because\\nit's like Computing the derivative e to\\nthe^ of X1 with respect to X X2 it will\\nbe zero so it will be zero so all the\\nfirst term here will become zero no\\nmatter what is g of x minus the\\nderivative of the denominator of this\\nfraction here with respect to the\\nvariable SI\\nK uh G Prime of s i k so this is all the\\nvariable in the input and we are\\nderiving it with respect to one\\nparticular variable of the input so only\\none item in the summation will survive\\nso it will be the item s i\\nk so it will be e to the power of s i k\\nmultiplied by f of x which is the\\nnumerator in this fraction which is e to\\nthe power oh we forgot a minus e to the\\npower of s\\nj uh let me see if I forgot something\\nall divided by the denominator of this\\nfraction here to the power of two\\nso it is equal to the\\nsummation l = 1 up to n of e to the\\npower of s i l all to the power of\\ntwo uh I believe I didn't forget\\nanything so let's continue so here also\\nwe can see that this one here is because\\nokay let's separate it minus E to the^\\nof s i k divided by the summation l = 1\\nup to n of e^ of s i l multiplied by e^\\nof s i j divided by the summation l = 1\\nup to n of e to the power of s i\\nl this stuff here is nothing more than\\nthe soft Max appli to the K element of\\nthe SI Vector this one here is nothing\\nmore than the softmax applied to the J\\nelement of the s I Vector so we know\\nwhat these are we know that we call them\\nP minus p i k p i\\nj so in the end we have two cases one is\\nthe derivative of this stuff here looks\\nlike the\\nfollowing\", mimetype='text/plain', start_char_idx=280687, end_char_idx=284465, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='9c49ecac-46e7-4810-bb61-9473bd15c56d', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='403cb761-0659-4ba3-a3c4-c6b326faeeb5', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1a5fc6d1d8181b28cfe378e78bf230fb3212e5b0bf8c0e60b8a44de8a5b555b')}, text=\"each item in the Jacobian\\nlooks like the following when the\\nnumerator and the denominator have the\\nsame index so J equal to K this stuff\\nhere is equal to now this notation here\\nis wrong so I shouldn't be writing it\\nwith equal sign but doesn't matter guys\\nit's we are doing a\\nlittle um okay so p i\\nj p i j multiplied by 1 minus p i k let\\nme check yes the other Cas is when the J\\nis not equal to k then this stuff here\\nlet me write it like this\\nwill be equal to minus p i k MTI p i\\nj now that we know what the two typical\\ncases of this Jacobian look like let's\\nactually look at what this Jacobian look\\nlike in The Matrix\\nform so this Jacobian will look like the\\nfollowing it will be a matrix that is\\nmore or less like the following it will\\nbe an n byn Matrix where n is the size\\nof the input vector and the output\\nvector and here the first element of the\\nJacobian as you saw as you remember uh\\nthe first row of the Jacobian in the\\nnumerator convention is the uh\\nderivative of the first output with\\nrespect to all the input so this first\\nterm here will be the derivative of P11\\nwith respect to\\nS11 so in this case J and K match so we\\nknow that it will be equal to P1 1 * 1 -\\nP11 the second element to the right of\\nthis one so the element one two will be\\nuh the derivative of p12 with respect to\\nuh sorry the P11 with respect to S12 the\\nJ and K do not match so we will be in\\nthis case here so it will be minus\\nP11\\np12 the third element you can check it\\nby yourself it will be minus P1 1 p13\\nblah blah blah until the end which will\\nbe - P11\\np1n the second row of this Jacobian will\\nbe uh will look like this so it will be\\nthe derivative of p12 with respect to\\nS11 the J and K do not match so we are\\nin this case here so it will be minus P1\\n2 P11\\nthen the next element it will be the\\nderivative of p12 with respect to S12 so\\nJ and K match so we are in the first\\ncase so it will be P1 2 * 1 minus\\np12 then this stuff here will be equal\\nto then the third element will be minus\\nP1 2 with respect to\\np13 blah blah blah and until we arrive\\nto the last one which is minus P1 2 with\\nrespect to P1 n not with respect to\\nmultiply\\nby and all the elements like this until\\nthe last row the last row will be the\\nthe first element of the last row will\\nbe the derivative of the uh last output\\nelement with respect to the first input\\nelement so it will be the derivative of\\np1n with respect to S11 so um the two\\nindices do not match so we are in the\\nsecond case so it will be minus P1 n\\nP11 this will be minus P1 n p12 etc etc\\netc let me do also the third element\\nsince we are here so minus P1 n P1 3 etc\\netc etc until the last element of the\\nlast row which will be minus\\np1n P1 N I\\nguess oh no that's wrong guys because\\nthe two indices match so it should be\\np1n multiplied 1 minus\\np1n this is what the Jacobian will look\\nlike let's see if we can find a better\\num uh how to generate this Jacobian with\\nsome pattern\\nrecognition let's write it in a\\ndifferent way first of all the thing\\nfirst thing that we can notice is that\\nthis Jacobian is symmetric so you can\\nsee that this element is equal to this\\nelement if you expand the third row you\\nwill see that it's equal to this element\\nthis one on the top right corner is\\nequal to the one in the top bottom left\\ncorner um so this Matrix is\\nsymmetric the second thing that we can\\nnotice is that only the element in the\\ndiagonal are different they have an\\nadditional term because you can look at\\nthis element here so let me write this\\nelement here can also be written as\\nP11 minus P11 * by P11 the second\\nelement here in the second uh row so\\nsecond diagonal element of this Matrix\\nis p12 minus p12 * p12 so this the\\nelement on the diagonal actually look\\nlike just like the other elements they\\njust have an additional\\nterm which is P11 in the first diagonal\\nelement p12 in the second diagonal\\nelement so we can\", mimetype='text/plain', start_char_idx=284466, end_char_idx=288296, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='c54b03b0-2571-4621-a74f-76808b790ddb', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9c49ecac-46e7-4810-bb61-9473bd15c56d', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='34ef5267853b2b1cdeda06607540f49c41d9528e8a23914a792a0b31f5ff4f01')}, text=\"also say that this\\nMatrix here is the product of all the\\npossible combinations of P J with p i\\nk which you we can obtain with an outer\\nproduct or even with the product of one\\ncolumn with the transpose of the same\\ncolumn so if you do one column Vector\\nfor example imagine p is a column vector\\nand you do p multiplied by PT you obtain\\nall the possible combinations of\\nproducts of these two vectors because\\nthis will be one\\nI can do a simple case so P11 P1 let's\\ncall it P2\\nP3 uh multiplied by the row\\nVector P1 P2 P3 this will generate all\\nthe possible uh combinations of products\\nbetween P1 and the P the first vector\\nand the second Vector because this will\\nbe a 3X one this is 1 by3 so it will be\\ngenerated 3x3 vector and it will be uh\\nequal to P1 P1\\nuh P1 P2 P1 P2 P1 P3 etc etc etc\\nmoreover we can see that in the diagonal\\nof the Matrix we have this additional\\nterm this additional term P1 in the\\nfirst diagonal element p p12 in the\\nsecond diagonal element p13 in the third\\ndiagonal element uh I actually call it\\nP1 it's wrong because I should call it\\nPi I that's why I didn't want to bring\\nthe I uh indices so it's not really P1\\nit's should be Pi I Pi Pi I Pi because\\nwe are doing it for the generic I Pi\\nVector so let me fix the indices p i n p\\nI3 uh this is\\none one one\\none Pi I and pi\\nokay so this is Pi\\nPi Pi Pi Pi Pi Pi I Pi I\\nokay we can obtain um so we can write\\nthe this\\num this Jacobian here also as the\\ndiagonal matrix that in the diagonal has\\nall the element of the pi I\\nVector minus the P Vector multiplied by\\nthe transpose of itself so with itself\\nbut transposed because we need all the\\nelements to be kind of a combination of\\none element of p with it with another\\nelement of p plus only on the diagonal\\nwe need some this additional term which\\nare the elements of p and all the\\nelements of the the the output of this P\\nmultipli by P transposed are negated\\nthat's why we need this minus sign so if\\nyou look at the flashh attention paper\\nthey give you this formula here they say\\nthat if Y is equal to the soft\\nMax of X then the\\nJacobian uh will look like the following\\nwill be um\\ndiagonal of Y minus y y transposed where\\nY is\\num\\nthe is a column\\nVector all right guys I know this has\\nbeen long so let's take a pause and we\\nare going to now um uh code finally\\nfirst of all let's check the mathematics\\nof the backward path of Flesh attention\\nwe will see it briefly I will not do any\\nmore derivation but I will explain it\\nand then we finally switch to coding it\\nso let's\\ngo all right guys now finally we can see\\nthe um the backward path of the flashh\\nattention so we will be looking at the\\nalgorithm and if you look at the the the\\nappendix of the flesh attention p paper\\nyou will see this part b.2 where they\\nderive the backward pass step by step\\nnow I don't want to do all the s all the\\nsteps of this derivation because it's\\ngoing to be too long but I want to give\\nyou all the tools necessary to\\nunderstand it now let's start from what\\nkind of um\\nuh how to say conventions they are using\\nuh notations they are using in this\\npaper so the first thing that we need to\\nrehearse is the naming of what is what\\nis the name of each\\nMatrix uh as you know in the forward\\nattention in the forward pass we do the\\nquery multiplied by the transpose of the\\nkey and the output of this we call it s\\nthen we apply the soft Max to this s\\nMatrix and it becomes the P Matrix the\\nsoft marks is applied by rows then we\\ntalk take this P Matrix and we multiply\\nby a v Matrix to obtain the output of\\nthe\\nattention\\num let's look at for example how the\\ncomputation of the I row of the output\\nis computed based on the P Matrix and\\nthe V Matrix so we can understand this\\nkind of notation that they are using\\nhere in the paper because the way I read\\nthis formula here is the E row of the\\noutput which is a column Vector because\\nin when we write in um in mathemat in\\nlinear algebra whenever we write the\\nname of a vector it is always by\\nconvention a column Vector but\", mimetype='text/plain', start_char_idx=288297, end_char_idx=292243, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='491b4ce8-73f2-4c03-a636-79690e4d2cfd', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c54b03b0-2571-4621-a74f-76808b790ddb', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='0d2e953f1615b9cdff9709d586ce3401c6000bd9bfc950ca25275a47b611ab1a')}, text=\"the\\norigin of this particular Vector is\\nactually a row of the output Matrix\\nlet's try to understand what is the\\noutput uh row of a matrix in a matrix\\nmultiplication now um so that we can\\nunderstand how to go from here to here\\num so let's write a generic matrix\\nmultiplication for example an a matrix\\nlet's say that it is the\\nfollowing and we only write one row\\nactually let me Zoom again and I want to\\nwrite smaller so we have enough space so\\nwe make a matrix that has a row let's\\ncall it A1 A2\\nA3 and then we multiply this will be a\\nmatrix with many rows like the this one\\nbecause we want to study the effect only\\nof one row and we multiply it by another\\nMatrix let's call it this one is the\\nMatrix a and it has I don't know let's\\nsay n rows by three columns then we\\nshould have another Matrix B with three\\ncolumns and some number of um three rows\\nand some number of column let's say four\\ncolumns so we call the first uh row\\nlet's call\\nit let me Zoom more so it's\\nb11\\nB12 B1 3\\nb14 then this one should be B2 2 1\\nB22\\nb23\\nB24 this should be b31\\nb32 uh\\nb33 B3 4 Etc I know I am not very\\nrigorous in my notation I should have\\ncalled all these elements with the\\ncapital letter a and the capital letter\\nB so this is the notation that you use\\nwhen referring to single item of a a\\nmatrix but please forgive me for this so\\nthe output of this matrix multiplication\\nwill be another Matrix that is n\\nby4 so it will be n by 4 so we will have\\nfour columns for each uh row of the\\noutput I want to write the output in a\\ndifferent way so I want to write it as\\nfollows as a vector only so the first\\noutput row as a vector and want to\\nunderstand what is is each dimension of\\nthis Vector so because otherwise I don't\\nhave enough space to write it here so\\nthe uh first uh let's write it so let's\\ncall it o I want to write what is O of\\none which is the first row of the output\\nbut written as a column Vector so o of\\none will be here we should use the small\\nletter O of one should be a vector where\\nthe First Dimension is the dot product\\nof this stuff here so the first row of\\nthe a matrix with the First Column of\\nthe B Matrix so the first uh let's say\\nDimension will be A1 with\\nb11 uh I should also call this one a11\\na12\\nactually and a13 so A1\\n3 uh because we have many rows in the a\\nmatrix so let me use the correct naming\\nso this will be a11 with b11 a11\\nb11 plus\\na12 * by\\nB21 plus\\na13 with\\nb31 and this will be the first dimension\\nof the first row of the output Matrix\\no the second dimension of the first row\\nof the output Matrix o will be the\\ndotproduct of this row of the a matrix\\nwith the second column of the B Matrix\\nand let me write here B so it will be\\na11\\nB12 plus\\na12\\nB22 plus\\na13 b\\n32 the third dimension will be a11\\nB1\\n3 plus A1 2 B2\\n3 plus A1 3\\nb33 the fourth dimension will be\\na11 uh\\nb14 + A1 2 b 2 4 plus A1 3 B 3\\n4 now this is the output the first\\noutput row of the O Matrix and it's a\\nvector called o1 and these are this is\\nthe first dimension of this Vector this\\nis the second this one is the third and\\nthis is the fourth dimension and each of\\nthis stuff here is one\\nscalar um so the\\noutput o1 which is the first row of the\\noutput Matrix can also be written\\nas the first element as you can see in\\nis a sum of many vectors where the first\\nelement is\\na11 Multiplied let me use a smaller uh\\nthis one but I want to use a smaller I\\ncan't change the size\\nhere okay doesn't matter so as you can\\nsee here there is A1 multiplying a\\ndifferent B number every time so this is\\nb11 B12 B13 b14 what is b11 B12 B13 b14\\nit is the first row of the B Matrix so\\nit is equal to B uh 1 and all the\\ndimensions of the first row then plus\\nthen we have the element\\na12 multiplied by B21\", mimetype='text/plain', start_char_idx=292244, end_char_idx=295905, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='d3702f59-bde9-4caa-99f8-59724c6aa147', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='491b4ce8-73f2-4c03-a636-79690e4d2cfd', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='845a7f389d6a0ed87873d6d8157db3643fcfcdb06cc631309af7e2f0e812b686')}, text=\"B22 b23 Etc and\\nthis is uh the second row of the B\\nMatrix so we use the tensor rotation of\\npy torch to describe this row which is a\\nb uh two and all the dimensions of\\nB2 uh so it looks this is a vector\\nscalar product and\\nplus uh\\na13 multiplied by B uh\\n3 and all the dimensions of P3 this one\\ncan also be written\\nas um the\\nsummation over all possible I that go\\nfrom 1 to three where 1 to 3 is how many\\nuh uh columns there are in the a\\nmatrix uh of\\na i\\nj uh well\\nA1 let's call let's call this one J\\nactually sorry let's call it\\nJ equal to one and let's call this the\\ngeneric e row of the output Matrix will\\nbe a i1 a I2 and a I3 each one\\nmultiplied by the corresponding Row in\\nthe B Matrix so we can write it as a i j\\nmultiplied by B uh J where BJ is the uh\\na row of\\nB uh we can also write it like this to\\nindicate that this is a vector and this\\nis exactly what they do here so the\\noutput in the output Matrix when we do\\nthe multiplication P multipli by V the E\\nrow of the output Matrix we call it oi\\nwhich is a vector but by notation it is\\na column Vector where the elements of\\nthis column Vector are actually the\\nelements of the E row of O uh this is\\nonly by notation guys uh is equal to the\\nE row of P so the E row of the Matrix\\nthat is on the left in the matrix\\nmultiplication multiply by all the\\ncolumns of the V Matrix which can also\\nbe written as the summation over all the\\nelements of the I row of P so all the\\nelements of the I row of the first\\nMatrix the one on the left in the matrix\\nmultiplication multiplied by each Vector\\nin the v Matrix where the J Matrix here\\nin V is each row of the V Matrix\\nso and P J can also be written as um p\\nis what is um the the output of the soft\\nMax so as you know the output of the\\nsoft Max is e to the power of the\\nelement input of the softmax what is the\\nelement input of the softmax is the\\nquery multiplied by the transpose of the\\nkeys so it's a DOT product between one\\nquery and one key and that's why you\\nhave this stuff here in the exponential\\nso this is the first step in\\nunderstanding this derivation another\\nthing that we have studied so far is how\\nto derive the uh backward path of the\\nmatrix multiplication\\nand of the soft Max so now let's use it\\nin the matrix multiplication let's\\nrehearse the formula so the if given a\\nmatrix multiplication that is y = to X\\nmultiplied w we know that given the\\ngradient of the loss function with\\nrespect to Y so the output of this\\noperation we know how to derive the\\ngradient of the loss with respect to one\\nof the input of this function which is\\nthe x or W to get the gradient with\\nrespect to X we need to take the\\nUpstream gradient so the the gradient\\nwith respect to the output multiply by\\nthe transpose of WT and to get the\\ngradient with respect to w we need to do\\nthe XT so the input transposed\\nmultiplied by the upstreaming gradient\\nthis one is the formula that we didn't\\nderive and this one is the formula that\\nwe derived but how to derive them is\\nexactly the same\\nprocedure in attention we are doing the\\nlast product that we are doing is O\\nequal to P multiplied by V what pyo will\\ngive us as input during the backward\\npass is the gradient of the loss with\\nrespect to the output and we need to use\\nthis gradient of the loss with respect\\nto the output of the attention to derive\\nthe gradient of the loss with respect to\\nQ with respect to K and with respect to\\nV so that it can then be used by the\\noperators in the backward pass in the in\\ncomputation graph in the operations\\nbefore okay so but in order to arrive to\\nthe gradient with respect to query key\\nand value we need to derive the gradient\\nwith respect to each intermediate\\noperation so the last operation that we\\ndo is O equal to P multiplied by V so\\nthe gradient with respect to O of the\\nloss with respect to V given the\\ngradient of the loss with respect to O\\nit is exactly like Computing the\\ngradient of um the of the loss with\\nrespect to X in a matrix multiplication\\nand we know that it is equal to the PT\\nso just by analogy guys so this is our\\nreference point and I am just changing\\nthe names here and\", mimetype='text/plain', start_char_idx=295906, end_char_idx=299961, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='2c57467f-40b5-43a1-b448-8cf00d58b65c', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='d3702f59-bde9-4caa-99f8-59724c6aa147', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c4b8c65ca669cd0585385304ab846a99d1f4c9278acc821bb297f5a02468419b')}, text=\"you should understand\\nwhat is the analogy here so um the\\ngradient of the loss with respect to V\\nwhich is the Matrix on the right which\\nis like Computing it with respect to to\\nW it is equal to just like this formula\\nhere so the transpose of the Matrix on\\nthe left multiplied by the Upstream\\ngradient which in the paper they write\\nit as this so DV is equal to PT\\nmultiplied by d o and it's the formula\\nthat you say you can see here the other\\nderivation is how to derive the gradient\\nwith respect to DP DP is just like\\nderiving the gradient of the loss with\\nrespect to the Matrix that is on the\\nleft side of the matrix multiplication\\nso it is just like deriving the gradient\\nof the loss with respect to X in the\\nreference uh formulas\\nwhich is equal to the Upstream gradient\\nmultiplied by the transpose of the other\\nMatrix which in the notation of the\\npaper they write it as DP is equal to d\\no multili by V transposed and it's this\\nformula here how they compute this Stu\\nhere is exactly as above so as as this\\nderivation here they call\\nVJ the J row of the V Matrix and they\\nwrite it as um p i J multip by d o how\\nto arrive to this formula here well\\nlet's do it\\nso let me write let's see\\nokay theoretically we know that from\\nthis derivation here so from this\\nderivation here or from this derivation\\nhere we know that the I row of the\\noutput in a matrix multiplication first\\nof all let's simplify our life every\\ntime you see a transposed and you don't\\nlike work with the transposed in a\\nmatrix multiplication just give it a\\ndifferent name and then work with the\\ndifferent name and after when you have\\nderived the formula you resubstitute the\\ntranspose operation in this case we are\\ndoing DV is equal to P transpose multip\\nby d o let's call P transposed let's\\ngive it a name that we are not we didn't\\nuse so far so let's call it f I always\\nused F when it's available so um we call\\nDV is equal to f d o we know from above\\nhere from this derivation here or this\\nderivation here is\\nequivalent that the output of a matrix\\nmultiplication so the out I row of the\\nlet's not the J row let's call it the J\\nrow d\\nv j is equal to a summation of each\\nelement of the J row of f of the first\\nMatrix so we do the let's see here they\\ndo the sum by I so let's do it by I it's\\nthe sum over all possible I of the uh I\\nelement in the\\nJ row of the first Matrix so f j uh yeah\\nFJ\\nI multiplied dot product not DOT product\\nthis is a a scolar uh Vector\\nmultiplication multiplied by a vector\\nthat is let me check what was the\\nformula so it was the J row of the other\\nMatrix so in this case it should be the\\nI row of the other\\nMatrix uh o of\\nI where I this is the I row of I this is\\nthe J row of the V\\nMatrix um and but also we we know that f\\nis not a matrix that we have it's\\nactually the transposed of P which means\\nthat fji will be equal to p i j because\\nin a matrix transposition you invert the\\ntwo indices so this is the summation\\nover all possible I's of P not j i but\\nIJ multiplied by oi and this should be\\nequal to the same formula that you see\\non the right here this allows you to\\ncompute one output Row in the v\\nMatrix okay and we know that p is just\\num the output of the softmax the soft\\noutput of the softmax is the input of\\nthe softmax to the exponential of the\\ninput of the softmax divided by the\\nnormalization factor\\nassociated with the that row so because\\nwe are iterating through the row of I it\\nwill be the I the normalization factor\\nassociated with that row of um of\\nOI so we know that the for formula for\\nthe p is equal to the soft Max of s now\\nthe I row of P will be the soft Max of\\nthe E row of s and this is what is\\nwritten here we know from our derivation\\nthat the Jacobian with respect to the\\nsoftmax operation so if we have an input\\nX and the output is y of the softmax\\noperation the Jacobian of this um of the\\nY with respect to the x is equal to the\\ndiagonal y it's a diagonal matrix of the\\nelement of the factor y minus y * y\\ntransposed and we have also seen before\\nthat this Matrix is\\nsymmetric however you may not\", mimetype='text/plain', start_char_idx=299962, end_char_idx=303971, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='9ff87ad7-af5b-4e09-a07f-3d6a50d0b370', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2c57467f-40b5-43a1-b448-8cf00d58b65c', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='a69d880145fc644bbb723127d405a2feefe6ac675b0861c81b04a2ce06c48553')}, text=\"understand\\nthis formula here because we have seen\\nfrom our uh in the chain rule we always\\nwrite it like this we always write that\\nthe downstream gradient so the D um uh\\nFe of let's say um TX\\nshould be equal to the uh Upstream\\ngradient so d f with respect to Dy\\nmultiplied by Dy and um with respect to\\nDX this only works if you make this\\nMatrix here as in the numerator\\nconvention the numerator convention is\\none of the two convention in which you\\ncan create a Jacobian we so far we have\\nalways written it as the numerator\\nconvention if you use the Ator\\nconvention this is a row vector and this\\nis a row Vector however if you want to\\ntreat this stuff here as a column Vector\\nthen you need to take the transposed or\\nyou need to make the Jacobian in the\\ndenominator\\nconvention how to get this formula here\\nbecause this formula here is basically\\ndoing the Jacobi and multiply by the\\nUpstream um uh gradient not the gradient\\nUpstream gradient multiplied by the\\nJacobian and it's only because here we\\ntreat it as a column vector and when you\\ndo the you want to transform a row\\nVector into a column Vector you take the\\ntranspose of both sides of the equation\\nand let's do it actually so we apply the\\ntranspose to the both side of the\\nequation okay um in a matrix\\nmultiplication if you do uh a b\\ntransposed it become B transposed\\nmultiplied by a transposed so the\\ntransposed is applied independently to\\neach input of the the matrix\\nmultiplication but we invert the matrix\\nmultiplication and if you remember the\\nmatrix multiplication is not commutative\\nso what we do here is that we say okay\\nit will be the D Fe of DX and here they\\ncall\\nit here they call it\\nDSi so it will basically just become D\\non DX if you treat this one as a column\\nVector so this one as a column Vector\\nwill be equal to Dy on DX as a column\\nVector as a Jacobian in um in\\ndenominator layout in this case\\nmultiplied by d f on d y as a column\\nVector this one is a column Vector this\\nis a column vector and this is what you\\nsee here that's why the Jacobian is on\\nthe left side of the Upstream\\ngradient uh what else we need well I I\\nknow that there is a lot of things here\\nin this derivation but I prefer actually\\ngoing directly to the code otherwise I\\nthink it's going to be too\\nboring um so let's go to the code and\\nwhile writing the code I go back to the\\nformulas in which we can find the\\nassociation of what we are doing and the\\nformula in the paper I think this is the\\nbest uh way so let's proceed further all\\nright guys now we can finally code the\\nbackward pass before we code the\\nbackward pass let's look at the\\nalgorithm of the backward pass as\\nwritten in the paper this is the paper\\nflashh attention one and I will because\\nwe will follow the structure of the code\\nthat is present on the Tron website so\\nit's not my idea to split it like this\\nbut I simplified it in s i simplified it\\nso it's different than the one that you\\ncan find online because mine is a\\nsimplified version and mine works with\\ncausal and non-causal\\nattention um so first if you look at\\nthis algorithm you need to you can see\\nthat we have an utor Loop through all\\nthe K and V blocks and an inner loop\\nthrough all the query\\nblocks however as you can see to compute\\nthe DQ which is the downstream gradient\\nof the the loss with respect to the Q uh\\nMatrix we need to have an iteration\\nthrough all the ks and to compute each\\nDC block we need to have an iteration\\nthrough all the cues so if we follow the\\nloop like it is it would involve writing\\nto the high bandwidth memory so to the\\ndram of the GPU at every inner iteration\\nand that could be also that that is not\\nso\\nefficient um and also if we don't want\\nto write it would require some sort of\\ninter some sort of um synchronization\\nbetween blocks which is also not very\\nefficient so we split we will split this\\nfour into two parts because we can see\\nthat each DQ depends on a loop over the\\ncase and each d DK depends on a loop\\nover all the cues so to compute DK we\\nwill fix the Kate block and iterate\\nthrough all the Q blocks then we will do\\nanother iteration in which we fix the Q\\nblock and iterate through all the KV\\nblocks to compute the Q this is what we\\nare going to follow and this is an\", mimetype='text/plain', start_char_idx=303972, end_char_idx=308147, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='e248a24a-3e43-46a3-8a28-2d321dbe2a70', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9ff87ad7-af5b-4e09-a07f-3d6a50d0b370', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='a149569f0bb5f0b55b3f7786ab62d73d67be11eda703a58c64914324e9a9b158')}, text=\"idea\\nthat I took from the original\\nimplementation that is present on Tron\\nwebsite another thing that we can notice\\nhere is um where where is it here to\\ncomput DQ and DK so um a DQ vector and a\\nDK Vector we need this element this\\ninformation here called di di and it's\\nshared between the two so we can\\nprecompute it and then we can reuse it\\nfor the qite vector to compute the qite\\nvector and the DK Vector what is this di\\ndi is um is uh introduced here and it's\\nthe dot product of a um Vector that is\\nthe DOI Vector multiply by o Vector so\\nthe first thing that we will do is do a\\nloop over all the vectors in O and d o\\nand do their dot products to compute\\nthis di element then we will use this di\\nelement and actually uh let me see yeah\\nand then we will use this di element to\\nupdate to to compute DQ and DK and we\\nwill also have another two Loops one in\\nwhich we fix Q and we iterate through\\nall the keys and one in we fix the keys\\nand iterate through all the cues so\\nlet's start so now that we know more or\\nless the structure of the code that we\\nare all right so um we start by writing\\nthis backward\\nfunction\\nhere uh let me check yeah okay so do you\\nremember this is saved tensor these are\\nall the information that we save during\\nthe forward pass uh to compute the\\nbackward pass now to to optimize the\\nmemory utilization in um flash attention\\nwe don't save the query multiplied by\\nthe transpose of the key Matrix because\\nthat would be a sequence by sequence\\nMatrix that is too big to save into the\\nhbm in the dam during the forward pass\\nand then re reget it back from the hbm\\ninto the local memory because I want to\\nremind you that in Tron uh compared to\\nCuda in Tron what we do is we load stuff\\nfrom the high band withd memory in the\\nshared memory so the SRAM we do all the\\noperations there and then after when we\\ncall the store method we save the\\nelement from the shared memory into the\\nhigh band WID memory so in order to not\\nmaterialize this s Matrix in its\\nentirety save it to the hbm and then\\nReet it back which could be very slow\\nand secondly actually it is very\\nexpensive because usually right now we\\nare Computing attention on thousands and\\nthousands of tokens so imagine saving a\\nmatrix that is 5,000 by 5,000 that's a\\nbig Matrix to save for each batch uh for\\neach batch and for each head so that\\nwould be really too expensive to\\nsave so the idea in Flash attention is\\nto recompute what we can compute on the\\nfly during the backward pass because\\nanyway if we were to load it it would be\\nMemory IO found so it's faster to\\nrecompute than to save it and restore it\\nfrom the memory this is the idea of\\nflashh\\nattention Okay so we saved some stuff\\nduring the forward pass and now we can\\naccess it back during the backward pass\\nand this stuff is saved in the context\\nand this it's it's a kind of a\\ndictionary that is made available by by\\nP\\ntorture all right so we get back the\\nquery key and values and as you know P\\ntorch during the autograph will just\\ngive us the gradient of the loss with\\nrespect to the output of our\\nimplementation of the attention of our\\nattention so this is Tron attention and\\nthen we need to compute DQ DK and DV by\\nusing only the gradient of the output\\nwith respect to the the deloss with\\nrespect to the output um we do for some\\nchecks so here I know I could optimize\\nthis code and make it even smaller by\\nfor example checking that here the\\nstride that I am using I actually inside\\nof the code I always uh pretend that the\\nstride is the same but uh doesn't matter\\nI just take the code from Tron and uh\\ntry to simplify it my goal was to\\nsimplify it not optimize it\\nso all right we create the um the\\nvectors the tensors in which we will\\nstore the result of this backward pass\\nwhich is the DQ DK and DV and as you\\nknow from what we have seen of the\\ndefinition of the gradient the size of\\nthe output of the gradient Vector is the\\nsize of the uh Vector with respect to\\nwhich we calculate the gradient because\\nin the numerator is always a scalar and\\nwe compute the gradient with respect to\\nall the elements in the input Vector so\\nthe output the gradient itself is a\\nvector of the same size of the element\\nby which we compute the gradient\", mimetype='text/plain', start_char_idx=308148, end_char_idx=312308, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='a0e7030c-06b8-4879-bdd4-d327bb8a292b', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e248a24a-3e43-46a3-8a28-2d321dbe2a70', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='d164639289d6360818e430b9826d3a80a367652e703cb054d2e9b669dafdc8af')}, text=\"with\\nrespect to so uh we got some information\\non the bed size blah blah blah and later\\nwe will see what is this number of warps\\nand number number of stages I will not\\nexplain it now it's how P torch number\\nof Parts warps is an indication on how\\nmany threads we want to launch in our\\ngrid and number of stages is actually\\nthe number of stages that is used in\\nsoftware pipelining we will see later\\nwhat is software pipelining when we talk\\nabout the autot\\ntuning then we Define some uh\\nblocks uh in the original um in the\\noriginal code I think they call it a\\nblock kv1 kv2 k q1 1 and Q2 I think it\\nwas confusing I call it block macro and\\nblock micro because the things that we\\nwill fix and the things that we will\\niterate from will be once is the query\\nso we fix the query block and we iterate\\nthrough all the keys and then we will\\nfix the keys and values block and we\\niterate through the queries the one that\\nwe iterate on is the micro one and the\\none that we fix is the macro one this is\\nmy uh the naming that I am\\nusing um then we as I said before we\\nneed to precompute the DI elements that\\nwe saw in the paper before so that's the\\nfirst kernel that we are going to launch\\nand this kernel will have its own launch\\ngrid because later we want to optimize\\nthe uh the tuning of this carel later we\\nwill talk about tuning with respect to\\nits own parameters so uh let me see what\\nare we going to do so here so the first\\nkernel that we are going to launch is\\nthis pre-process kernel\\nthis preprocess can will pre-compute all\\nthe DI elements that we need to compute\\nI remember DK and DV if I no DQ and\\nDK and this di element depends only on o\\nand d o um so let's do it and uh let's\\ncreate another function uh called\\nbackward pre-process what is the process\\npre-process grid this is the launch grid\\nof this function of this car\\nand this will be launched on um\\nindependently for each batch and for\\neach head and moreover it will be work\\nwith a block size of vectors of O what\\nis this block what is this number of\\nvectors of O it will be the block size\\nmacro so on 128 vectors of O so uh let\\nme copy the signat of this function this\\nis here so let's write it here it's fine\\nyeah okay this function takes uh The\\nMatrix o so it's a pointer to The Matrix\\no it's a pointer to the d o and it's a\\npointer to The Matrix D where we will\\nstore this di elements and we have one\\nfor each Vector in the\\noutput uh that's why the shape of this D\\nis batch size number heads sequence\\nlength it means it's one for each of the\\noutput element in the output of the\\nattention this di uh where is it\\nactually it's not this one it's this one\\nyeah like M so it has the same shape as\\nM which is as you can see it is this\\nsize here so bch size number heads and\\nsequence length M if you remember is the\\nMatrix that we Sav during the forward\\npass which includes the normalization\\nfactor of the soft Max and also the\\nmaximum element but in log some exp\\nformat so that when we apply it will\\nautomatically apply the maximum element\\nfor each row and also normalize that at\\nthe same time which I think I proved\\npreviously uh\\nso let me do it so we write it like this\\nso we extract\\nthe uh the index of this program so this\\nprogram has two uh index uh like\\nidentifier this is is equivalent to the\\nCuda identifier and this is along the\\naxis zero so let's see what we uh what\\nwe what did we launch on the axis zero\\nso on the axis zero of this launch grid\\nwe defined what is the block of vectors\\nof uh the O that this particular will\\nprogram will work with and the second\\naxis is which batch and which head\\ninside of each batch this particular\\nprogram will work with so this\\nidentifies the block index of Q so which\\ngroup of vectors in the O Matrix this\\nparticular program will work with here\\nis called Q I believe because I copied\\nit from the original code where they\\ncall it q but I could have eventually\\nalso called it\\no um so we Define uh so basically this\\nmeans that we uh for this program we\\nneed to skip some query vectors that\\nhave been already or that will be or\\nhave been already processed by other\\nprograms in parallel so we will only\\nblock with a number of query vectors\\ninside of O that have the\", mimetype='text/plain', start_char_idx=312309, end_char_idx=316466, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='89e4ae51-18fe-418d-bec8-d99353152570', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a0e7030c-06b8-4879-bdd4-d327bb8a292b', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='ccbe0dc3254622ce97fe66d721876b02a166555ed9e4b4cdcc6494749c475315')}, text=\"following\\nindices so so imagine that query block\\nsize is I think it's 128 the way we have\\ndefined it but suppose it's four for\\nSimplicity so this one will be and the\\nquery vectors are how many are sequence\\nlength number of query vectors we have\\nso some of Imagine The query vectors are\\nin total they are I don't know let's say\\nuh 64 and 32 will be managed by other\\nprograms so this particular off skq will\\nbe equal to 33 34 35 and 36 this tells\\nme which query vectors or which vectors\\nin the output o Matrix among all the\\nvectors in the O Matrix this particular\\nprogram is going to work\\nwith okay so then we extract also the\\nindex of the batch which tells us which\\nbatch and which head in each batch this\\nparticular program is going to work with\\nwhich is the dimension one of our launch\\ngrid\\nand then we Define the offset of the\\ndimension because we need to load all\\nthe dimensions of each Vector so these\\nare the uh it's a vector uh that tells\\num which Dimensions we need to load from\\neach vector and we will load all of them\\nso we don't divide on the head Dimension\\nuh Dimension we just divide on the\\nsequence length\\nDimension the the load among multiple\\nprograms um you will see in this part of\\nthe the video so when we are writing the\\nbackward pass that we will not be using\\nthe make block pointer like we did\\nduring the forward pass so this function\\nhere we will work with directly with\\nindexing by using the\\nstrides so let's do\\nit so let's load a single block of rows\\nof O which I want to remind you has the\\nsame shape as q and that's why we can\\ncall it block size Q um so the O Block\\nthat we are loading is O so uh the load\\nfunction accepts a pointer to what it\\nshould load uh actually not a pointer it\\naccepts a array of pointers or a\\nmulti-dimensional array of pointer in\\ncase you want to load a multidimensional\\ndata so actually load also allows you to\\nload um two dimensional data in this\\ncase we are going to load two\\ndimensional data which is a block of\\nrows of O which should be a block a\\ntensor of the shape uh block size Q in\\nthis case multiplied by the other\\ndimension being head\\nDimension but we don't we need to tell\\nit where in this o Matrix it needs to\\nfind this one first of all we need to\\nskip some batches and some heads based\\non what the head and the batch that will\\nbe processed by other programs So based\\non the index that this um program will\\nprocess of the batch and the head we\\nneed to skip all the other um batches\\nand heads\\nuh let's write the shape of this tensor\\nso the O tensor has a shape block size\\nuh not block size bch\\nsize number of\\nheads then sequence length and then head\\nDimension each block and each head will\\nhave sequence length multiplied by dim\\nhead dim number of items So based on our\\nindex we skip how many items the our\\nindex multiplied by head Dimension\\nmultiplied by sequence length so what I\\nmean is this the batch zero and the head\\nzero will have sequence length\\nmultiplied by head Dimension items the\\nbatch zero and the head one will also\\nhave the same number of items and the\\nbatch zero and head two will also have\\nthe same number of items so how many\\nitems sequence length multiplied by head\\nDimension do we need to skip from the\\nstarting of the O tensor it is equal to\\nthe index of the current batch and head\\nindicator so because this index\\nindicates both um the head in the batch\\nand the head inside of each bat because\\nit's already the product of the head and\\nthe batch so how many we skip indicated\\nby the this index and after we point to\\nthis starting point of the current batch\\nand the current head we need to select a\\ntwo-dimensional tensor where the offsets\\nare indicated for the rows by off SK and\\nthat's that's why we have this one um\\nthe I don't know what this is called\\nthis is the the index U semicolon index\\nthat tells all the all these vectors in\\noffc will with an additional dimension\\nfor the columns and this columns will be\\nthe offs dim so basically this will\\nselect a tensor of the following shape\\ninside of this big tensor that includes\\npet\\nsize and number number of\\nheads this is what we are doing so we\\nare saying select a tensor of this size\\ninside of one that is made up of four\\ndimensions by skipping the elements of\\nall the batch and heads that will be\\nprocessed by other programs I always\\ntalk in terms of programs because\", mimetype='text/plain', start_char_idx=316467, end_char_idx=320773, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='7b12af00-391c-4d92-a174-32f22d22446e', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='89e4ae51-18fe-418d-bec8-d99353152570', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='ca897a9915c2a90569478ebc85ac73a77f2008ad8878f37d41abc1bf803dec15')}, text=\"in\\nTron these are called programs in Cuda\\nyou would refer to them as\\nkernels all right so this one is done I\\nhope it is this recently clear um all\\nright so then we also load a single\\nblock of\\nD in the same way because we are going\\nto load a a group of vectors from all\\nthe sequence length also from D and the\\nD has the same shape as o which has the\\nsame shape as q and that's why we can\\nuse the um the the block index we call\\nit Q because it's equivalent because\\nthey have the same\\nshape okay and how to compute this di\\nelement well it's written in the paper\\nso if we go in the in the what is it man\\nif we go here it shows you how to\\ncompute the DI of each given a block of\\nD and um a block of O it tells you how\\nto compute di which is the row sum which\\nmeans the the sum of by rows for each\\nrow we will have one sum for each Vector\\nin the O Matrix we will have one sum of\\nthe element wise product so this stuff\\nhere is the element wise product of D oi\\nmultiplied by oi so it's not um uh\\nmatrix multiplication it's element wise\\nproduct which means each element of one\\nMatrix with the corresponding element of\\nthe second Matrix and the output shape\\nit will be the same as the two matric\\nwhich must have the same\\nshape okay so we compute this di\\nblock which will have shape block size Q\\nbecause we will have one sum for each\\nVector uh then well we need to store it\\nsomewhere so we need to calculate where\\nto store it inside of the D Matrix uh\\nwell the D Matrix is I remember\\ncorrectly has the same shape as M so it\\nshould be batch\\nsize uh number of heads and sequence\\nlength so we need to select the right\\nbatch and the right head and also the\\nright position inside of the sequence\\nlength based on the block index Cube\\nthat we\\nhave uh okay so let me\\nindex\\n[Music]\\nokay all right because we already um so\\nwe skip um again just like before we\\nknow that D is of this size each botch\\nand each head will have sequence length\\nnumber of elements so how many number of\\nelements we need to skip from the\\nstarting of the uh tensor is sequence\\nlength multiplied by the combined index\\nB size head\\nnumber uh and plus we need to also skip\\nsome queries based on our block index q\\nand it's this skipping is already done\\ninside of off skq so we add off skew and\\nthen once we have computed the index\\nwhere we should store this di I block\\nwhy did I even call it d block let's\\nstore it so let\\nme I didn't call it block I think it was\\nalready in the original code but this is\\ndi I and this big Matrix D is actually\\nthe Matrix that includes all the DI for\\none for each token in the sequence\\nlength all right so the pre-processing\\nhas been done now we need to do prepare\\nthe two for Loops as you remember I said\\nbefore we will be doing two for Loops\\none in which we fix the query and we\\niterate through all the keys and values\\nand one in which we fix the key and\\nvalue block and we iterate through all\\nthe queries and while coding it I will\\nalways show you the formula from the\\npaper so don't worry let's start with\\nthe next iteration so first we create\\nthe launch grid for the next iteration\\num as the launch grid is always the same\\nso we first because we we need to keep\\none block fixed and iterate through all\\nthe other blocks uh the block that we\\nkeep fixed we Define how many programs\\nwe have that run in parallel uh and the\\nblock that is fixed has a block size\\nmacro number of elements that's why we\\ncreate a sequence length divide by block\\nsize macro number of blocks uh thread\\nblocks uh or programs in this AIS uh the\\nAIS to in this grid is I could have used\\nalso the XIs one IND differently I think\\nit was already done here in the original\\ncode it's um will indicate which batch\\nand which head inside of each batch we\\nare going to work\\nwith uh so so and just like the uh\\nforward pass we will also use a variable\\ncalled stage that if the attention that\\nwe are Computing is caal it will be\\nequal to three and if we are Computing a\\nnon-causal attention then it will be\\nequal to\\none um the first iteration we will fix K\\nand V blocks and we will iterate through\\nall the Q blocks in size of block size\\nmicro number of query\\nvectors uh so let's look at the\\nsignature so we pass we we\", mimetype='text/plain', start_char_idx=320774, end_char_idx=324934, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='83db43dc-56d4-4396-b6f1-773c70504fc4', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='7b12af00-391c-4d92-a174-32f22d22446e', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='cdce717f5276dd1e1e33b37b18bf1185a08847ec14afd3a0172c066fbf35657b')}, text=\"launch it as\\na launch grid because um and we we have\\ndefined how many programs we have so we\\nhave how many uh KV blocks we will have\\nit's a sequence land divide by the block\\nsize macro because that's the the block\\nthat we will keep fixed in this uh for\\nLoop in this\\nfunction and then we go through all the\\nquery blocks in size of block size micro\\nwhich I defined it as 32 and later we\\nwill talk about autot tuning and how to\\ntune these\\nvalues all right so I passed the query\\nVector the key vector and the V Vector\\nuh sorry not Vector tensors now the\\nquery tensor K tensor and V tensor and\\nthey are pointing to the beginning of\\nthe tensor which means that they are\\nbeginning to the first botch and the\\nfirst head and the first token and the\\nfirst dimension of the tensors then we\\npass the soft Max scale we pass d o DQ\\nDK and DV m is the one that is needed to\\ncompute as you remember from what we\\nsaid before we did not see if the P\\nMatrix in the hbm because we want to\\nrecompute it on the fly during the\\nbackward pass so the query multiply by\\ntranspose of the keys it's a very big\\nMatrix to save in the hbm and restore it\\nso we want to compute it on the fly but\\nwe don't need to recompute the\\nnormalization factor and the maximum\\nelement for each row to apply the soft\\nMax that was already computed during the\\nforward pass and saved into this Matrix\\nM which includes the log sum exp of the\\nmaximum of each row plus the logarithm\\nof the normalization factor with the\\ntheug sum X trick we can just apply it\\nand it will also normalize each value\\nthen we have the d uh V uh tensor that\\nwe computed here with all the DI values\\none for each Vector in the O tensor then\\nwe need to pass some uh the number of\\nheads the sequence length the block size\\nthat we want to use for the KV which is\\nthe macro block size and the micro block\\nsize is always the one that we iterate\\non I think using this name it should be\\neasier to understand which one we are\\niterating and which we want keep fixed\\nso the fixed one is macro and the\\niterating one is the micro um head\\nDimension um and later we will see why\\nwe use a different block size to iterate\\nfrom because this is related to the\\nnumber of stages that Tron can divide\\nyour for Loop into thanks to soft\\npipelining then we have head Dimension\\nthe stage indicates if the attention\\nthat we computed in the forward pass was\\nCal or not\\ncal um the number of warps and the\\nnumber of stages which we defined as\\nfixed but later we will talk about Auto\\ntuning so uh sometimes I repeat the same\\nstuff over and over so I should change\\nthat\\num okay let's write the signature of\\nthis function um\\nlet's put it here but\\nso we already described what is the\\nsignature of this function let's go\\ndirectly to the meat so the first thing\\nthat we need to do is understand the\\noffset by which we need to move this\\nquery key and value and the offset is\\ngiven by the first of all we need to\\nenter the right batch and the right head\\ninside of each batch we compute the\\nindex of the batch just like during the\\nforward pass by dividing the program the\\nprogram index which is a multiplic of\\nthe index of the head and of the the\\nbatch we divide it by the number of\\nheads to get which batch this program is\\nworking with and to get the head we just\\ndo the modulus just like in the for Loop\\nuh forward\\npass um the offset batch head indicates\\nlet me check what is it for okay it\\nenters the right batch and the right\\nhead so what is the stride if you\\nremember correctly The Stride tells us\\nhow many items you need to skip in that\\nDimension to arrive to the next index in\\nthe same dimension so if we want to skip\\nindex number of batch we need multiply\\nit by the stride batch which is how many\\nelements you need to skip to arrive to\\nthe next batch plus we also need to\\nenter the right head so we multiply um\\nthe index of the head multiplied by The\\nStride of the head to enter exactly in\\nthat head in the tensor for each of the\\nqk and V\\nmatrices uh plus we also have this is\\nwill be used for if I remember for m and\\nd because m and d only don't have the um\\nthe head di head Dimension so they are\\nonly bch size number of heads sequence\\nlength so we just use the index batch\\nmultiplied by sequence length because\\nfor each\", mimetype='text/plain', start_char_idx=324935, end_char_idx=329148, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='c4bd8a9a-0aad-4374-bb91-268962f01efb', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='83db43dc-56d4-4396-b6f1-773c70504fc4', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='00b258f5b64f642c910604b00b0197d4528ce6417b9221dad667b4c68d9f69e8')}, text=\"batch and on each head we will\\nhave sequence length number of item so\\nyou can think of it at the stride to\\nmove from one batch head to the next\\nbatch\\nhead uh or to the yeah so\\nuh let's move the pointers\\nand this was so we move the pointer q k\\nand V by the offset batch head because\\nwe want to enter the right um batch and\\nthe right head inside of these big\\ntensors and we do it also for d o DQ DK\\nand DV because they have the same shape\\nas a q k and V and d o also has the same\\nshape as Q so they have the same shape\\nso we move by the same uh by the same\\noffset all right\\nso and then we move m and d to move them\\nto the right starting point on which the\\nsequence of the current head and the\\ncurrent batch and the current head\\nstarts so they are pointing to the first\\nVector of the sequence dedicated to the\\ncurrent badge and the current\\nhead and the same is true for qk and V\\nand the d o DQ DK\\nandb okay then we load some other stuff\\nbecause here we fix in this iteration in\\nthis method we are going to do a for\\nLoop in which we fix KV and we iterate\\nthrough Q so we first need to load this\\ndibs block of KV and we do it as\\nfollows as follows so we know we need to\\nload a 2d tensor so we need to Define\\nwhat are the ranges in the second\\ndimension of each um vector k and V that\\nwe need to load and it's defined by the\\nthis\\num by this\\nFactor then we uh we want to understand\\nwhich KV block this particular program\\nis going to work with so this particular\\nprogram is going to skip some KVs that\\nwill already be managed by other\\nprograms that may be running in parallel\\nand how to understand what this program\\nshould be working with in based on the\\nindex of the program zero which is\\ndefined on sequence divide by the block\\nsize macro and if you remember block\\nsize macro is the thing that we fix so\\nit's telling us this program ID zero\\nwill tell us how many uh block size\\nmacro KV are already being managed by\\nother programs so we shouldn't care\\nabout them so we skip them so let's go\\nback here and this is the number of\\nvectors that we need to skip so our KB\\nStart From Start KB and how many we need\\nto load them well depends on what is the\\nblock KV this block KV is equal to block\\nsize macro so it will be\\n128\\nvectors so we Define our um tensors two\\ndimensional tensors that we will store\\nin the SRAM because in Tron every time\\nyou load something you load it from the\\nhbm into the SRAM so we Define where\\nthey should be saved in the SRAM and\\nthey are initially zeros and now we load\\nthem so we load them as as\\nfollows um we say that okay in the K uh\\nin the K tensor pointer which is already\\npointing to the right index to the right\\nbatch and to the right head because\\nthat's something that we did\\nhere we say we should need we need to\\nload the right sequence of keys which\\nshould start from offs key because this\\nalready includes how many We Should Skip\\nin the sequence length Dimension and for\\neach of these Vector we need to load all\\nthe dimensions in\\nthe in the head Dimension Dimension uh\\nbecause the K if I want to remind you is\\nbatch number of\\nheads um sequence length and head\\ndim Now by using this line we are\\nskipping to the right B and to the right\\nhead so it's like we already indexed\\nhere and here we already selected an\\nindex so right now this K is pointing to\\nthe beginning of a tensor of two\\nDimension and we tell okay we don't want\\nall the sequence we want some part of\\nthis sequence which part the one that is\\nindicated by this start\\nKV um and how many of in the sequence\\nlength we want well we want uh I I think\\nit's easy to write it like this so we\\ncan write it that from start KV to start\\nKV plus block\\nKV uh so we want this number of tensor\\nexactly at this location and for head\\nDimension what do we want to select we\\nwant to select all the dimension so we\\nsay that we want from zero to head\\nDimension which is exactly this offs\\ndim\\nokay uh we do it for the K block and we\\ndo it for the V block\\nuh here I think I didn't change the\\ncomment this should be block\\nKV and this should be block KV before it\\nwas called block kv1 right like in the\\noriginal code uh I simplified a\", mimetype='text/plain', start_char_idx=329149, end_char_idx=333241, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='c99d5e91-7d09-43d3-b7b5-3fe205a3c349', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c4bd8a9a-0aad-4374-bb91-268962f01efb', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='25c1597a8cbe0a6eb1aafc7de73b2e1ef844c3cf1583e2d3a715b4bba5f6e266')}, text=\"little\\nbit the naming I think this one is\\nbetter easier to follow because in the\\noriginal code they also do for two for\\nLoops but in the second for Loop they do\\nit backward just to not change the\\nstructure of the loops but I think mine\\nis more verbos but easier to\\nunderstand and probably less efficient\\nmine is much less\\nefficient um then we have offc because\\nwe need to understand for each block of\\nqueries how many vectors we need to load\\nand it's indicated by this offs q and\\nhow many are them it's a block Q block Q\\nin the col color of this method was\\nblock size micro so it is 32\\nvectors okay um now we need to access\\nQ vectors and O vectors trans uh no Q\\nvectors but already transposed and the O\\nvectors also we need to access them\\nbecause we are going to iterate through\\nqueries and O vectors actually also why\\nbecause let's look at here let's look at\\nthe formulas in the paper to compute VJ\\nso to compute the dvj that's what we are\\ntrying to compute here we need to\\niterate through all the D vectors and to\\ncompute DK we need to iterate through\\nall the QI uh\\nvectors because the QI is a block of\\nvectors uh so uh that's why we need um\\nand why do we need to access a q as a\\ntransposed because we need to compute\\nlet me show you here uh P transpose that\\nto compute P transpose we need to we\\nneed the Q trans transpose because the p\\nwould be the soft Max of the query\\nmultipli by the transpose of the\\nkeys after we apply the soft Max it\\nbecomes P but if you want the transposed\\nof P then you need to do query\\ntransposed K multip by query transposed\\nso that's why we accessed query\\ntransposed instead of\\nqueries and the way we access query\\ntransposed is just by playing with the\\nstride so let's do it like this and I\\nhave also written the comment on why we\\ncan do\\nit so this is equivalent to accessing\\nthe\\nquery uh how many first okay what is\\nthis um what is this operation uh what\\nis this operation here this is saying go\\nto the query starting point starting um\\npointer to the query which is already\\npointing to the right batch and to the\\nright head for which this particular\\nprogram should work with and select a\\ntwo-dimensional Vector where you repeat\\nthe query starting point along the uh in\\nthis case along the columns but we\\nshould be repeating it along the rows\\nbecause we want to select rows of\\nqueries however if we want to select uh\\nthe query transpose we just invert the\\ntwo Dimensions so this is a let me\\nactually show you without doing the\\nquery transpose so let's do it\\nsimplified like this so to access the\\nquery um\\nthe query pointers without transposition\\nwe can just do like this go to the query\\ntensor and create a 2d tensor where in\\nthe rows you put the starting point of\\neach query that you want to get and um\\nand replicate each of this pointer also\\non the column that's the meaning of\\nadding this Dimension none this is\\nequivalent to when you do in py torch\\nthe UNS squeeze like you are call\\noffs Q multiplied not un squeeze I think\\none so this is equivalent to adding the\\ncolumn Dimension to this tensor and\\nrepeating all the values that are on the\\non all the um on the columns how many\\ncolumns will be there it will be\\nbroadcasted when we sum it with the dist\\ntensor here um this is a combination of\\nUNS squeezing and broadcasting so we are\\ntaking the query vectors indicated by\\noffs\\ncuq um and um then we are uh for each\\nquery Vector we are selecting all the\\nhead Dimensions indicated by dim if you\\ninvert this broadcasting it will create\\nthe transpose of the the the query\\nVector that you're trying to access so\\nthis stuff here is equivalent to the\\nthese two lines so accessing query and\\nthen transposing\\nand uh it's something that you can do U\\nI could write down what is happening at\\nthe pointer level so basically you need\\nto think of offc as being a vector of\\npointers uh we multiplied by the\\nsequence stride which tells us how many\\nelement we need to skip to go from one\\nquery Vector to the next because each\\nstride Q will be the stride will IND\\nwill be equal to in the case the head\\nDimension is 128 The Stride of the\\nsequence Dimension will be 128 it means\\nthat to go from one query Vector to the\\nnext you need to um you need to uh uh go\\nforward by 128 elements because I want\\nto remind you that in the memory\", mimetype='text/plain', start_char_idx=333242, end_char_idx=337487, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='8cb345c6-c85f-44ce-8e4c-a901aef84bf1', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c99d5e91-7d09-43d3-b7b5-3fe205a3c349', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='e7ba3ba087325176775c51193dfd8cb1301d27d80a67cdfe56a49dd39d097561')}, text=\"the\\ntensors are always stored like flattened\\nlike each Dimension is flattened with\\nthe next Dimension so imagine you have\\nthree rows and four columns but the\\nfirst you will have the first three rows\\nthen the sorry the first row so the\\nfirst four columns then the next four\\ncolumns then the next four columns row\\nafter\\nrow U it's difficult to visualize until\\nyou write it down so uh how to write it\\ndown take um create a vector of offs\\nskew so what is offc at the beginning is\\nis a range that is from here from 0 to\\n100 no 0 to 32 0 1 2 3 4 five 6 7 etc\\netc we are multiplying each one with the\\nstride of the sequence so this will not\\nskip any element this will skip exactly\\n128 elements this will skip exactly\\nimplying that the head Dimension is\\n128 uh this will skip two times 128\\nelement this will skip three times 128\\nelements and then we are adding also\\nanother Di di menion to this Vector so\\nthis will be a vector then you broadcast\\nit on head Dimension number of columns\\nand to each of them you add one number\\nso it will become a vector like fall\\nokay let me just do it guys otherwise I\\nthink it's too\\nconfusing okay so we have a vector that\\nis as follows so zero then we have 128\\nthen we have two * 128 then we have 3 *\\n128 etc etc we are adding how many\\ncolumns indicated by off dim so off dim\\nhas how many columns it has a head dim\\nnumber of columns please for Simplicity\\nlet's pretend it's not\\n128 Dimensions let's pretend it's four\\ndimensions so this will be four this\\nwill be 2 * 4 this will be three * 4 we\\nare adding another dimension that is the\\ndim\\nDimension each one multiplied by The\\nStride of dim which will be one because\\nit's the last\\nDimension um stride de so we are adding\\nhow many columns\\nfour uh so we are adding um one 0 1 2 3\\nI guess 0 1 2 3 right also to here this\\none we are adding oh oh my God 0 1 2 3\\nand also to this one we are\\nadding 0 1 2\\n3 uh okay and then also to this one we\\nare adding 0 1 2 3 so what this will\\nselect this will select from the\\nstarting point of the pointer Q it will\\nselect the element zero then the element\\none then the element two and then the\\nelement three which is exactly the head\\ndimension of the first Vector that we\\nshould be selecting then it will\\nselect uh the element four from the\\nstarting point of the vector the element\\nuh sorry this one let me write the\\nresult of this operation so this one\\nwill be 0 1 2 three then it will select\\nthe element four 5 6 7 then it will\\nselect the element um eight I guess 9 10\\n11 and then it will select the element\\n12 13 14\\n15 so from the starting point of where\\nthis Q is pointing it will select the\\nfirst element right after this que the\\nsecond element right after this que the\\nthird element right after this Q etc etc\\nand this will be the f you can see that\\nthis will be the first query Vector this\\nwill be the second query Vector this\\nwill be the third query Vector this is\\nthe fourth query Vector because in the\\nmemory they are stored one after another\\nthey are flattened so in the memory they\\nare stored like this they are stored\\nlike the\\nfollowing they are stored like this one\\nafter another so it will select all of\\nthem and we also create a virtual tensor\\nwith the right shape that we want to\\nvisualize it into so as you saw as we\\nsaw before when you work with a tensor\\nlayout in memory you can always view it\\nas whatever shape you like based on the\\nshape that you want and the reshaping is\\nalways free doesn't involve changing the\\narrangement of the elements in the\\nmemory I hope now it's more clear so now\\nwe can proceed\\nfurther oh my God it was quite\\ncomplicated so whenever I get stuck I\\njust draw things and I think you should\\ndo it too because that's the only way to\\nlearn uh if you try to imagine\\neverything in your head it's always\\ndifficult\\nand we do the same job for the O vectors\\nin the O vectors we don't access it as\\naccess it as um transposed because we\\ndon't need it in transposed only the Q\\nwe need it\\ntransposed okay it race through the\\nsequence dimension of the query\", mimetype='text/plain', start_char_idx=337488, end_char_idx=341484, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='2ae6b312-ccd4-46c0-b3b7-faf84c7f581a', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8cb345c6-c85f-44ce-8e4c-a901aef84bf1', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='ef696f9d1e60331ea4ac7f7397ead2667c5bdb633de4b122590e68f1297fc611')}, text=\"so we\\nstart from the query number\\nzero in the current um well in the query\\nwe need to go through the all the\\nsequence length Dimension because only\\nthe key we select the right key that we\\nwant to work with so I want to remind\\nyou here we fix the key and we go\\nthrough all the\\nqueries but the query we need to start\\nfrom zero until sequence length so the\\nnumber of steps of this for Loop will\\nbe uh sequence length divide by block Q\\nuh so if we have 1,000 elements in the\\nsequence and the block Q is the 32 it\\nwill be 1,00 divide by 32 I'm bad choice\\nof 1,000 should be 1,24 otherwise it's\\nnot divisible so then we go through each\\nblock in this for Loop and we load a\\nblock of Q the first one indicated by\\nour pointer and at the end of the\\niteration we will move it to the next um\\nto the next block of\\nQ okay we'll add also the log log Su exp\\nvalues that are stored in the M Matrix\\nuh because we want to compute on the Fly\\nPT PT is the transposed of the soft Max\\nof query multiplied by the keys but we\\nwant to not take quy multiply by the\\ntranspose of the key and then do the\\ntranspose we just already access um Q as\\ntransposed so we can already compute a\\nPT instead of computing p and then\\ntransposing\\nit um so we load the offsets of the\\nelements that we need from this log some\\nexp\\nx uh Matrix which is the m Matrix that\\nwe computed during the forward pass and\\nwe access a block of Q at a time the one\\nwe are currently working with in the\\niteration then we access a\\nquery key transposed already so we do\\nthe if you want to get a PT um P should\\nbe um this is actually not P because we\\ndidn't do the soft Marx it's actually s\\nt but okay if you want to get PT you\\nneed to get the soft Max of\\nSTD um the soft Max of St is what it's a\\ntranspose of s what is s is a query\\nmultipied by transpose of ke so to get\\nSt you need to do um key transposed no\\nkey multiplied by query transposed so as\\nyou remember in the matrix\\nmultiplication if you transpose the\\nmatrix multiplication you need to also\\ninvert the two uh element in the matrix\\nmultiplication so that's why we are\\ndoing a key multiply by quy transposed\\nthis will give us s transposed we are\\nalso scaling it with the softmax\\nscale before we apply the to apply the\\nsoftmax we just need to do the\\nexponential of each element minus its\\nmaximum divide by the normalization\\nvalue but with the log sum XT we just\\nneed to um each element um subtracted by\\nthe M value which already includes the\\nnormalization factor uh I think I\\nalready did the derivation of this so we\\ndon't need to go through that again okay\\nso now we have the PT block actually so\\nthis this formula I should have written\\nSt\\nactually okay um then when doing the\\ncausal attention we also need to um mask\\nout some\\nvalues um so as you can see here so the\\nin this case the Cal mask is applied\\nafter the soft Max has been computed\\nbecause during this one is a you are\\nused to uh compute the apply the soft\\nthe Cal mask before Computing the soft\\nMax attention but this is actually\\nduring the forward pass because you\\ndon't want the normalization factor to\\nbe affected by the element that should\\nbe\\nzero uh but we already computed the\\nnormalization factor so it cannot be\\naffected anymore so we can compute we\\ncan mask out after applying the sofware\\nbecause the normalization factor has\\nalready been calculated based on the\\nfact that we applied the mask and that's\\nwhy we we we can apply it after applying\\nthe soft Max\\nuh so the mask is always the same so if\\nthe query is more than the the index of\\nthe query um so the mask is true in this\\ncase for all the values that do not need\\nto be masked so all the values that do\\nnot need to be masked are this ones here\\nuh and all the other value will be um\\nwill be replaced with the\\nzeros all right so after we have the PT\\nblock already masked we can calculate\\nthe DV DV I will write I will point to\\nthe right formula in the paper so we\\nload a block of the O why do we know to\\nload a block of do let's look at the\\npaper so how to compute the DV block so\\nthe DV block is computed\\nas the old DV plus so a repeated sum as\\nyou can see as you can see it's\", mimetype='text/plain', start_char_idx=341485, end_char_idx=345580, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='5e5196e8-fc09-49c7-9815-01580122c41e', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2ae6b312-ccd4-46c0-b3b7-faf84c7f581a', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='446f6c8ba70050eb3a7fc04fdcbdb714fa4e67dc8c190e9543d8617049352a20')}, text=\"here\\nplus\\nequal the old DV plus PT so here PT\\ndropped indicates the P IJ after\\napplying the Dropout in this\\nimplementation we don't support the\\nDropout and also very few models\\nactually use the Dropout in the\\nattention um so PT multiplied by DOI so\\na block of DOI and DOI is the same block\\nthat should be also um DOI and Ki Qi are\\nreferring to always the same block of\\nrows in the respective um t s that's why\\nbecause this inly iteration I indicates\\na block of q and a block of O but\\nthey're always referring to the same\\npositions in the tensors because d o has\\nthe same shape as DQ so we go through\\nthe blocks of query and d o\\nsimultaneously because one is needed for\\nDV so for D DV we need d o and for DK we\\nneed\\nq and that's why we compute the DV as\\nfollows just like from the paper so PT\\nblock multiplied by d o as you can see\\nit's a ppose multiplied by d o block so\\nwe have comput computed the do block um\\nthen we need to load the DI element that\\nwe computed precomputed\\ninitially uh the d uh with the first\\ncall to the function called the\\nattention backward Pro\\npre-process because we will need it for\\nDecay so let's see\\num and how many of them we are loading\\nexactly the same number of query that we\\nload because um they are we load always\\nthe same number of block size micro\\nnumber of\\nvectors okay I will copy some stuff and\\nexplain it step by step so um the next\\noperation that we need to do is to\\ncompute this DK to compute DK we need\\nDST to compute DST we need need to to\\nget\\nDPT so let's go one by one let's go from\\nthe B from the end to the beginning of\\nthis uh formulas so we don't uh we we we\\nwhere where everything is used to where\\neverything is created so let's start\\nfrom DK if you look at the paper DK is\\nequal to the old DK plus DS transposed\\nmultiply by a block of\\nQ um and this is is what is written here\\nso it is plus equal means basically just\\nthe old plus the new some um it's an\\nincremental addition so increment the\\nold K with some new stuff which is this\\nstuff here so the soft Max scale\\nmultiplied because also there is a soft\\nma scale this tow here multiplied by the\\nmatrix multiplication between the St\\nblock and the transposed of um and and\\nQ and Q you can see here this q but we\\ndon't have a Q we have a Q transpose so\\nwe take the transpose of Q transpose and\\nit becomes back Q now let's look at this\\nDST block DST is calculated as follows\\nso in the formula of the paper we have\\nDS d s is here it is yeah it is here and\\nit is equal to a block P J multiplied\\nelement wise with DPI minus Di now um we\\ndon't need the S we need the S\\ntransposed so to compute the S\\ntransposed this is an element wise\\nmultiplication not a matrix\\nmultiplication which means that when you\\ntake the transpose of this operation you\\ndon't need to invert anything you just\\nneed to take the transpose of the two\\noperan so to compute the St we take the\\ntranspose of P which is the PT and we\\nalready have that and then the transpose\\nof everything that is inside of the\\nparenthesis so this\\nDPT minus di I where we inverted the\\nrows with the columns so this DPT is\\nwhat well in the paper we know the\\nformula for DP DP is here and it is\\nequal to D wait wait wait DP here and it\\nis equal to d o multi by V transposed so\\nuh but we don't need the DP we need the\\nDPT and in this case it's not an element\\nwise multiplication it is a matrix\\nmultiplication so um in order to get not\\nDP DP but DPT we need to take the\\ntranspose of these two operants of this\\nmatrix multiplication and in the matrix\\nmultiplication when you take the\\ntranspose you need to also invert the\\norder of the two operants so we need to\\ntake the VT transposed which becomes V\\nso the V block Matrix multiplied by the\\nother operand so D oi transposed and\\nthat's why we are doing the transpose of\\nd o\\num right now I'm not going through all\\nthe single pointers because I already\\ntold you how to check what a pointer is\\npointing to and what an offsets is\\nreferring to I hope that now you have a\\nbetter understanding on how these\\npointers work in uh Triton which is also\\nthe\", mimetype='text/plain', start_char_idx=345581, end_char_idx=349612, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='e94e92dd-7134-4fbd-a87e-57fa53f4c35c', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='5e5196e8-fc09-49c7-9815-01580122c41e', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='ef711d3e279efeb68d258a0c6b6c806d6dcbedacbd8f04b99593bd6a181d8174')}, text=\"same way in the in which they work\\nin Cuda because um in the GPU we only\\nget a pointer to the starting point uh\\nto the starting address of the tensor\\nand then we need to work out all this\\nindices we have computed the DK block\\nblock so we now go to the next query uh\\nto the next block of\\nqueries and um so uh the next GL block\\nof queries because we are fixing K and V\\nblocks and we are iterating through all\\nthe queries so we need to move the query\\ntranspose\\npointers uh by stride sequence which\\nmeans that how can we go from one query\\nto the next and we multiply with the\\ncurrent um block Q which is a vector\\nwhich indicates the pointers to the\\ncurrent element in Q that we are\\naccessing and we do it also for the O\\nand we use the block Q as element and\\nthe stride Q because the O and Q all\\nhave the same\\nshape okay after we have run the for\\nLoop of all the queries we can store\\nthis DK and DV block so we write it back\\nas\\nfollows and this is the end of our\\nfunction guys so so we save the DV block\\nexactly in the position inside of the\\ncurrent okay DV is already I believe\\npointing to the right batch and to the\\nright head because we incremented it\\nhere and also in the case of Decay then\\nwe need to tell it in the sequence\\nDimension where they should save this um\\nblock of K and V and this is indicated\\nby this one we say and we create the the\\npointers just like before guys don't\\nmake me do it again it's a really really\\num easy if you write it down like you\\nwrite this uh um Vector of key and\\nvalues pointers which is not pointers\\nactually they are a range of the of key\\nand value that you need to take from the\\nsequence\\nDimension you add another dimension that\\nis the column so you repeat each value\\nin the columns and then you add the\\ndimension here for the head Dimension\\nanyway after we have Cal at the pointers\\nwhere we should store the DK and DV we\\nstore them in the um the pointers of um\\nwe store them in the DV um uh I mean we\\nstore them in the DV uh tensor and the\\nDK tensor what do we save we save the DV\\nblock and the DK block which is the one\\nthat we were um uh incrementally\\nchanging in the for Loop that we have\\nwritten okay now that we finish this one\\nwe can go to the next function that will\\ndo the other for Loop so let's do it\\nokay so now we do the second part of the\\niteration which is this one so let me\\njust copy it and then we we we describe\\nit uh let's write it here okay we use\\nthe same lunch grid as before of course\\nwe need to declare this function and\\nagain we um we because the grid is\\ndefined for the block size Macro for\\nwhat is the thing that we keep fixed and\\nthen we in the side of the for iteration\\nwe do um steps of block size micro in\\nthis case we are fixing q and we are\\niterating through K\\nandv uh because we need to compute DQ\\nright now we have computed DK and\\nDV okay uh the I believe the arguments\\nare the same as before so and actually\\nthis is also the reason why in the\\noriginal implementation on the Tron\\nwebsite the author decided to um to use\\nthe same for Loop but different\\narguments and uh I believe it was a\\nlittle confusing so that's why I just\\nseparated them I just repeat the code\\ntwice it's the goal of this video is to\\nbe as easy to understand as possible not\\nto be as efficient as possible\\nso uh let's go uh here so let me copy\\nthe signature again and we Define this\\nfunction here okay so uh again we need\\nto first move the query key and value uh\\nto the right pointers which will point\\nto the exact batch and the exact head\\nthat we are working with in this program\\nso um let's do it let me check where is\\nthe code here and the first part is\\nexactly the same as the other for Loop\\nthat we have\\nwritten so let's go here and really is I\\njust copied so it's exactly the same so\\nwe check what is the index batch head we\\nmove the query key value pointers to the\\nright place the d o DQ DK DV point to\\nthe right place the m and d to the right\\nplace exactly like before so I don't\\nthink I need to explain that\\nagain and then we load a block of Q the\\none that we will keep\\nfixed so\\nDQ let me load a lot of stuff here\\nactually\\nokay we Define the offset that\", mimetype='text/plain', start_char_idx=349613, end_char_idx=353718, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='264e7562-c73a-46a5-85d4-b6cbfae1052a', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e94e92dd-7134-4fbd-a87e-57fa53f4c35c', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='2dc0a4ecb60299cf3c4a4f37a27b8d5fb1669220a24fe15572ff4ef4b4561d07')}, text=\"we will\\nneed to load the blocks of k&p in the\\nhead Dimension because we are going to\\niterate in the k\\nandv um we will access them as\\ntransposed blocks so instead of\\naccessing them directly as K and v We\\naccess access them as KT and VT and you\\nknow that that's possible just by\\nchanging the\\nstrides uh in this case because we are\\ntreating them as 2D vectors we treat the\\noffs KV when you want to access K as\\njust not transposed but K you treat this\\noffs KV as a row Vector uh sorry a\\ncolumn Vector so you repeat on the rows\\neach k um offset that you want to access\\nin this case we are repeating it as a we\\nare treating it as a row Vector so it\\nwill be repeated on the\\nrows um sorry it will be broadcasted on\\nthe column Dimension and that's how you\\ncan access the transposed version of K\\nand how you can access the transposed\\nversion of V another thing that we are\\ndoing is we are loading the Q Vector\\nwhich Vector well based on ofq which is\\nbased on the start Q which is based on\\nthe exact starting point in which this\\nparticular program should be working\\nwith because this particular program\\nworks as two\\nDimensions uh the First Dimension\\nindicate which batch and which head this\\nprogram should be working with and the\\nsecond dimension which is the program\\nindex number zero indicates which among\\nall the sequence length which query this\\nparticular program is going to work with\\nthis is indicated by the index block um\\nthis should be actually Q in this case I\\nforgot to change the name so actually\\nlet me change it so it's index Q because\\nwe start we skip some Q how many Q we\\nskip um based on the index of the\\ncurrent program multiplied by how many\\nuh blocks have already been processed by\\nthe previous\\nprograms this this will tell us inside\\nof the sequence length what are the\\nqueries that this one needs to select so\\nthat's why we use the start query plus\\nthe range that is block Q so imagine the\\nstarting query for this program among\\nall the sequence length is 100 then this\\nwill load the query row 100 1001 1002\\nblah blah blah until 100 plus block Q\\nminus\\none this is the range that we of the\\nquery vectors that we will load in this\\nprogram\\nuh we load the block of Q by using a Q\\nPlus the offset repeated on the columns\\nso we treat it as a column Vector but we\\nrepeat broadcast it on the rows\\nVector um where each uh column will be\\none head Dimension multiply by The\\nStride in this case we actually can also\\nnot multiply by The Stride because the\\nstride in the uh Dimension Dimension so\\nthe last dimension of the batch is one\\nuh because to go from one\\num actually the stride um how it is\\ndefined the stride of the last Dimension\\nis always one because to go one element\\nto the next element you should move to\\nmove it to by one\\nelement um so we load the DQ which is\\nthe stuff that we are going to compute\\nin this\\num iteration and then we have D that we\\nneed to load and the do use the same\\noffset as Q because D and DQ have the\\nsame shape and they work in the same way\\nso we load a block of Q and we load the\\ncorresponding block of\\nO of d o in this case and d o has the\\nsame shape as o which has the same shape\\nas Q Plus we need to load also the M\\nnormalization factors which are in the M\\nMatrix which one the cor the one\\ncorresponding to this particular group\\nof queries that we are going to work\\nwith in this particular program\\nwe start with the uh offsets uh are the\\nas you can see the offsets are the first\\nblock of KV starting from the zero\\nposition so because we will iterate\\nthrough all the KVs and we start from\\nthe zero KV so the key key Vector zero\\nand the V Vector zero and then we will\\nmove by block KV number of vectors\\nforward at each\\niteration I hope I didn't go too fast\\nbecause most of the things that are\\nwritten here are very similar to what we\\nhave already done um in the the other\\nfor Loop so I don't want to be you know\\nrepeat myself too much um what it matter\\nis actually the the formulas that we\\nwill use which is exactly the one in the\\npaper\\nso uh we go through this GL blocks of\\nkeyway KV we load the first block of KR\\ntransposed and V transposed which is\\nloaded like this as usual you tell it\\nwhat pointers the elements you want to\\nload and what are the pointers of\", mimetype='text/plain', start_char_idx=353719, end_char_idx=357914, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='3fae3027-a84f-4a4f-a862-dcab4e644273', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='264e7562-c73a-46a5-85d4-b6cbfae1052a', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='db423439c3b87b8469b555a729914a4b20601bbcbf8102182359e4328df3065e')}, text=\"the\\nanother element that you want to load\\nand it will load the the block that you\\nare asking Tron to load inside of the\\nSRAM so this stuff all reside in the\\nSRAM and also Q resides in the SRAM and\\nalso d o reside in the\\nSRAM um then we compute the query\\nmultiply by the transpose of the keys\\nbecause we need to compute the P block\\nso the query the qk block is just the\\nquery in the current query block with\\nthe K transposed in the current query\\nBlock in the current key\\nblock um why but we access query the\\nkeys already as transposed so we don't\\nneed to transpose it and anyway even if\\nwe did if we need to transpose it it's\\njust um it's not um doesn't require any\\ncomputation to transpose a matrix we\\njust access it in a different\\nway uh because in the memory layout it's\\nalways stored kind of as a flattened\\narray um then we computed the P block\\nwhich is the output of the soft Marx so\\neach of the query key we substract the\\nlog sum exp value for the this uh block\\nof queries that's why for loading the M\\nBlock we use the offsets of the queries\\nthat we are\\nloading uh and as you remember the M\\nBlock already includes also the\\nnormalization factor because each m is\\nactually the maximum value for each row\\nplus the logarithm of the normalization\\nfactor that when you apply with the\\nproperties of the exponential it goes\\ninto the\\ndenominator okay and then we apply again\\nthe auto regressive\\nmasking\\noops what did I\\ndo let me go back to the code here so we\\nhave the stage this one so when we\\nlaunch the back backward pass stage\\nthree indicates that it's a the in the\\nforward pass we computed the caal\\nattention and the one indicates that we\\ncomputed the non-causal attention so if\\nwe computed the causal attention in the\\nforward pass we also need to mask out\\nthese elements in the backward\\npass so we check um we created The Mask\\nwhich tells us which index uh this mask\\nis true for only for the elements for\\nwhich the query index is more than the\\nkey index and if this is true then we uh\\nwe don't mask otherwise we\\nmask um let's compute the next operation\\nwhich is to compute DP and DS actually I\\nlet's compute directly DK and then we\\nexplain it like before so we start from\\nthe end and we go to where this stuff\\nwhat is needed to computed so if you\\nlook at the formula uh let me check\\ncheck this one we don't need I think\\nokay uh let's go here to the iPad okay\\nwhat we are trying to compute here is\\nDQ so DQ as you can see in the paper is\\nDQ is equal to the old DQ plus to which\\nis the soft Max scale which is this\\nstuff here m multiplied by the matrix\\nmultiplication between DS and K block so\\nthe DS block is here and the K block is\\nthe transpose of the KT block because we\\nare accessing K already as a transposed\\nblock we could also access K directly as\\nnot transposed block by inverting if you\\nwant to access it as a transpose block\\njust do like this like here none this\\nwill treat it as a row vector and\\nbroadcast along the columns otherwise\\nand also this one you need to change so\\nthis one you should need to change\\nbecause this one you need\\nto treat it as um a column Vector the\\ndimensions but if you want to access it\\nas a k transpose then you just inverted\\nthese two operations I hope I didn't\\nmess up anything so let's move forward\\num so okay we know that the formula for\\nthe uh DQ is exactly the same as the uh\\nas the paper one but what is this DS\\nblock let's look at the paper this DS\\nblock is coming from this stuff here so\\nthis I believe this stuff here d s which\\nis a\\npi the P block element wise\\nmultiplication with DPI minus di which\\nis DPI minus d i now what is um the this\\nP block the p block is exactly the\\noutput of the soft Max which we already\\nhave what is the DP block well the DP\\nblock is exactly d o multiplied by V\\ntransposed which is d o which we already\\nloaded and it's here multiplied by the\\ntranspose of the V which we already load\\nas transposed and this is how we comput\\nthe\\nDQ uh let's Inc then of course we need\\nto move to the next uh block of uh KVs\\nso we increment the pointers just just\\nlike before so we move to the next block\\nof keys and\\nvalues and also we move the\\npointers um just like before and then\", mimetype='text/plain', start_char_idx=357915, end_char_idx=362055, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='adb810ec-1e7a-4ad5-a213-e08e5cccc13d', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='3fae3027-a84f-4a4f-a862-dcab4e644273', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='58e4b86d35dbb040b334d4f36e0655724044b58fde8c67e396e47f03a69213e2')}, text=\"we\\nneed to store the result of DQ and this\\nway we only need to do one right to the\\nhbm by dividing the for Loop like the\\nfollowing so if you look at the original\\nalgorithm uh I I don't know if the\\noriginal algorithm actually corresponds\\nin to to the implementation that they\\ndid in Cuda but I don't think so because\\nit would not be so\\noptimized but in the original algorithm\\nin the paper they say that you need to\\ngo through all the keys and then while\\ngoing through the keys you need to go to\\nall the cues and for each queue that you\\nvisit then you need to write back the\\nqueue while you are updating it which is\\nnot optimized that's why we needed to do\\ntwo for Loops one in which we fix the\\nquery and we updated the keys because\\neach Keys update depends only on\\nparticular block of Que\\nuh on all the blocks of Q sorry and then\\nwe fix the queries and we iterate\\nthrough all the keys because one block\\nof Q depends on all the blocks of\\nK and this is why we split and this is\\nthe second Loop that we have\\nwritten now we have written everything\\nthat we need to for flashh attention um\\nthe forward pass and the backward pass\\nso uh we should be ready to uh launch\\nthe uh the kernel I hope I didn't make\\nany mistake in copying the\\nso I don't think I will try to launch it\\nand if there is any any error I will\\njust use my reference code which I have\\nalready written that I used as a copy\\nthe only difference up to now between my\\nreference code and the one that we have\\nwritten is the autot tuning which I\\ndidn't explain so let's talk about the\\nautot tuning so the autot tuning is also\\nsomething that was already present in\\nthe original paper and I kept it as is\\nuh I removed the autot tuning for the\\nbackward\\npass but in the forward pass you if you\\ncheck there is this code here that\\nindicates um the autotuning\\nconfiguration for Tron so Tryon\\nbasically um cannot know uh beforehand\\nwhat is the best block size or what is\\nthe best block size for the query or\\nwhat is the best block size for the key\\nand values or what is the best block\\nsize for another dimension that we have\\nwe need to try based on the hardware\\nthat we are running on based on the\\navailability on the SRAM based on the\\nthread quaren that Tron can apply so I\\ndidn't talk also about thread quaring\\nbasically in Cuda you can choose if each\\nthread does one Atomic operation for\\nexample in a matrix addition each thread\\nis doing one addition of one particular\\nelement of the output Matrix or it's\\nmanaging multiple elements this is\\ncalled thread quaring and I think I\\ndidn't check the documentation but I\\nbelieve Tron does it for you based on\\nthe Block size that you give it and the\\nnumber of warps that you want the number\\nof warps is what is um a block of\\nthreads of 32 threads that work Cooper\\ncooperatively running the same\\ninstruction always at the same time the\\nnumber of stages is more interesting\\nit's an optimization that Tron does um\\nbasically it is not Loop and rolling so\\nactually let's talk about uh let's talk\\nabout software pipelining because this\\nis the last part that we need to\\nunderstand from this code which is the\\naing so I believe that most interesting\\npart here is not choosing the block size\\nQ and The Block size K because that is\\njust kind of you try whatever whatever\\nconfiguration works best based on the\\ntiming CA Tron will actually run all\\nthese configurations for you every time\\nthe sequence length or the head\\nDimension changes and for every pair of\\nhead Dimension and sequence length it\\nwill choose the best configuration that\\nruns in the least amount of time that\\ngives you the best throughput actually\\num so let's look at this num stages what\\nis it and how it works um so let's do it\\nokay so uh software pipelining\\nis is used when you have kind of a for\\nLoop so you have a sequential operation\\nin which each iteration does not depend\\non the previous iteration so the\\noperations that you do in one iteration\\nare independent from what you have done\\nin the previous iteration which is more\\nor less what we have done before in our\\nfor Loops actually there I believe there\\nare um uh how to say conditions in which\\nthis doesn't have to be three so like\\nthe operations can depend on each other\\nand you still can do software\\npipelining so for example imagine you\\nhave the following for Loop uh for Loop\\nthat Row\", mimetype='text/plain', start_char_idx=362056, end_char_idx=366350, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='e2e1b656-3927-4b6a-8990-78bcc0436844', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='adb810ec-1e7a-4ad5-a213-e08e5cccc13d', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='7ea25b216f37688b11d38156db59bf72c0daa8509beb423d56cf18d94f92dd03')}, text=\"from one to n and first you\\nload some data then you load some other\\ndata then you do a matrix multiplication\\nand then you store some data so here you\\nare reading data here you are reading\\ndata here you are Computing some stuff\\nand here you are writing data if we look\\nat what happens at it each iteration we\\nwill see the following picture imagine\\nour GPU is made up of a compute unit and\\na a unit that is dedicated to loading\\nstuff so reading from the memory or\\nwriting to the\\nmemory what we will see in the time\\nscale is that at the first iteration\\nfirst we are reading some data and the\\ncompute unit is Idle because we need\\nthis data then we are reading some more\\ndata and the computer unit is Idle\\nbecause we need this data then finally\\nwe have enough data and then we can\\ncompute this operation and the reading\\nunit is idle and then we are writing\\nsome data back to the memory and the\\ncomput unit is again idle and then it\\nwill be idle for another two time steps\\nuntil it has enough data to run the\\ncomputation so as you can see this is\\nnot very efficient because at any time\\npoint in time there is only one unit\\nworking and the other is a sitting Idol\\nso one way to uh optimize this for Loop\\nis to do software pipelining and you can\\ntell try to do it for your for Loops by\\ntelling it how many stages you want so\\nlet's see how it works so to pipeline a\\nfor Loop means that first of all you\\nneed to convert all these operations\\ninto async operations and in Cuda at\\nleast in the GPU of mvidia there are the\\nasync loading from the memory and the\\nasync load writing to the memory which\\nbasically means that I spawn a load\\noperation and after and um when I I only\\nI check if it has completed when I\\nactually need it so I will spawn this\\noperation and this instruction will\\nreturn immediately and move to the next\\ninstruction here I will spawn a load\\niteration and this will return\\nimmediately and move to the next\\ninstruction and then I can uh compute\\nbut before Computing I just check if\\nthese two operations have completed so I\\ncan spawn immediately two reads and then\\nI just check if this uh they have\\ncompleted so with software pipelining\\nwhat we are doing is we are pipelining\\noperations of different iterations into\\na single iterations so first basically\\nwhat we will do is we will do the read\\nthe first Matrix that we need for\\ncomputing this matrix multiplication\\nthen at this next iteration we read the\\num the uh we we read the first Matrix of\\nthe second iteration and also read the\\nsecond matric of the first iteration so\\nI call it read a and read B which\\nindicates read the first Matrix of the\\nuh that we need and the b means the read\\nthe second Matrix that we\\nneed uh all these operations are\\nasynchronous then I launch another\\nasynchronous operation at the third\\niteration that says read the um the\\nfirst Matrix of the third iteration and\\nthen read the uh second Matrix of the um\\nof the second iteration and then compute\\nthe matrix multiplication because at the\\nthird iteration this one and this one\\nshould have completed but but while\\nComputing the matrix multiplication I\\ndon't keep the loading unit idle because\\nthey are still Computing this this and\\nthis load this can only work if you can\\nspawn async\\noperations so at the third iteration I\\ncan compute this matrix multiplication\\nby using this one and this one because\\nthey should have finished but while I'm\\nComputing the matrix multiplication I\\nalready spound some async operations to\\nload the data necessary for the second\\niteration and the third iteration so at\\nthe fourth iteration I will spound the\\nloading of the data for the fourth\\niteration loading the data for the third\\niteration while Computing the matrix\\nmultiplication of the second iteration\\nbecause they should have already\\ncompleted by now uh actually it's not\\nlike we expect them to have been\\ncompleted there are um Primitives in the\\nlanguage or in the Cuda language to\\ncheck if the operation has completed so\\nactually before doing the matric\\nmultiplication we will actually check if\\nthe asyn operation has finished so it's\\nnot like we just expect it to be\\nfinished with respect to time uh this is\\nlike in JavaScript you have these things\\ncalled prom promise I remember and you\\ncan wait for the promise to be finished\\nbefore you actually need them but you\\ncan spawn as many promise as you want in\\nC I think they are called tasks so you\\nspawn as many tasks as you want and then\\nwhen you need the it then you just wait\\nfor them only the one that you needed\\nwhile the other are still running in\", mimetype='text/plain', start_char_idx=366351, end_char_idx=370875, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='842c8d4b-6021-4dfd-a927-eafea9952754', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e2e1b656-3927-4b6a-8990-78bcc0436844', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='28d02f0b5a2c3f7ca906b5da2944957c5e48c78825e0cd7fcf8f8098cba9606b')}, text=\"the\\nbackground asynchronously this is the\\nwhole idea of software\\npipelining um software pipelining as you\\ncan see only works when you have a sync\\noperations and also it increases the\\nmemory requirement for your program\\nbecause when uh matrix multiplication\\none uh is going to run we may have\\nenough data for the first two iterations\\nplus plus half data for the third\\niteration so we increase the memory\\nrequirement for the um\\nSRAM okay and P Tron will do this\\nsoftware pipelining for you it will\\nconvert all the load all the stores and\\nmaybe also the metrix multiplication\\ninto a sync operations and do this\\npipelining for you if you are confused\\nby how it works there is another easy\\nsolution to explain you how it works\\nbecause it's already something that we\\ndo in model training it is called\\npipeline parallelism\\nso in pipeline parallelism it works as\\nfollows we have a very big neural\\nnetwork that does not fit in a single\\nGPU so imagine this neural network is\\nmade up of three layers layer one layer\\ntwo and layer three but this is so big\\nit does not fit entirely in one simple\\nGPU so one way would be to put this each\\nlayer into one GPU so we put for example\\nlayer one into GPU\\none a layer two into GPU 2 layer three\\ninto GPU number three so imagine we have\\nan input for this Neal Network so we put\\nit to the first GPU the GPU one will\\nprocess the layer one and generate some\\noutput which will be transferred to the\\ngpu2 which will calculate its own output\\nand transfer it to the gp3 which will\\ncompute its own output and finally we\\nwill have the output of the Nal Network\\nthe problem is when you send the output\\nof the gpu1 to the gpu2 for the gpu2 to\\ndo its own thing the gpu1 now is free so\\nit is a waste of resources we could\\nalways should keep the gpus busy so what\\none thing that we can do is instead of\\nsending all the the mega batch to the\\ngpu1 we send many smaller batches how\\ndoes it work imagine that we send the\\nbatch number zero so patch zero uh to\\nthe gpu1 the gpu1 will compute its\\noutput and send it to the gpu2 so now\\nthe gpu2 is Computing the batch number\\nzero so now the batch zero is not here\\nanymore but now the GPU one is free so\\nwe send another micro batch called the\\nbatch\\none then gpu2 will finish processing the\\nbatch zero and we'll send it to the\\nbatch to the GPU number three so now the\\nGPU 3 has the batch number\\nzero and the gpu2 now is free so we\\ntransferred and hopefully also gpu1 has\\nfinished so we transfer the batch number\\none from gpu1 to gpu2 the bches um and\\nthen the GP one will be free so so we\\ntransfer here becomes one and now this\\none is free so because it's gpu1 is free\\nwe can introduce another batch so batch\\nnumber two etc etc etc so we always\\nintroduce when while moving one batch\\nfrom one GPU to the other we introduce a\\nnew batch at the beginning of the\\npipeline and they shift by one position\\nat every iteration this will keep the\\ngpus always busy uh there is only one\\nproblem of the pipeline parallelism\\nwhich is the this bubbling effect\\nbecause to create this pipeline you at\\nthe beginning of this um okay actually\\nin the pipeline paradism you also have\\nthe problem of the backward step so the\\nbackward step has to run exactly in\\nReverse in the order in which you\\nreceive the micro batches while in Tron\\nwhen doing software\\npipelining you have the problem of the\\nprologue and the epilog because you need\\nto create this Pipeline and and\\nto start the pipelining and at the end\\nof the pipeline you need to uh use all\\nthe stuff that is currently in the\\npipeline so only in the beginning step\\nand in the last step of this for Loop\\nyour um all the units of this GPU may\\nnot be working\\nsimultaneously which what does it mean\\nit means that in order to use pipelining\\nyou want the number of iterations of\\nyour for Loop to be much more bigger\\nthan the number of stages in which your\\niteration is divided into in this case\\nwe have four stages these are called\\nstages so you want the number of\\niterations to be much more to to be much\\nlarger than the number of stages all\\nright guys finally I have completed the\\nvideo um I hope that you learned a lot\\nfrom this video I believe that we can\\nrun the Tron code so let's run it\\nactually uh let's see I copied\\neverything I believe we also\", mimetype='text/plain', start_char_idx=370876, end_char_idx=375106, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='09f25c3d-11e9-42bf-90ef-13a1e29ff665', embedding=None, metadata={'video_id': 'zy8ChVd_oTM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='zy8ChVd_oTM', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='c1b8952bfca72e537014ccb3ba8d0ab0292f3a9c529d9f22c89359f8c243feec'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='842c8d4b-6021-4dfd-a927-eafea9952754', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'zy8ChVd_oTM'}, hash='6af6a55910676361ed971fd250c4c9acd9c089e1af1643f0e6be0f9cf8e60cd8')}, text=\"put the\\ncode to test it but we didn't uh put the\\nuh main method which we can copy right\\nnow I hope there is no error so I really\\nhope there is no error I really hope so\\num let me check if I am in the right\\nmachine I am so let's just run\\nprogram prey if there is an error I will\\njust copy my own reference\\nimplementation but I hope it works\\nbecause otherwise I forgot something so\\nI'm running my code on an h100 uh\\nbecause my company has h100 uh if you\\nhave a smaller uh GPU what you can do is\\nyou can reduce the sequence length uh\\nyou can reduce the BET BET size I think\\nis already one when we call it uh oh no\\nthe BET size you can reduce the BET size\\nthe number of heads the sequence length\\nyou can even put head Dimension equal to\\n8 and sequence length equal to 16 uh\\nlet's\\ncheck uh run backward Tron backward\\nreturn the incorrect number of grading\\nexpected five got one\\nwe probably forgot some return statement\\nI\\nbelieve\\nyes so I forgot the return statement\\nhere so in the backward pass after\\nrunning the last for Loop we need to\\nreturn the stuff that we have\\ncomputed cross finger\\nagain okay passed so the backward pass\\nthat is computed by torch it is\\nequivalent to our backward patch up to\\n10 to the power of minus two error\\nabsolute error um so when you as you can\\nsee this backward that we run here is\\ndifferent than the backward that we run\\nhere because when you apply Tron\\nattention it will introduce a new\\ncomputation graph in the computation\\ngraph of our tensors that will include\\nthis Triton attention operator and when\\npyto want to compute the backward pass\\nit will just call the backward function\\nof this Tron attention to computed and\\nit will populate the grad value of all\\nthe tensors that are the input to this\\nTriton\\nattention and this is how pytorch\\nautograd Works guys uh thank you for\\nwatching my video guys it has been super\\nsuper super\\ndemanding I spent many uh months first\\nof all to learn myself about the Tron\\nabout Cuda about Flash atten Etc uh also\\nI have a full-time job so it it's is\\nreally hard to make videos like this\\nlike I need to dedicate you know the\\nnights the mornings the the weekends I\\nspend three days just to record this\\nvideo because sometimes I don't like how\\nI explain something sometimes I make\\nmistake or sometimes I need to restart\\nbecause what I'm doing is wrong etc etc\\num I believe there should be no big\\nerrors in what I have done so far but\\nfor sure my notation is completely bad\\nlike because all the mathematics I know\\nhas been self-taught by I I learned it\\nby myself so because I didn't learn it\\nin Academia I have bad habits and I'm\\ntrying to get rid of them so I use the\\nvery bad notation sometime I call with\\nthe capital letter sometime with this\\nlower case sometimes I just forget the\\nindex Etc so I'm trying to solve these\\nproblems um I believe I have explained\\neverything so I should be uh you should\\nhave all the knowledge to derive all the\\nformulas that you see in the paper of\\nthe flashh attention uh and you should\\nalso have an internal image on how the\\nback the the the attention calculation\\nis working block by blocks I know that I\\ncould have spent 20 hours explaining\\nthings better but I also have a life and\\nI also have a wife so I I I I cannot\\nmake a 100 hours videos um also there\\nwere some interruptions making these\\nvideos I I removed some wisdom teeth so\\nit took me at least one more than one\\nweek to to to to recover because it was\\nso painful uh so thank you guys for\\nwatching my video I hope you learned a\\nlot also this time I as you can see\\nTriton is something new there is not\\nmuch documentation so something that I\\nhave said about Tron may not be totally\\ncorrect because really there is very\\nlittle documentation so all the Tron\\nthat I have learned is by looking at the\\ncode written by others and try to\\nunderstand\\nit\\num and I think that's it guys so I wish\\nyou a wonderful day and see you next\\ntime on my channel\", mimetype='text/plain', start_char_idx=375107, end_char_idx=379000, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting embedding model"
      ],
      "metadata": {
        "id": "06yZmQLsKsYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='BAAI/bge-small-en-v1.5'\n",
        "embed_model =embeddings.HuggingFaceEmbeddings(model_name=model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "2a7d8a55eac74f7081085f34f9705ce5",
            "90b75c58a7034750b29643d270af19ca",
            "5fe867f1d0d143318c11c568d0c200d2",
            "4da1aa8a3c1b47c9995eb2a0299dbdcb",
            "b4e538fc8f0f4a2d930c5724621ad3a4",
            "55ed9e15bd7849569cb2736d6bbcf273",
            "a05d206dee494905a2333d221e81c841",
            "f771915c65e542a99cfde2f6c96bb2d9",
            "d02742026b004653b1bb030e569f9055",
            "cdc6af1472ba409087d1921a79a11a91",
            "253f1221c94f46a3b035fe4d4f717c43",
            "13c9df9229d941e5974bb4b9626691d3",
            "1945e5da2493420caa0096bceda10759",
            "998d04307944456fa6eca1b5e640a640",
            "bfc4d212d1bc471fb94b325e93cdd8cd",
            "21718bef4c534db4a3c5f77e61abafc6",
            "7783989021b6410489de834fea7d5950",
            "ef7c09785acd4b9c849ac874173dc893",
            "26e858497e05493f8927dad08a9c58da",
            "556ce9eb3fd64094a898c76c356773c9",
            "ed37f06ccc164fe1899b760235775386",
            "dc709f0bac3548e0bc07190723003e0a",
            "41232f708f9e493ab62549089b9ecfea",
            "94e2325b03be4256b02cafabc9e8a0b4",
            "e6c2e49e62ca46579e2b659217f0d4d8",
            "1a02d5627dd64b9ca5dc38bd1445b13d",
            "c8811ee1bb9247b5b6bc1c2b531b2c84",
            "2fc8c387accc4ea88bc2e3bd90982b38",
            "a0826237751a4723b4cb323c8855f45d",
            "b198a1b0b91342479c321ccff3c07e21",
            "0811b393d8d74decb5c8b4e6c31592fe",
            "1fa650ce8d2c477685a5d95b5630e498",
            "e4beb8a806be417c8d169f2561bfb9e0",
            "77138b9b3f5e47bb86ba8433b8b5d88a",
            "bc01f71d7d09445387d51a06918b0938",
            "5c21e3c19b92447db289884978bb71f3",
            "4a363e02f4684a0d92836bd646757454",
            "497d562e62b541de90e297c44bf8727c",
            "f846bdcabc884a6f876cb65ae005d369",
            "4ba1846d9d9b425ba398a73f5fa0db77",
            "8b366625b2cc48c38fb17ecf4b70fbcf",
            "38c4a8833be2485dba955fa5bc1a7db8",
            "40b99a4100c64c5694fe9f43f9eaad32",
            "068dcf3ee1e34e848110b64aacf14fa9",
            "dadfd96c2f694cf7a6c76de18ddb4f73",
            "e641a047601d4e4084dad08c8dbce369",
            "f8a27179e79848bfaffb13b8dfd518eb",
            "52c5a9dad0f74bf5b6cecbc16b752b97",
            "103496ff0a854b08a4babb241f7f791d",
            "2f3d5cd1d1994d849dc826f113622252",
            "124c33c947ab4262a82e9176deb1b4f5",
            "e1a0cf2af7a94464b6098d065e510dd1",
            "ed2b7884e5c1495db4f0aad3719dce47",
            "d56de21a67bc4a3bbcb00c7e11e84b0f",
            "a58bae7c70b142549c135642e662fb05",
            "79e683d2166945a3bf8ca0bb2e192396",
            "22f8c9d7759743e2a6e2dc7f20adcdc9",
            "f9040b02d73e4656b0c9858b197cdcf8",
            "9089020aed8a424baf4f44bed36a21cb",
            "77d73aa211574624abaaf0970ff42233",
            "725f6c79abea48708fe5894da1395ec5",
            "b8cfa09bb23840ec9cb6c2724d12793a",
            "49a1cb7df51149498fc1ad5fc28f9051",
            "d47700272ed64ae4af67623c0520e4fa",
            "42c087bb2a9d4a0c909a6c5c1d6b6175",
            "07f66a604b7347f3a97934be7b25f4d4",
            "2336df86b00647d88e6432c70882b1e9",
            "f4da1a6bac1d4347b66ef933eaabc5d7",
            "f2aa535311e943308a1301c2d7ad1308",
            "4d38dc64e46e4853ab2ce135a07444cd",
            "c615b438e5d4432492fdb4e2f9ff067c",
            "97da8739054246e1ae252fbd0305660c",
            "8ae8d0b0feb547efab1fff75cd0bbc8c",
            "77416f5caa194a5c87603bdf34fd3055",
            "ab2e26c45b58452cad300c478a0ae4ab",
            "55713837e67644499435fbaeecb1f877",
            "84f6c9520776453a97ee36b694dad165",
            "c2f17eab22214f628ecc5b951698bb97",
            "0b9b5f5053594e06a99d65359d396636",
            "b302b5d2e7004dddbfaa4f12138c2be4",
            "4df6a21febd748ff8055e5f69c7fb305",
            "4201a207e32e4917ad3bc5d6937b4711",
            "e1c866bd810744d9a918ecc2770fc01d",
            "4b756ade93734b0aa8a0c84fcb81e1a9",
            "2250370b18614ef1bf1b387ef49334d6",
            "b8321d9dfd8745bab585a4f244d020b2",
            "5b91620940ed482882d668b9509ab1ab",
            "e451595505f7415d84645dfcb9de7ab0",
            "e43c1c7d7e6a4a57a7382f0557780854",
            "a02450fc54184b92ae93e7bc089fcfe7",
            "5ca4ca5b36d3465d9db02312a8a4f868",
            "176990a933804e5cb69535d016af36a1",
            "0ea66ad5dace48eab19caa0282d75421",
            "1e3baab3c6934b0aa6cc406aaa7ba3fc",
            "18814018052b4aa0a0e2d9c420383420",
            "87e0ff78b52644b794fd14fdb45d0b7b",
            "9cfadd1d701942caae5507eff1760d20",
            "4ec7b378d1814759b97cc1b1d716c986",
            "952b1e1787764ca588d5fde9b243ef5a",
            "1127cc8a58234881b97a61e9e78e7a37",
            "e1a4f1def60a4f9abb7dab588e7e7c58",
            "2320889cda2d4f8691e7754b1b8b02da",
            "1f128e18c2ee43c0a0c1bfe94326f129",
            "cf94016c78b1451b98dd46e30e74566a",
            "bfa63e998e8242e296cea0f487ff558d",
            "e06da15fc94a48d0998a47ed6aacf2dc",
            "40404e8581744d87b0ecafef024c6eb3",
            "4378384bcb304e9faef47469cd381158",
            "8fe57d2d9cc343428ee5a9c41a22af40",
            "3d582e6039ef430b9cf45e80440e1f52",
            "5dce8892b05a4013880fe0c2d472ab29",
            "3ce2fece8a3e486ebc516cbfb2db3a47",
            "db65385a84f7405987f87966363d1afc",
            "bbfaaf83c135445ab66f29c968297cac",
            "43c75f0ce6fb4702a5bbc75480db098a",
            "f409487512b44edcb945319116cf118c",
            "6c7fbe83569a409fb1d105fb9216658c",
            "43e22b9696ab4cea948d598601d8ed57",
            "1de627970a1545848f30c89fb56763da",
            "213b5ba84f5a4be3969a00de487647ca",
            "1b3d39cf72a54367944301f770d3ed58"
          ]
        },
        "id": "F934LZRfKlXW",
        "outputId": "67694d69-68f8-472b-e784-48b4e9020380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a7d8a55eac74f7081085f34f9705ce5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13c9df9229d941e5974bb4b9626691d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41232f708f9e493ab62549089b9ecfea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77138b9b3f5e47bb86ba8433b8b5d88a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dadfd96c2f694cf7a6c76de18ddb4f73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79e683d2166945a3bf8ca0bb2e192396"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2336df86b00647d88e6432c70882b1e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2f17eab22214f628ecc5b951698bb97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e43c1c7d7e6a4a57a7382f0557780854"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1127cc8a58234881b97a61e9e78e7a37"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5dce8892b05a4013880fe0c2d472ab29"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "setting the retriever"
      ],
      "metadata": {
        "id": "cvbjEp4IL8DQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = retrieve.auto_retriever(\n",
        "    data = data,\n",
        "    embed_model=embed_model,\n",
        "    type='hybrid',\n",
        "    mode='OR',\n",
        "    top_k=2\n",
        ")"
      ],
      "metadata": {
        "id": "mguSy0yhL-59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting LLM"
      ],
      "metadata": {
        "id": "ROAvB1RpMzAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = llms.GeminiModel(\n",
        "    model_name='gemini-pro',\n",
        "    google_api_key=google_api_key\n",
        ")"
      ],
      "metadata": {
        "id": "z2YoaZpkM28d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate answer"
      ],
      "metadata": {
        "id": "VuxAHBYxNVNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'which tool is mentioned in the video?'"
      ],
      "metadata": {
        "id": "XzCE3uyfNSdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = generator.Generate(\n",
        "    question=question,\n",
        "    retriever=retriever,\n",
        "    llm=llm\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "DAM4FSRhNgZn",
        "outputId": "60e8b7ed-4fc2-4623-c57c-8d23364cb439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM is explicitly disabled. Using MockLLM.\n",
            "Embeddings have been explicitly disabled. Using MockEmbedding.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pipeline.call())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-VjNggjN4Y4",
        "outputId": "a5207dc5-8b03-4f2e-ca5e-34a911f58536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The video mentions the following tools:\n",
            "- CSV loader\n",
            "- Recursive character text splitter\n",
            "- Hugging face inference API embeddings\n",
            "- Chroma Vector store\n",
            "- Azure chat openi\n",
            "- Combine docs chain\n",
            "- Retrieval qm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluation"
      ],
      "metadata": {
        "id": "Xq1nX5aPOBdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pipeline.get_rag_triad_evals())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "j_QjEYFDOD63",
        "outputId": "c0beba9d-60f1-44bc-be5c-0554111a0edb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing RAG Triad Evaluations...\n",
            "Context relevancy Score: 7.0\n",
            "This response does not meet the evaluation threshold. Consider refining the structure and content for better clarity and effectiveness.\n",
            "Answer relevancy Score: 10.0\n",
            "This response meets the evaluation threshold. It demonstrates strong comprehension and coherence.\n",
            "Groundness score: 7.4\n",
            "This response does not meet the evaluation threshold. Consider refining the structure and content for better clarity and effectiveness.\n"
          ]
        }
      ]
    }
  ]
}